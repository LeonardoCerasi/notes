\selectlanguage{english}

\section{Matrices}

\begin{definition}{Matrix}{}
  Given a field $ \K $ and $ n , m \in \N $, an $ n \times m $ \bcdef{matrix}\index{matrix} on $ \K $ is the object:
  \begin{equation*}
    \mt{A} =
    \begin{bmatrix}
      a_{11} & a_{12} & \dots & a_{1m} \\
      a_{21} & a_{22} & \dots & a_{2m} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{n1} & a_{n2} & \dots & a_{nm}
    \end{bmatrix}
    \equiv [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m}
    \quad : \quad
    a_{ij} \in \K \,\,\forall i = 1, \dots, n ,\, j = 1, \dots, m
  \end{equation*}
  The set of all $ n \times m $ matrices on $ \K $ is denoted by $ \K^{n \times m} $.
\end{definition}

When the dimensions of the matrix $ \mt{A} $ are unambiguous, we simply write $ \mt{A} = [a_{ij}] $. We say that an $ n \times n $ matrix is a \bctxt{square matrix}, an $ n \times 1 $ matrix is a \bctxt{column vector} and a $ 1 \times n $ matrix is a \bctxt{row vector}.

It is possible to define three operations between matrices:
\begin{itemize}
  \item sum $ + : \K^{n \times m} \times \K^{n \times m} \rightarrow \K^{n \times m} : [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} + [b_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} \mapsto [a_{ij} + b_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $
  \item product by a scalar $ \cdot : \K \times \K^{n \times m} \rightarrow \K^{n \times m} : \alpha \cdot [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} = [\alpha a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $
  \item product $ \cdot : \K^{n \times p} \times \K^{p \times m} \rightarrow \K^{n \times m} : [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, p} \cdot [b_{ij}]^{i = 1, \dots, p}_{j = 1, \dots, m} = [c_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $, $ c_{ij} = \sum_{k = 1}^p a_{ik} b_{kj} $
\end{itemize}
Note that $ \alpha a_{ij} $ is the $ \K $-product.

\begin{proposition}{}{mat-abel-gr}
  $ (\K^{n \times m} , +) $ is an abelian group.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    The matrix sum is equivalent to the $ \K $-sum of corresponding elements, which is associative and commutative. The neutral element is the zero matrix $ 0_{n \times m} = [0]^{i = 1, \dots, n}_{j = 1, \dots, m} $, while the inverse element is $ -\mt{A} = [-a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $.
  \end{proof}
\end{proofbox}

\begin{theorem}{}{}
  $ (\K^{n \times n} , + , \cdot) $ is a non-commutative ring.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    By \pref{prop:mat-abel-gr}, $ (\K^{n \times n} , +) $ is an abelian group. It is trivial to show the associativity and distributivity of the matrix product, i.e.:
    \begin{enumerate}
      \item $ \mt{A} \cdot (\mt{B} \cdot \mt{C}) = (\mt{A} \cdot \mt{B}) \cdot \mt{C} ,\, \lambda (\mt{A} \cdot \mt{B}) = (\lambda \mt{A}) \cdot \mt{B} = \mt{A} \cdot (\lambda \mt{B}) \,\,\forall \mt{A} , \mt{B} , \mt{C} \in \K^{n \times n} , \lambda \in \K $
      \item $ \mt{A} \cdot (\mt{B} + \mt{C}) = \mt{A} \cdot \mt{B} + \mt{A} \cdot \mt{C} ,\, (\mt{A} + \mt{B}) \cdot \mt{C} = \mt{A} \cdot \mt{C} + \mt{B} \cdot \mt{C} \,\,\forall \mt{A} , \mt{B} , \mt{C} \in \K^{n \times n} $
    \end{enumerate}
    Finally, the neutral element of the matrix product is the identity matrix $ \mt{I}_n = [\delta_{ij}]_{i,j = 1, \dots, n} $.
  \end{proof}
\end{proofbox}

\begin{definition}{Transposed matrix}{}
  Given a matrix $ \mt{A} \in \K^{n \times m} $, its \bcdef{transpose} is defined as $ \mt{A}\tsp \in \K^{m \times n} : [a\tsp_{ij}]^{i = 1, \dots, m}_{j = 1, \dots, n} = [a_{ji}]^{j = 1, \dots, n}_{i = 1, \dots, m} $.
\end{definition}

A square matrix $ \mt{A} \in \K^{n \times n} $ is said \bctxt{symmetric} if $ \mt{A}\tsp = \mt{A} $ or \bctxt{antisymmetric} if $ \mt{A}\tsp = - \mt{A} $, and it is \bctxt{diagonal} if $ a_{ij} = 0 \,\,\forall i \neq j \in \{1, \dots, n\} $.

\begin{definition}{Inverse matrix}{}
  A square matrix $ \mt{A} \in \K^{n \times n} $ is \bcdef{invertible} if $ \exists \mt{A}^{-1} \in \K^{n \times n} : \mt{A}^{-1} \cdot \mt{A} = \mt{A} \cdot \mt{A}^{-1} = \mt{I}_n $.
\end{definition}

\begin{example}{Non-invertible matrix}{}
  The matrix $ \begin{bmatrix} 2 & 0 \\ 0 & 0 \end{bmatrix} $ is non-invertible, as $ \begin{bmatrix} 2 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} = \begin{bmatrix} 2 \alpha & 2 \beta \\ 0 & 0 \end{bmatrix} \neq \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \,\forall \alpha , \beta , \gamma , \delta \in \R $.
\end{example}

\begin{definition}{General linear group}{}
  The \bcdef{general linear group}\index{$ \GL{n,\K} $} $ \GL{n,\K} $ is defined as the subset of $ \K^{n \times n} $ of all invertible matrices.
\end{definition}

Note that $ \GL{1,\K} = \K - \{0\} $.

\begin{theorem}{}{}
  $ (\GL{n,\K} , \cdot) $ is a non-abelian group.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    The neutral element is $ \mt{I}_n $, as $ \mt{I}_n^{-1} = \mt{I}_n \implies \mt{I}_n \in \GL{n,\K} $, while the existence of the inverse is granted by definition. We only have to show closure under matrix multiplication:
    \begin{equation*}
      (\mt{A} \mt{B})^{-1} = \mt{B}^{-1} \mt{A}^{-1} \impliedby \mt{I}_n = \mt{A} \cdot \mt{A}^{-1} = \mt{A} \mt{I}_n \mt{A}^{-1} = \mt{A} \mt{B} \mt{B}^{-1} \mt{A}^{-1} = (\mt{A} \mt{B}) (\mt{A} \mt{B})^{-1}
    \end{equation*}
    Hence, $ \mt{A} , \mt{B} \in \GL{n,\K} \implies \mt{A} \mt{B} \in \GL{n,\K} $.
  \end{proof}
\end{proofbox}

\subsection{Linear systems of equations}

A \bctxt{linear equation} with $ n \in \N $ variables and $ \K $-coefficients is an expression of the form:
\begin{equation*}
  a_1 x_1 + \dots + a_n x_n = b
  \qquad
  a_i , b \in \K \,\,\forall i = 1, \dots, n
\end{equation*}
A \bctxt{solution} of the equation is an $ n $-tuple $ (\bar{x}_1, \dots, \bar{x}_n) \in \K^n $ which satisfies this expression.

\begin{definition}{Linear system of equations}{}
  A linear system of equations (or simply \bcdef{linear system}\index{linear system}) is a collection of $ m $ linear equations with $ n $ variables:
  \begin{equation*}
    \begin{cases}
      a_{11} x_1 + \dots + a_{1n} x_n = b_1 \\
      a_{21} x_1 + \dots + a_{2n} x_n = b_2 \\
      \qquad \qquad \quad \vdots \\
      a_{m1} x_1 + \dots + a_{mn} x_n = b_m \\
    \end{cases}
    \qquad \iff \qquad
    \mt{A} \ve{x} = \ve{b}
  \end{equation*}
  where we defined:
  \begin{equation*}
    \mt{A} =
    \begin{bmatrix}
      a_{11} & \dots & a_{1n} \\
      a_{21} & \dots & a_{2n} \\
      \vdots & \ddots & \vdots \\
      a_{m1} & \dots & a_{mn}
    \end{bmatrix}
    \in \K^{m \times n}
    \qquad \qquad
    \ve{b} =
    \begin{pmatrix}
      b_1 \\ b_2 \\ \vdots \\ b_m
    \end{pmatrix}
    \in \K^{m \times 1}
    \qquad \qquad
    \ve{x} =
    \begin{pmatrix}
      x_1 \\ x_2 \\ \vdots \\ x_n
    \end{pmatrix}
    \in \K^{n \times 1}
  \end{equation*}
\end{definition}

Two linear systems with the same set of solutions are called \bctxt{equivalent systems}: note that two equivalent systems must have the same number of variables, but not necessarily the same number of equations.

Based on the cardinality of its solution set, a linear system is said to be \bctxt{impossible} if it has no solutions, \bctxt{determined} if it has one solution and \bctxt{undetermined} if it has infinitely-many solutions. Moreover, if the solution set can be parametrized by $ k \in \N_0 $ variables, the system is of kind $ \infty^k $: a determined system is of kind $ \infty^0 $.

Linear systems can be systematically solved applying a reduction algorithm to their corresponding matrices: \bctxt{Gauss algorithm}\index{Gauss algorithm}. Starting with a general composed matrix $ [\mt{A} | \ve{b}] \in \K^{m \times (n+1)} $, first we multiply the first row by $ a_{11}^{-1} $, so that:
\begin{equation*}
  \left[
  \begin{array}{cccc|c}
    a_{11} & a_{12} & \dots & a_{1n} & b_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
  \quad \longrightarrow \quad
 \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
\end{equation*}
Then, at each row $ \mt{R}_2 , \dots , \mt{R}_m $ we apply the transformation $ \mt{R}_k \mapsto \mt{R}_k - a_{k1} \mt{R}_1 $, so that:
\begin{equation*}
  \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
  \quad \longrightarrow \quad
 \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    0 & a'_{22} & \dots & a'_{2n} & b'_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & a'_{m2} & \dots & a'_{mn} & b'_m \\
  \end{array}
  \right]
\end{equation*}
Reiterating this process to progressively smalles submatrices, the algorithm yields the general transformation:
\begin{equation*}
  \left[
  \begin{array}{cccc|c}
    a_{11} & a_{12} & \dots & a_{1n} & b_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
  \quad \longrightarrow \quad
  \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    0 & 1 & \dots & a'_{2n} & b'_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \dots & 1 & b'_m \\
  \end{array}
  \right]
\end{equation*}
As these are linear transformations, the two matrices represent equivalent linear systems: the transformed linear system is substantially easier to solve, and its solution set is a solution set of the starting linear system too.

\begin{definition}{Character}{}
  Given a matrix $ \mt{M} \in \K^{n \times m} $, its \bcdef{character} $ \mathrm{car}(\mt{M}) $ is the number of non-zero rows remaining after Gauss reduction.
\end{definition}

It can be proven that the character is independent of the operations performed during the reduction algorithm.

\begin{theorem}{Rouché--Capelli theorem}{}
  A linear system $ \mt{A} \ve{x} = \ve{b} $ has solutions only if $ \mathrm{car}(\mt{A}) = \mathrm{car}([\mt{A} | \ve{b}]) $. Moreover, if the system has solutions, then it is of kind $ \infty^{n - r} $, with $ n $ number of variables and $ r = \mathrm{car}(\mt{A}) $.\index{theorem!Rouché--Capelli}
\end{theorem}

\section{Vector spaces}

\begin{definition}{Vector space}{}
  Given a set $ V \neq \emptyset $ and a field $ \K $, then $ V $ is a \bcdef{$ \K $-vector space} if there exist two operations:
  \begin{equation*}
    + : V \times V \ra V \,:\, (\ve{v} , \ve{w}) \mapsto \ve{v} + \ve{w}
    \qquad \qquad
    \cdot : \K \times V \ra V \,:\, (\lambda , \ve{v}) \mapsto \lambda \cdot \ve{v}
  \end{equation*}
  such that $ (V,+) $ is an abelian group and the following properties hold $ \forall \lambda , \mu \in \K , \ve{v} , \ve{w} \in V $:
  \begin{enumerate}
    \item $ (\lambda + \mu) \cdot (\ve{v} + \ve{w}) = \lambda \cdot \ve{v} + \mu \cdot \ve{v} + \lambda \cdot \ve{w} + \mu \cdot \ve{w} $
    \item $ (\lambda \cdot \mu) \cdot \ve{v} = \lambda \cdot (\mu \cdot \ve{v}) = \mu \cdot (\lambda \cdot \ve{v}) $
    \item $ 1_\K \cdot \ve{v} = \ve{v} $
  \end{enumerate}
\end{definition}

Note that there are three unique neutral elements: $ 0_\K \equiv 0 $, $ 1_\K \equiv 1 $ and $ 0_V \equiv \ve{0} $.
In the following, the multiplication symbol $ \cdot $ is suppressed, as the factors clarify which multiplication is occurring ($ \cdot : \K \times \K \ra \K $ or $ \cdot : \K \times V \ra V $, which have the same neutral element $ 1_\K $).

\begin{example}{Complex numbers}{}
  $ V = \C $ is a vector space both for $ \K = \R $ and $ \K = \C $, although they are different objects.
\end{example}

\begin{example}{Field as vector space}{}
  $ V = \K $ is a $ \K $-vector space. Note that, in this case, $ 0_\K \equiv 0_V $.
\end{example}

Note that, by the uniqueness of $ 0_V $, then $ \forall \ve{v} \in V \,\,\exists! -\ve{v} \in V : \ve{v} + (-\ve{v}) = 0_V $, so the following cancellation rule holds $ \forall \ve{u} , \ve{v} , \ve{w} \in V $:
\begin{equation}
  \ve{u} + \ve{v} = \ve{w} + \ve{v}
  \quad \implies \quad
  \ve{u} = \ve{w}
  \label{eq:cancellation-rule}
\end{equation}

We can now state some basic properties of vector spaces.

\begin{lemma}{Basic properties of vector spaces}{}
  Given a $ \K $-vector space $ V $, then $ \forall \lambda \in \K , \ve{v} \in V $:
  \begin{multicols}{2}
    \begin{enumerate}[label = {\alph*.}]
      \item $ 0_\K \cdot \ve{v} = 0_V $
      \item $ (-\lambda) \cdot \ve{v} = - (\lambda \cdot \ve{v}) $
      \item $ \lambda \cdot 0_V = 0_V $
      \item $ \lambda \cdot \ve{v} = 0_V \iff \lambda = 0_\K \lor \ve{v} = 0_V $
    \end{enumerate}
  \end{multicols}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item Consider $ c \in \K - \{0_\K\} $; then $ c \ve{v} + 0_V = c \ve{v} = (c + 0_\K) \ve{v} = c \ve{v} + 0_\K \cdot \ve{v} $, which by \eref{eq:cancellation-rule} proves $ 0_\K \cdot \ve{v} = 0_V $.
      \item $ \lambda \ve{v} + (-\lambda) \ve{v} = (\lambda - \lambda) \ve{v} = 0_\K \cdot \ve{v} = 0_V $, which by the uniqueness of the negative element proves $ (-\lambda) \ve{v} = - (\lambda \ve{v}) $.
      \item $ \lambda \cdot 0_V = \lambda (\ve{v} - \ve{v}) = \lambda \ve{v} + \lambda \cdot (-1_\K) \cdot \ve{v} = \lambda \ve{v} + (-\lambda) \ve{v} = \lambda \ve{v} - (\lambda \ve{v}) = 0_V $
      \item $ \lambda = 0_\K $ is trivial, so consider $ \lambda \neq 0_\K $; then $ \exists ! \lambda^{-1} \in \K : \lambda^{-1} \cdot \lambda = 1_\K $, so $ 0_V = \lambda^{-1} \cdot 0_V = \lambda^{-1} \cdot (\lambda \ve{v}) = (\lambda^{-1} \cdot \lambda) \ve{v} = 1_\K \cdot \ve{v} = \ve{v} $, i.e. $ \ve{v} = 0_V $.
    \end{enumerate}
  \end{proof}
\end{proofbox}

\subsection{Subspaces}

\begin{definition}{Subspace}{}
  Given a $ \K $-vector space $ V $ and a subset $ U \subseteq V : U \neq \emptyset $, then $ U $ is a \bcdef{subspace}\index{subspace} of $ V $ if it is closed under $ + : U \times U \ra U $ and $ \cdot : \K \times U \ra U $.
\end{definition}

\begin{lemma}{}{}
  If $ U $ is a subspace of $ V(\K) $, then $ 0_V \in U $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    By definition $ U \neq \emptyset \implies \exists \ve{v} \in U $. By the closure condition $ \lambda \ve{v} \in U \,\,\forall \lambda \in \K $, hence taking $ \lambda = 0_\K $ proves the thesis.
  \end{proof}
\end{proofbox}

A typical strategy to prove that $ U $ is a subspace of $ V(\K) $ is showing the closure properties, while to prove that it is \emph{not} a subspace we usually show that $ 0_V \notin U $.

\begin{example}{Polynomial subspaces}{}
  Given $ V = \K[x] $, then $ U = \K_n[x] $ is a subspace $ \forall n \in \N_0 $.
\end{example}

An important concept to analyze vector spaces is that of linear combination. Given two sets $ \{\lambda_k\}_{k = 1, \dots, n} \subset \K $ and $ \{\ve{v}_k\}_{k = 1, \dots, n} \subset V $, their \bctxt{linear combination}\index{linear combination} is:
\begin{equation}
  \sum_{k = 1}^n \lambda_k \ve{v}_k = \lambda_1 \ve{v}_1 + \dots \lambda_n \ve{v}_n \in V
\end{equation}

\begin{proposition}{Subspaces and linear combinations}{subspaces-linear-combinations}
  Given a $ \K $-vector space $ V $ and $ U \subset V : U \neq \emptyset $, then $ U $ is a subspace of $ V $ if and only if it is closed under linear combinations, that is:
  \begin{equation*}
    \{\lambda_k\}_{k = 1, \dots, n} \subset \K , \{\ve{v}_k\}_{k = 1, \dots, n} \subset U
    \quad \implies \quad
    \sum_{k = 1}^n \lambda_k \ve{v}_k \in U
  \end{equation*}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    First, note that the general case of linear combinations of $ n $ vectors can be reduced to the case of $ 2 $ vectors.

    $ (\Rightarrow) $ Being $ U $ a subspace, it is closed under $ + : U \times U \ra U $ and $ \cdot : \K \times U \ra U $; then, by definition $ \lambda , \mu \in \K , \ve{v} , \ve{w} \in U \implies \lambda \ve{v} + \mu \ve{w} \in U $.

    $ (\Leftarrow) $ Given $ \lambda \in \K $ and $ \ve{v} , \ve{w} \in V $, then $ \ve{v} + \ve{w} = 1_\K \ve{v} + 1_\K \ve{w} $ and $ \lambda \ve{v} = \lambda \ve{v} + 0_\K \ve{w} $, hence closure under linear combinations implies closure under $ + : U \times U \ra U $ and $ \cdot : \K \times U \ra U $.
  \end{proof}
\end{proofbox}

Generally, it is easier to show closure under linear combinations rather than under addition and scalar multiplication.

\begin{lemma}{Intersection of subspaces}{subspace-intersection}
  Given two subspaces of $ V_1 , V_2 $ of $ V(\K) $, then $ V_1 \cap V_2 $ is still a subset of $ V(\K) $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Being $ V_1 , V_2 $ subspaces, both $ V_1 $ and $ V_2 $ are closed under linear combinations, so $ V_1 \cap V_2 $ is too, as $ \ve{v} \in V_1 \cap V_2 \implies \ve{v} \in V_1 \land \ve{v} \in V_2 $.
  \end{proof}
\end{proofbox}

On the other hand, in general $ V_1 \cup V_2 $ is not a subspace. As a counterexample, consider e.g. $ V = \Vect_0(\mathbb{E}^3) $, the plane $ \pi : z = 0 $ and the line $ r : (x,y,z) = (0,0,t) , t \in \R $; then, consider the subspaces $ V_1 = \Vect_0(\pi) , V_2 = \Vect_0(r) $: their union is clearly not closed under addition, as:
\begin{equation*}
  \begin{pmatrix}
    1 \\ 1 \\ 0
  \end{pmatrix} \in V_1
  ,
  \begin{pmatrix}
    0 \\ 0 \\ 1
  \end{pmatrix} \in V_2
  \qquad \qquad
  \begin{pmatrix}
    1 \\ 1 \\ 0
  \end{pmatrix}
  +
  \begin{pmatrix}
    0 \\ 0 \\ 1
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 \\ 1 \\ 1
  \end{pmatrix} \notin V_1 \cup V_2
\end{equation*}

\begin{definition}{Sum of subspaces}{}
  Given a $ \K $-vector space $ V $ and two subspaces $ V_1 , V_2 $, their \bcdef{sum}\index{subspace!sum of} is defined as:
  \begin{equation*}
    V_1 + V_2 \defeq \{\ve{w} \in V : \ve{w} = \ve{u} + \ve{v}, \ve{u} \in V_1, \ve{v} \in V_2\}
  \end{equation*}
  This is a \bcdef{direct sum}\index{direct sum!of subspaces}, denoted by $ V_1 \oplus V_2 $, if every $ \ve{w} \in V_1 + V_2 $ has a unique representation as $ \ve{w} = \ve{u} + \ve{v} , \ve{u} \in V_1 , \ve{v} \in V_2 $.
\end{definition}

Trivially $ V_1 , V_2 \subseteq V_1 + V_2 $.

\begin{lemma}{Direct sum as disjoint sum}{}
  Given two subspaces $ V_1 , V_2 $ of $ V(\K) $, then $ V_1 + V_2 = V_1 \oplus V_2 \iff V_1 \cap V_2 = \{\ve{0}\} $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ Suppose $ \exists \ve{v} \in V_1 \cap V_2 : \ve{v} \neq \ve{0} $; then $ \ve{v} = \ve{v} + \ve{0} = \ve{0} + \ve{v} $, i.e. the expression of $ \ve{v} \in V_1 + V_2 $, but the expression of $ \ve{v} \in V_1 \oplus V_2 $ must be unique, hence $ \ve{v} = \ve{0} \absurd $

    $ (\Leftarrow) $ Suppose $ \exists \ve{w} \in V_1 + V_2 : \ve{w} = \ve{u}_1 + \ve{v}_1 = \ve{u}_2 + \ve{v}_2 , \ve{u}_1 \neq \ve{u}_2 \in V_1 , \ve{v}_1 \neq \ve{v}_2 \in V_2 $; then $ V_1 \ni \ve{u}_1 - \ve{u}_2 = \ve{v}_2 - \ve{v}_1 \in V_2 \implies \ve{v}_2 - \ve{v}_1 \in V_1 $, so $ \ve{v}_2 - \ve{v}_1 \in V_1 \cap V_2 $, but $ V_1 \cap V_2 = \{\ve{0}\} $, hence $ \ve{v}_2 = \ve{v}_1 $ and idem for $ \ve{u}_1 = \ve{u}_2 \absurd $
  \end{proof}
\end{proofbox}

The sum of subspaces preserves the subspace structure, contrary to the simple union.

\begin{proposition}{Sum as subspace}{}
  Given a $ \K $-vector space and two subspaces $ V_1 , V_2 $, their sum $ V_1 + V_2 $ is still a subspace of $ V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Consider $ \ve{a} , \ve{b} \in V_1 + V_2 $ and define $ \ve{u}_{a,b} \in V_1 , \ve{v}_{a,b} \in V_2 : \ve{a} = \ve{u}_a + \ve{v}_a \land \ve{b} = \ve{u}_b + \ve{v}_b $: as $ V_1 , V_2 $ are subspaces, they are closed under linear combinations, so, given $ \lambda ,\mu \in \K $, then $ \lambda \ve{a} + \mu \ve{b} = (\lambda \ve{u}_a + \mu \ve{u}_b) + (\lambda \ve{v}_a + \mu \ve{v}_b) \equiv \ve{u} + \ve{v} \in V_1 + V_2 $, where $ \ve{u} \in V_1 $ and $ \ve{v} \in V_2 $, which shows that $ V_1 + V_2 $ too is closed under linear combinations and a subspace by \pref{prop:subspaces-linear-combinations}.
  \end{proof}
\end{proofbox}

\subsection{Bases}

To give a more explicit description of vector spaces, we have to define the concept of basis and its properties.

\subsubsection{Generators}

\begin{definition}{Linear dependence}{}
  Given a $ \K $-vector space $ V $ and a set $ \{\ve{v}_j\}_{j = 1, \dots, k} \equiv S \subseteq V $, then the vectors of $ S $ are:
  \begin{itemize}
    \item \bcdef{linearly dependent} (LD) if $ \exists \{\lambda_j\}_{j = 1, \dots, k} \subset \K - \{0\} : \lambda_1 \ve{v}_1 + \dots \lambda_k \ve{v}_k = \ve{0} $
    \item \bcdef{linearly independent}\index{linear independence!of vectors} (LI) if $ \lambda_1 \ve{v}_1 + \dots \lambda_k \ve{v}_k = \ve{0} \iff \lambda_j = 0 \,\,\forall j = 1, \dots, k $
  \end{itemize}
\end{definition}

The generalization to infinite sets is trivial: $ \{\ve{v}_\alpha\}_{\alpha \in \mathcal{I}} \equiv S \subset V(\K) $ is LI if every finite subset of $ S $ is LI, while it is LD if there exists at least one non-empty subset which is LD.

\begin{example}{Complex numbers}{}
  $ \{1, \img\} $ are LD in $ \C(\C) $, as $ 1 \cdot 1 + \img \cdot \img = 0 $, while they are LI in $ \C(\R) $.
\end{example}

\begin{example}{Polynomials}{}
  $ \{1, x, \dots, x^n, \dots\} $ are LI in $ \K[x] $.
\end{example}

We can prove some basic properties of linear dependence.

\begin{lemma}{Basic properties of linear dependence}{}
  Given a $ \K $-vector space $ V $ and $ S \subseteq V : S \neq \emptyset $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item given $ S \subseteq T \subseteq V $, then $ S \text{ LD} \implies T \text{ LD} $
    \item $ S = \{\ve{v}\} \text{ LD} \implies \ve{v} = \ve{0} $
    \item $ S = \{\ve{v}_1 , \ve{v}_2\} \text{ LD} \implies \exists \lambda \in \K : \ve{v}_1 = \lambda \ve{v}_2 $
    \item if $ S = \{\ve{v}_1 , \dots , \ve{v}_n\} $ LD, then at least one $ \ve{v}_i $ is a linear combination of the other vectors
    \item if $ S $ LI and $ S \cup \{\ve{w}\} $ LD, then $ \ve{w} $ is a linear combination of the vectors of $ S $
    \item if $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n = \ve{0} $ and $ \lambda_n \neq 0 $, then $ \ve{v}_n $ is a linear combination of $ \{\ve{v}_1 , \dots , \ve{v}_{n-1}\} $
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item $ S \subseteq T \implies \ve{v} \in T \,\,\forall \ve{v} \in S $, hence $ \{\ve{v}_i\}_{i = 1, \dots, n} \subset S \text{ LD} \implies \{\ve{v}_i\}_{i = 1, \dots, n} \subset T \text{ LD} $
      \item $ \lambda \ve{v} = \ve{0} \iff \lambda = 0 \lor \ve{v} = \ve{0} $, so $ \ve{v} = \ve{0} \implies S \text{ LD} $, while $ S \text{ LD} \implies \lambda \neq 0 \implies \ve{v} = 0 $
      \item $ \{\ve{v}_1 , \ve{v}_2\} \text{ LD} \implies \exists \lambda , \mu \in \K - \{0\} : \lambda \ve{v}_1 + \mu \ve{v}_2 = \ve{0} \iff \ve{v}_1 = \lambda^{-1} \mu \ve{v}_2 $
      \item If $ \{\ve{v}_j\}_{j = 1, \dots, n} $ LD, then by definition $ \exists \{\lambda_j\}_{j = 1, \dots, n} \subset \K - \{0\} : \sum_{j = 1}^n \lambda_j \ve{v}_j = \ve{0} $, hence WLOG $ \ve{v}_1 $ can be isolated as $ \ve{v}_1 = - \lambda_1^{-1} \sum_{j = 2}^n \lambda_j \ve{v}_j $
      \item $ \{\ve{v}_1 , \dots , \ve{v}_n , \ve{w}\} \text{ LD} \implies \exists \lambda_1 , \dots , \lambda_n , \alpha \in \K - \{0\} : \sum_{j = 1}^n \lambda_j \ve{v}_j + \alpha \ve{w} = \ve{0} $, so $ \ve{w} $ can be isolated as $ \ve{w} = - \alpha^{-1} \sum_{j = 1}^n \lambda_j \ve{v}_j $
      \item $ \sum_{j = 1}^n \lambda_j \ve{v}_j = \ve{0} \land \lambda_n \neq 0 \implies \ve{v}_n = - \lambda_n^{-1} \sum_{j = 1}^{n-1} \lambda_j \ve{v}_j $
    \end{enumerate}
  \end{proof}
\end{proofbox}

We can now introduce the notion of generators.

\begin{definition}{Generated subset}{}
  Given a $ \K $-vector space $ V $ and $ \{\ve{v}_\alpha\}_{\alpha \in \mathcal{I}} \equiv S \subseteq V $, the \bcdef{subset generated by $ S $} is the set:
  \begin{equation*}
    \lspan{S} \defeq \{\ve{v} \in V : \exists \lambda_1 , \dots , \lambda_n \in \K, \ve{v}_{\alpha_1} , \dots , \ve{v}_{\alpha_n} \in S : \ve{v} = \lambda_1 \ve{v}_{\alpha_1} + \dots + \lambda_n \ve{\alpha_n}\}
  \end{equation*}
  The elements of $ S $ are called \bcdef{generators} of $ \lspan{S} $.
\end{definition}

We often denote $ \lspan{S} \equiv \braket{S} $: this subset contains all vectors of $ V $ which can be expressed as linear combinations of vectors of $ S $.

\begin{proposition}{Generated subspace}{}
  Given a $ \K $-vector space and $ S \subseteq V : S \neq \emptyset $, then $ \braket{S} $ is a subspace of $ V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Let $ S = \{\ve{s}_\alpha\}_{\alpha \in \mathcal{I}} $ and $ \ve{v} , \ve{w} \in S : \ve{v} = \sum_{j = 1}^k \lambda_j \ve{s}_{\alpha_j} , \ve{w} = \sum_{j = 1}^n \mu_j \ve{s}_{\beta_j} $, with coefficients $ \{\lambda_j\}_{j = 1, \dots, k} , \{\mu_j\}_{j = 1, \dots, n} \subset \K - \{0\} $. Adding vectors with vanishing coefficients, we can rewrite $ \ve{v} $ and $ \ve{w} $ in terms of the same vectors:
    \begin{equation*}
      \ve{v} = \sum_{j = 1}^m a_j \ve{s}_{\gamma_j}
      \qquad \qquad
      \ve{w} = \sum_{j = 1}^m b_j \ve{s}_{\gamma_j}
      \quad \implies \quad
      \zeta \ve{v} + \xi \ve{w} = \sum_{j = 1}^m \left( \zeta a_j + \xi b_j \right) \ve{s}_{\gamma_j} \in \braket{S}
    \end{equation*}
    This shows that $ \braket{S} $ is closed under linear combination, hence the thesis.
  \end{proof}
\end{proofbox}

Note that, give a subspace $ U \subseteq V(\K) $, then at most $ U = \braket{U} $, hence every subspace admits a family of generators. If $ U $ has a finite number of generators, then it is a \bctxt{finitely-generated subspace}: for example, $ \K_n[x] = \braket{1, \dots, x^n} $, $ \C(\C) = \braket{1} $ and $ \C(\R) = \braket{1, \img} $ are finitely-generated.
We can state two trivial properties of generated subsets.

\begin{lemma}{}{}
  Given $ S \subseteq V(\K) $ and $ U = \braket{S} $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item given $ S \subseteq T \subseteq V $, then $ U = \braket{T} $
    \item if $ U = \braket{\ve{s}_1 , \dots , \ve{s}_n} $ and $ \ve{s}_n \in \braket{\ve{s}_1 , \ve{s}_{n-1}} $, then $ U = \braket{\ve{s}_1 , \dots , \ve{s}_{n-1}} $
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item If $ S \subseteq T $, then each linear combination in $ S $ is a linear combination in $ T $ too, hence $ \braket{S} = \braket{T} $
      \item Given $ \ve{v} = \lambda_1 \ve{s}_1 + \dots + \lambda_n \ve{s}_n \in U $ and $ \ve{s}_n = \mu_1 \ve{s}_1 + \dots + \mu_{n-1} \ve{s}_{n-1} $, then $ \ve{v} = \left( \lambda_1 + \mu_1 \right) \ve{s}_1 + \dots + \left( \lambda_{n-1} + \mu_{n-1} \right) \ve{s}_{n-1} $, hence the thesis
    \end{enumerate}
  \end{proof}
\end{proofbox}

\subsubsection{Bases of generic vector spaces}

\begin{definition}{Basis of a vector space}{basis}
  Given a $ \K $-vector space $ V $, a \bcdef{basis} of $ V $ is a LI subset $ \bas \subseteq V : V = \braket{\bas} $.
\end{definition}

Every non-trivial vector space (i.e. $ V \neq \{\ve{0}\} $) admits the existence of a basis, but the proof is non-trivial as it relies on Zorn's Lemma (or equivalently to the Axiom of Choice).

\begin{theorem}{Basis theorem}{basis}
  Every non-trivial vector space admits a basis.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    First, we prove that every LI subset of $ V $ can be extended to a basis of $ V $. Let $ A \subseteq V $ be a non-empty LI subset of $ V $, and define $ \mathscr{S} $ the collection of all LI supersets of $ A $.

    \begin{lemma}{}{}
      Given a chain $ \{A_\alpha\}_{\alpha \in \mathcal{I}} \subseteq \mathscr{S} : A_1 \subseteq A_2 \subseteq \dots $, then $ \bigcup_{\alpha \in \mathcal{I}} A_\alpha \in \mathscr{S} $.
    \end{lemma}

    \begin{proofbox}
      \begin{proof}
        Set $ \mathcal{A} \equiv \bigcup_{\alpha \in \mathcal{I}} A_\alpha $. If $ A \subseteq A_\alpha \,\,\forall \alpha \in \mathcal{I} $, then trivially $ A \subseteq \mathcal{A} $. To prove the linear independence, consider a linear combination $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n $ in $ \mathcal{A} $, with $ n \in \N $, and choose an $ A_{\alpha_n} $ large enough so that $ v_1 , \dots , v_n \in A_{\alpha_n} $. Then, $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n = \ve{0} \implies \lambda_1 , \dots , \lambda_n = 0 $, as $ A_{\alpha_n} $ is LI by definition. Since $ n \in \N $ is generic, $ \mathcal{A} $ is LI.
      \end{proof}
    \end{proofbox}

    It is then clear that $ \mathscr{S} $ satisfies the hypotheses of Zorn's Lemma (\lref{lemma:zorn}), therefore it has a maximal element $ \bas $. Now, suppose $ \braket{\bas} \neq V $, i.e. $ \exists \ve{b} \in V - \braket{\bas} $, and consider the linear combination $ \mu \ve{b} + \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{b}_n = \ve{0} $, with $ \ve{v}_1 , \dots , \ve{v}_n \in \bas $ and $ n \in \N $: then $ - \mu \ve{b} \in \braket{\bas} $, but $ \ve{b} \notin \braket{\bas} $, so $ \mu = 0 $ (as $ \ve{b} \neq \ve{0} \in \braket{\bas} $). Consequently, $ \lambda_1 = \dots = \lambda_n = 0 $ as $ \bas $ is LI, thus $ \bas \cup \{\ve{b}\} $ is LI and a superset of $ \bas \in \mathscr{S} $, which contradicts $ \bas $ being a maximal element of $ \mathscr{S} \absurd $

    Having showed that every LI subset $ A \subseteq V $ can be extended to a basis $ \bas $ of $ V $, the thesis is trivially found taking $ A = \emptyset $, which is a subset of every non-trivial vector space.
  \end{proof}
\end{proofbox}

This, though trivial for finite-dimensional spaces, is quite impressive for infinite-dimensional ones (for dimensionality, see SECTION).

\begin{proposition}{}{}
  Given a $ \K $-vector space $ V $, then $ S \subseteq V $ is a basis of $ V $ if and only if every element of $ V $ has a unique representation as a linear combination of elements of $ S $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Note that two representations are equal if they differ only by vanishing coefficients.

    $ (\Rightarrow) $ As $ V = \braket{S} $, then every $ \ve{v} \in V $ can be written as a linear combination of elements of $ S $. Suppose that $ \ve{v} $ has two representations:
    \begin{equation*}
      \ve{v} = \lambda_1 \ve{s}_1 + \dots \lambda_n \ve{s}_n
      \qquad \qquad
      \ve{v} = \mu_1 \ve{t}_1 + \dots + \mu_m \ve{t}_m
    \end{equation*}
    with $ \{\ve{s}_j\}_{j = 1, \dots, n} , \{\ve{t}_k\}_{k = 1, \dots, m} \subseteq S $ and $ \{\lambda_j\}_{j = 1, \dots, n} , \{\mu_k\}_{k = 1, \dots, m} \subseteq \K $. Now, we can extend both representations by adding vanishing coefficients, so that both include the same vectors of $ S $:
    \begin{equation*}
      \ve{v} = \zeta_1 \ve{v}_1 + \dots + \zeta_r \ve{v}_r
      \qquad \qquad
      \ve{v} = \xi_1 \ve{v}_1 + \dots + \xi_r \ve{v}_r
    \end{equation*}
    with $ \{\ve{v}_j\}_{j = 1, \dots, r} \subseteq S $ and $ \{\zeta_j\}_{j = 1, \dots, r} , \{\xi_j\}_{j = 1, \dots, r} \subseteq \K $. Subtracting these two expressions:
    \begin{equation*}
      \ve{0} = \left( \zeta_1 - \xi_1 \right) \ve{v}_1 + \dots + \left( \zeta_r - \xi_r \right) \ve{v}_r
    \end{equation*}
    But $ S $ is LI, hence $ \zeta_j = \xi_j \,\,\forall j = 1, \dots, r $, i.e. the two representations are equal.

    $ (\Leftarrow) $ As every $ \ve{v} \in V $ can be written as a linear combination of elements of $ S $, then $ V = \braket{S} $. We only have to prove that $ S $ is LI. Consider $ \ve{0} \in V $: by hypothesis, it has a unique representation as a linear combination of vectors in $ S $, and a possible representation is $ \ve{0} = 0 \cdot \ve{s} $ for some $ \ve{s} \in S $, i.e. the trivial representation with all vanishing coefficients. Now, consider a linear combination in $ S $:
    \begin{equation*}
      \lambda_1 \ve{s}_1 + \dots + \lambda_n \ve{s}_n = \ve{0}
    \end{equation*}
    with $ n \in \N $. This too is a representation of $ \ve{0} $, hence $ \lambda_j = 0 \,\,\forall j = 1, \dots, n $ by the uniqueness of the representation. As $ n \in \N $ is generic, this is the definition of $ S $ being LI.
  \end{proof}
\end{proofbox}

\subsubsection{Bases of finitely-generated vector spaces}

We now turn our attention to finitely-generated vector spaces, i.e. $ V = \braket{\ve{v}_1 , \dots , \ve{v}_n} $ with $ n \in \N $.

\begin{proposition}{}{}
  Given a $ \K $-vector space $ V = \braket{\ve{v}_1 , \dots , \ve{v}_n} $, then $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ contains a basis of $ V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    If $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ is LI, then it is a basis of $ V $, so consider $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ LD, i.e. $ \exists \ve{v} \in \braket{\{\ve{v}_1 , \dots , \ve{v}_n\} - \{\ve{v}\}} $. WLOG, consider $ \ve{v} = \ve{v}_n $, so that $ V = \braket{\ve{v}_1 , \dots , \ve{v}_{n-1}} $: reiterating this procedure, all LD vectors are eliminated, leaving a basis of $ V $, as at most only a single vector $ \ve{v}_1 $ remains ($ \ve{v}_1 \neq \ve{0} $ as it is LI).
  \end{proof}
\end{proofbox}

A direct corollary is that every finitely-generated vector space admits a finite basis, found by the elimination algorithm highlighted in the previous proof.

\begin{definition}{MSLIV}{}
  Given a $ \K $-vector space $ V $, then a LI subset $ \{\ve{v}_1 , \dots , \ve{v}_n\} \subseteq V $ is a \bcdef{maximal set of linearly-independent vectors} (MSLIV) if $ \{\ve{v}_1 , \dots , \ve{v}_n\} \cup \{\ve{v}\} $ is LD $ \forall \ve{v} \in V $.
\end{definition}

We extend this notion considering $ V = \{\ve{v}_1 , \dots , \ve{v}_n\} $: then, a LI subset $ \{\ve{v}_{j_1} , \dots , \ve{v}_{j_r}\} \subseteq \{\ve{v}_1 , \dots , \ve{v}_n\} $, with $ r \leq n $, is a \bctxt{maximal subset of linearly-independent vectors} (MSLIV) if $ \{\ve{v}_{j_1} , \dots , \ve{v}_{j_r}\} \cup \{\ve{v}_j\} $ is LD $ \forall j \in \{1, \dots, n\} - \{j_1, \dots, j_r\} $. Trivially, a maximal subset of LI vectors is also a maximal set of LI vectors in $ V $, so the redundant acronym MSLIV is justified.
We can now prove that bases and MSLIVs are equivalent notions.

\begin{theorem}{Bases as MSLIVs}{basis-msliv}
  Given a non-trivial $ \K $-vector space $ V = \braket{\ve{v}_1 , \dots , \ve{v}_n} $, then $ \bas \subseteq V $ is a basis if and only if it is a MSLIV.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    $ (\Leftarrow) $ WLOG let $ \{\ve{v}_1 , \dots , \ve{v}_r\} $, with $ r \leq n $, be a MSLIV of $ \{\ve{v}_1 , \dots , \ve{v}_n\} $: then WTS $ V = \braket{\ve{v}_1 , \dots , \ve{v}_r} $. If $ r = n $ the proof is complete, so consider $ r < n $ and $ \ve{v}_j : r < j \leq n $: by definition $ \{\ve{v}_1 , \dots , \ve{v}_r\} \cup \{\ve{v}_j\} $ is LD, i.e. $ \exists \{\lambda_{j_k}\}_{k = 1, \dots, r} \subseteq \K : \ve{v}_i = \lambda_{j_1} \ve{v}_1 + \dots + \lambda_{j_r} \ve{v}_r $, which means that $ \ve{v}_i \in \braket{\ve{v}_1 , \dots , \ve{v}_r} \implies V = \braket{\{\ve{v}_1 , \dots , \ve{v}_n\} - \{\ve{v}_i\}} $. This holds $ \forall i \in [r+1, n] \subseteq \N $, hence $ V = \braket{\ve{v}_1 , \dots , \ve{v}_r} $.

    $ (\Rightarrow) $ Let $ \bas = \{\ve{v}_1 , \dots , \ve{v}_n\} $ be a basis of $ V $and $ \{\ve{w}_1 , \dots , \ve{w}_m\} \ssq V : m > n $, and suppose this is LI. By definition $ \exists \lambda_1 , \dots , \lambda_n \in \K : \ve{w}_1 = \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n $, but $ \ve{w}_1 $ is LI, therefore $ \exists j \in [1, \dots, n] \subseteq \N : \lambda_j \neq 0 $. WLOG $ j = 1 $, hence $ \ve{v}_1 \in \braket{\ve{w}_1 , \ve{v}_2 , \dots , \ve{v}_n} $. Iterating, we can substitute $ \ve{v}_1 , \dots , \ve{v}_n $ with $ \ve{w}_1 , \dots , \ve{w}_n $: indeed, supposing that $ \{v_1 , \dots , \ve{v}_r\} $ have been substituted with $ \ve{w}_1 , \dots , \ve{w}_r $, with $ 1 \leq r < n $, then $ \ve{v}_{r+1} $ can be substituted with $ \ve{w}_{r+1} $ as $ V = \braket{\ve{w}_1 , \dots , \ve{w}_r , \ve{v}_{r+1} , \dots , \ve{v}_n} \implies \exists \alpha_1 , \dots , \alpha_r , \beta_{r+1} , \dots , \beta_n \in \K : \ve{w}_{r+1} = \alpha_1 \ve{w}_1 + \dots + \alpha_r \ve{w}_r + \beta_{r+1} \ve{v}_{r+1} + \dots + \beta_n \ve{v}_n $, but $ \{\ve{w}_1 , \dots , \ve{w}_{r+1}\} $ are LI, thus $ \exists j \in [r+1 , n] \subseteq \N : \beta_j \neq 0 $, and WLOG $ j = r+1 $ by reordering indices. Performing the reiteration $ V = \braket{\ve{w}_1 , \dots , \ve{w}_n} $, so $ \ve{w}_{n+1} $ is a linear combination of $ \{\ve{w}_1 , \dots , \ve{w}_n\} \absurd $
  \end{proof}
\end{proofbox}

There is still another equivalent concept to introduce.

\begin{definition}{MSG}{}
  Given a $ \K $-vector space $ V $, then $ \{\ve{v}_1 , \dots , \ve{v}_n\} \ssq V $ is a \bcdef{minimal set of generators} (MSG) if $ V = \braket{\ve{v}_1 , \dots , \ve{v}_n} $ and $ \{\ve{v}_1 , \dots , \ve{v}_n\} - \{\ve{v}_j\} $ does not generate $ V \,\,\forall j = 1, \dots , n $.
\end{definition}

\begin{theorem}{Bases ad MSGs}{basis-msg}
  Given a non-trivial $ \K $-vector space $ V $, then $ \bas \ssq V $ is a basis of $ V $ if and only if it is a MSG.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    $ (\Leftarrow) $ Let $ \{\ve{v}_1 , \dots , \ve{v}_n\} \ssq V $ be a MSG: then WTS $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ is LI. Consider a linear combination $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n = \ve{0} $ and suppose $ \lambda_1 \neq 0 $: this allows to express $ \ve{v}_1 $ as a linear combination of $ \{\ve{v}_2 , \dots , \ve{v}_n\} $, but then $ V = \braket{\ve{v}_2 , \dots , \ve{v}_n} \absurd $

    $ (\Rightarrow) $ Suppose $ \bas = \{\ve{v}_1 , \dots , \ve{v}_n\} $ is not a MSG, and WLOG $ V = \braket{\ve{v}_2 , \dots , \ve{v}_n} $: then $ \ve{v}_1 $ can be expressed as linear combination of $ \{\ve{v}_2 , \dots , \ve{v}_n\} $, i.e. $ \bas $ is LD $ \absurd $
  \end{proof}
\end{proofbox}

This shows that bases, MSLIVs and MSGs are all equivalent notions.

\subsubsection{Dimensionality}

To properly define the concept of dimensionality of a vector space, we first have to prove that all bases are equivalent.

\begin{theorem}{Equicardinality of bases}{basis-equicardinality}
  Given a non-trivial $ \K $-vector space $ V $ and two bases $ \bas_1 = \{\ve{v}_1 , \dots , \ve{v}_n\} , \bas_2 = \{\ve{w}_1 , \dots , \ve{w}_m\} $, then $ n = m $.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    As $ \bas_1 $ is a MSLIV by \tref{th:basis-msliv}, then every subset of $ n + 1 $ vectors in $ V $ is LD, hence $ m \leq n $ as $ \bas_2 $ must be LI. The vice versa applies too, hence $ n = m $.
  \end{proof}
\end{proofbox}

By this theorem, all bases of finitely-generated spaces are equivalent, since the equicardinality ensures that we can define a bijection $ f : \bas_1 \leftrightarrow \bas_2 \,\,\forall \bas_1 , \bas_2 $ bases of $ V $.

Moreover, this result hints to the fact that the cardinality of the bases of $ V $ is a fundamental property of the vector space, linked to hits dimensionality, so we give a proper definition of this quantity.

\begin{definition}{Dimension}{}
  Given a $ \K $-vector space $ V $, then we define its \bcdef{dimension} as:
  \begin{equation*}
    \dim_\K V \defeq
    \begin{cases}
      0 & V = \{\ve{0}\} \\
      n & \abs{\bas} = n \,\,\forall \bas \text{ basis of } V \\
      \infty & V \text{ not finitely-generated}
    \end{cases}
  \end{equation*}
\end{definition}

The dimension of a vector space is a well-defined quantity by \tref{th:basis} and \tref{th:basis-equicardinality}.

\begin{example}{Various spaces}{}
  Trivially, $ \dim_\K \K^n = n $, so $ \dim_\C \C^n = n $ and $ \dim_\R \C^n = 2n $, while $ \dim_\R \R^\R = \infty $.
\end{example}

We can now give some trivial properties of dimensionality.

\begin{lemma}{Basic property of dimension}{}
  Given an $ n $-dimensional $ \K $-vector space $ V $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item $ \{\ve{v}_1 , \dots , \ve{v}_m\} \ssq V $ is LD $ \forall m > n $
    \item $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ LI is a basis of $ V $
    \item $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ set of generators of $ V $ is a basis of $ V $
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    These results are corollaries of \tref{th:basis-msliv} and \tref{th:basis-msg}.
  \end{proof}
\end{proofbox}

\begin{proposition}{Dimension of subspaces}{}
  Given $ \dim_\K V = n $ and a subspace $ U \ssq V $, then $ \dim_\K U \equiv k \leq n $ and $ k = n \iff U = V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    The case $ U = \{\ve{0}\} $ is trivial, so consider $ U \neq \{\ve{0}\} $. Let $ \ve{u}_1 \in U $ LI and add $ \ve{u}_2 , \ve{u}_3 , \dots \in U $ to get $ \{\ve{u}_1 , \ve{u}_2\} , \{\ve{u}_1 , \ve{u}_2 , \ve{u}_3\} , \dots $: a LD subset is reached in at most $ n $ steps. Let WLOG $ \{\ve{u}_1 , \dots , \ve{u}_k\} $ the MSLIV of $ \{\ve{u}_1 , \dots , \ve{u}_n\} $, with $ k \leq n $: by \tref{th:basis-msliv}, this is a basis of $ U $, hence $ k = \dim_\K U \leq n $.

    $ U = V \implies k = n $ is trivial, while $ k = n \implies \{\ve{u}_1 , \dots , \ve{u}_n\} $ is a MSLIV of $ V $, hence a basis of $ V $, so $ V = \braket{\ve{u}_1 , \dots , \ve{u}_n} = V $.
  \end{proof}
\end{proofbox}

A consequence of this theorem is the fact that LI subset $ \{\ve{v}_1 , \dots , \ve{v}_r\} \ssq V $, with $ r < n $, can always be completed to a basis, i.e. $ \exists \ve{w}_{r+1} , \dots , \ve{w}_n \in V : \{\ve{v}_1 , \dots, \ve{v}_r , \ve{w}_{r+1} , \ve{w}_n\} $ is a basis of $ V $.

\begin{theorem}{Grassmann's Theorem}{}
  Given a $ \K $-vector space $ V $ and finitely-generated subspaces $ X,Y \ssq V $, then:
  \begin{equation}
    \dim_\K X + \dim_\K Y = \dim_\K \left( X + Y \right) + \dim_\K \left( X \cap Y \right)
  \end{equation}
\end{theorem}

\begin{proofbox}
  \begin{proof}
    Let $ \bas_X = \{\ve{x}_1 , \dots , \ve{x}_r\} , \bas_Y = \{\ve{y}_1 , \dots , \ve{y}_s\} $ be bases of $ X , Y $ and $ m \equiv \dim_\K \left( X \cap Y \right) $.
    If $ m = 0 $, then $ X \cap Y = \{\ve{0}\} $, while if $ m \ge 1 $ let $ \bas_{XY} = \{\ve{v}_1 , \dots , \ve{v}_m\} $ be a basis of $ X \cap Y $, which is a finitely-generated subspace by \lref{lemma:subspace-intersection}. Then, completing the bases, $ \exists \ve{x}_{m+1} , \dots, \ve{x}_r \in X : \{\ve{v}_1 , \dots , \ve{v}_m , \ve{x}_{m+1} , \dots , \ve{x}_r\} $ is a basis of $ X $ and $ \exists \ve{y}_{m+1} , \dots , \ve{y}_s \in Y : \{\ve{v}_1 , \dots , \ve{v}_m , \ve{y}_{m+1} , \dots , \ve{y}_s\} $ is a basis of $ Y $ (WLOG same vectors as in $ \bas_X $ and $ \bas_Y $). Now, WTS $ \dim_\K \left( X + Y \right) = r + s - m $, so consider $ \bas = \{\ve{v}_1 , \dots , \ve{v}_m , \ve{x}_{m+1} , \dots , \ve{x}_r , \ve{y}_{m+1} , \dots , \ve{y}_s\} $:
    \begin{itemize}
      \item $ X + Y \defeq \{\ve{v} = \ve{x} + \ve{y} : \ve{x} \in X , \ve{y} \in Y\} $, but $ \ve{x} \in \braket{\ve{v}_1 , \dots , \ve{v}_m , \ve{x}_{m+1} , \dots , \ve{x}_r} $ and $ \ve{y} \in \braket{\ve{v}_1 , \dots , \ve{v}_m , \ve{y}_{m+1} , \dots , \ve{y}_s} $, so $ \ve{x} + \ve{y} \in \braket{\ve{v}_1 , \dots , \ve{v}_m , \ve{x}_{m+1} , \dots , \ve{x}_r , \ve{y}_{m+1} , \dots , \ve{y}_s} $, i.e. $ X + Y = \braket{\bas} $;
      \item consider the following linear combination:
        \begin{equation*}
          \alpha_1 \ve{v}_1 + \dots + \alpha_m \ve{v}_m + \beta_{m+1} \ve{x}_{m+1} + \dots + \beta_r \ve{x}_r + \gamma_{m+1} \ve{y}_{m+1} + \dots + \gamma_s \ve{y}_s = \ve{0}
        \end{equation*}
        and rearrange it as:
        \begin{equation*}
          \underbrace{\alpha_1 \ve{v}_1 + \dots + \alpha_m \ve{v}_m + \beta_{m+1} \ve{x}_{m+1} + \dots + \beta_r \ve{x}_r}_{\in \, X} = \underbrace{- \gamma_{m+1} \ve{y}_{m+1} - \dots - \gamma_s \ve{y}_s}_{\in \, Y}
        \end{equation*}
        Therefore, both expressions are in $ X \cap Y = \braket{\ve{v}_1 , \dots , \ve{v}_m} $, hence \exists $ \delta_1, \dots, \delta_m \in \K $ such that:
        \begin{equation*}
          \delta_1 \ve{v}_1 + \dots + \delta_m \ve{v}_m + \gamma_{m+1} \ve{y}_{m+1} + \dots + \gamma_s \ve{y}_s = \ve{0}
        \end{equation*}
        But $ \bas_Y $ is a basis of $ Y $, i.e. LI, so $ \delta_1 = \dots = \delta_m = \gamma_{m+1} = \dots = \gamma_s = 0 $, thus:
        \begin{equation*}
          \alpha_1 \ve{v}_1 + \dots + \alpha_m \ve{v}_m + \beta_{m+1} \ve{x}_{m+1} + \dots + \beta_r \ve{x}_r = \ve{0}
        \end{equation*}
        But $ \bas_X $ is a basis of $ X $, i.e. LI, so $ \alpha_1 = \dots = \alpha_m = \beta_{m+1} = \dots = \beta_r = 0 $. This shows that $ \bas $ is LI.
    \end{itemize}
    By \dref{def:basis}, $ \bas $ is a basis of $ X + Y $, i.e. $ \dim_\K \left( X + Y \right) = r + s - m $.
  \end{proof}
\end{proofbox}

\begin{example}{Eucldean geometry}{}
  Consider $ V = \Vect_0(\mathbb{E}^3) $ and $ \alpha , \beta $ planes such that $ \ve{0} \in \alpha , \beta $: they then determine a line $ r \equiv \alpha \cap \beta \ni \{\ve{0}\} $. Setting $ X = \Vect_0(\alpha) $, $ Y = \Vect_0(\beta) $ and $ X \cap Y = \Vect_0(r) $, we correctly have $ 2 + 2 = 3 + 1 $.
\end{example}

\section{Linear applications}

\begin{definition}{Linear application}{}
  Given $ \K $-vector spaces $ V,W $, an application $ f : V \ra W $ is \bcdef{$ \K $-linear} if:
  \begin{equation*}
    f(\lambda \ve{v} + \mu \ve{w}) = \lambda f(\ve{v}) + \mu f(\ve{w})
    \quad
    \forall \lambda , \mu \in \K , \ve{v} , \ve{w} \in V
  \end{equation*}
\end{definition}

This condition means that $ \K $-linear applications preserve linear combinations.

\begin{example}{Matrices}{}
  Given $ \mt{A} \in \K^{m \times n} $, we can associate to it an application $ L_\mt{A} : \K^n \ra \K^m : \ve{v} \mapsto \mt{A} \ve{v} $, which is $ \K $-linear by the linearity of the matrix product. Note that $ L_{\mt{I}_n} = \id_{\K^n} $.

  Moreover, given $ \mt{A} \in \K^{m \times n} $ and $ \mt{B} \in \K^{n \times p} $, then $ L_\mt{A} \circ L_\mt{B} = L_{\mt{A} \cdot \mt{B}} : \K^p \ra \K^m $ by the following commutative diagram:
  \begin{equation*}
    \begin{tikzcd}
      \K^m \arrow[rd, "L_{\mt{A} \cdot \mt{B}}", swap] \arrow[r, "L_{\mt{A}}"]
      & \K^n \arrow[d, "L_{\mt{B}}"] \\
      & \K^p
    \end{tikzcd}
  \end{equation*}
\end{example}

We can now state some properties of linear applications.

\begin{lemma}{Basic properties of linear applications}{}
  Given $ \K $-vector spaces $ V,W,Z $ and $ \K $-linear applications $ f : V \ra W , g : W \ra Z $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item $ f(\ve{0}_V) = \ve{0}_W $
    \item $ g \circ f : V \ra Z $ is $ \K $-linear
    \item $ f $ is bijective $ \implies f^{-1} : W \ra V $ is $ \K $-linear
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item $ f(\ve{0}_V) = f(0_\K \cdot \ve{v}) = 0_\K \cdot f(\ve{v}) = \ve{0}_W $
      \item $ g \circ f (\lambda \ve{u} + \mu \ve{v}) = g(\lambda f(\ve{u}) + \mu f(\ve{v})) = \lambda g(f(\ve{u})) + \mu g(f(\ve{v})) $
      \item $ f(\lambda f^{-1}(\ve{u}) + \mu f^{-1}(\ve{v})) = \lambda \ve{u} + \mu \ve{v} \implies f^{-1}(\lambda \ve{u} + \mu \ve{v}) = \lambda f^{-1}(\ve{u}) + \mu f^{-1}(\ve{v}) $
    \end{enumerate}
  \end{proof}
\end{proofbox}

We can also prove an existence-uniqueness theorem for linear applications.

\begin{theorem}{Existence and uniqueness}{}
  Let $ V,W $ be $ \K $-vector spaces, $ \bas = \{\ve{b}_1 , \dots , \ve{b}_n\} $ a basis of $ V $ and $ \{\ve{w}_1 , \dots , \ve{w}_n\} \ssq W $ an ordered set of vectors. Then $ \exists ! \varphi : V \ra W : \varphi(\ve{b}_j) = \ve{w}_j \,\,\forall j = 1, \dots , n $ which is $ \K $-linear.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    Let $ \ve{v} \in V $; then $ \exists \alpha_1 , \dots , \alpha_n \in \K : \ve{v} = \alpha_1 \ve{b}_1 + \dots + \alpha_n \ve{b}_n $ fixed since $ \bas $ is a basis. Now, define $ \varphi : V \ra W : \varphi(\ve{v}) = \alpha_1 \ve{w}_1 + \dots \alpha_n \ve{w}_n $: clearly $ \varphi(\ve{b}_j) = \ve{w}_j \,\,\forall j = 1, \dots, n $, and also $ \varphi $ is unique since both $ \{\alpha_j\}_{j = 1, \dots, n} \ssq \K $ and $ \{\ve{w}_j\}_{j = 1 , \dots, n} \ssq W $ are fixed. Finally, $ \varphi $ is $ \K $-linear, since $ f(\lambda \ve{v}_1 + \mu \ve{v}_2) = \left( \lambda \alpha_1 + \mu \beta_1 \right) \ve{w}_1 + \dots + \left( \lambda \alpha_n + \mu \beta_n \right) \ve{w}_n = \lambda f(\ve{v}_1) + \mu f(\ve{v}_2) $.
  \end{proof}
\end{proofbox}

In general, fixed $ \dim_\K V = n $, then given two sets $ \{\ve{v}_1 , \dots , \ve{v}_k\} \ssq V $ and $ \{\ve{w}_1 , \dots , \ve{w}_k\} \ssq W $, with $ k \in \N $, then the existence of $ \varphi : V \ra W : \varphi(\ve{v}_j) = \ve{w}_j \,\,\forall j = 1, \dots , k $ is only granted if $ \{\ve{v}_1 , \dots , \ve{v}_k\} $ is LI: in this case, if $ n = k $ then $ \varphi $ is unique too, by the previous theorem, while if $ k < n $ in general we can define multiple $ \varphi $ with such property, as we can complete $ \{\ve{v}_1 , \dots , \ve {v}_k\} $ to a basis of $ V $, which can then be mapped to arbitrary vectors in $ W $. On the other hand, if $ \{\ve{v}_1 , \dots , \ve{v}_k\} $ is LD, then $ \varphi $ can be defined only if $ \{\ve{w}_1 , \dots , \ve{w}_k\} $ satisfies the same linear-dependence relations, otherwise linearity cannot be satisfied.

Given two $ \K $-vector space $ V $ and $ W $, we denote the set of all $ \K $-linear applications $ f : V \ra W $ as $ \hmk(V,W) $: this has a natural structure of $ \K $-vector space with $ (f + g)(\ve{v}) \equiv f(\ve{v}) + g(\ve{v}) $ and $ (\lambda \cdot f)(\ve{v}) = \lambda \cdot f(\ve{v}) $.

\begin{definition}{Kernel and image}{}
  Given $ f \in \hmk(V,W) $, its \bcdef{kernel} is defined as $ \ker{f} \defeq \{\ve{v} \in V : f(\ve{v}) = \ve{0}_W\} \ssq V $, while its \bcdef{image} (or range) is defined as $ \ran{f} \defeq \{\ve{w} \in W : \exists \ve{v} \in V : \ve{w} = f(\ve{v})\} \ssq W $.
\end{definition}

\begin{lemma}{Kernel and image as subspaces}{}
  Given $ f \in \hmk(V,W) $, then $ \ker{f} $ is a subspace of $ V $ and $ \ran{f} $ is a subspace of $ W $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    By the linearity of $ f $, given $ \ve{v} , \ve{v}' \in V $ and $ \ve{w} , \ve{w}' \in W $:
    \begin{equation*}
      f(\lambda \ve{v} + \mu \ve{v}') = \lambda f(\ve{v}) + \mu f(\ve{v}') = \lambda \cdot \ve{0}_W + \mu \ve{0}_W = \ve{0}_\ve{w}
    \end{equation*}
    \begin{equation*}
      \ran{f} \ni \lambda \ve{w} + \mu \ve{w}' = \lambda f(\ve{v}) + \mu f(\ve{v}') = f(\lambda \ve{v} + \mu \ve{v}')
    \end{equation*}
    Thus, both $ \ker{f} $ and $ \ran{f} $ are closed under linear combinations, i.e. vector spaces.
  \end{proof}
\end{proofbox}

We can further carachterize the kernel and the image of a linear application.

\begin{proposition}{Kernel and injections}{kernel-injections}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces and $ f \in \hmk(V,W) $. Then the following conditions are equivalent:
  \begin{enumerate}[label = {\alph*.}]
    \item $ f $ is injective
    \item $ \ker{f} = \{\ve{0}_V\} $
    \item $ \{\ve{v}_1 , \dots , \ve{v}_k\} \ssq V \text{ LI } \implies \{f(\ve{v}_1 , \dots , f(\ve{v}_k)\} \ssq W \text{ LI} $
  \end{enumerate}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Consider the following implications:

    $ (\text{a} \Rightarrow \text{b}) $ Suppose $ \exists \ve{v} \in \ker{f} : \ve{v} \neq \ve{0}_V $; then $ f(\ve{v}) = \ve{0}_W = f(\ve{0}_V) $, but $ f $ is injective $ \absurd $

    $ (\text{b} \Rightarrow \text{c}) $ Let $ \{\ve{v}_1 , \dots , \ve{v}_k\} \ssq V $ LI and consider $ \ve{0}_W = \lambda_1 f(\ve{v}_1) + \dots + \lambda_k f(\ve{v}_k) = f(\lambda_1 \ve{v}_1 + \dots + \lambda_k \ve{v}_k) $, hence $ \lambda_1 \ve{v}_1 + \dots + \lambda_k \ve{v}_k = \ve{0}_V $ as $ \ker{f} = \{\ve{0}_V\} $, therefore $ \lambda_1 = \dots = \lambda_k = 0 $ as $ \{\ve{v}_1 , \dots , \ve{v}_k\} $ LI

    $ (\text{c} \Rightarrow \text{a}) $ Given $ \ve{v}_1 , \ve{v}_2 \in V $, by linearity $ f(\ve{v}_1) = f(\ve{v}_2) \implies f(\ve{v}_1 - \ve{v}_2) = \ve{0}_W $, so suppose $ \ve{v}_1 \neq \ve{v}_2 $: then $ \ve{v} \equiv \ve{v}_1 - \ve{v}_2 \neq \ve{0}_V $, i.e. LI, but $ f(\ve{v}) = \ve{0}_W $, i.e. LD $ \absurd $
  \end{proof}
\end{proofbox}

\begin{proposition}{Image and surjections}{image-surjections}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces and $ f \in \hmk(V,W) $. Then the following conditions are equivalent:
  \begin{enumerate}[label = {\alph*.}]
    \item $ f $ is surjective
    \item $ \ran{f} = W $
    \item $ V = \braket{\ve{v}_1 , \dots , \ve{v}_k} \implies W = \braket{f(\ve{v}_1) , \dots , f(\ve{v}_k)} $
  \end{enumerate}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Consider the following implications:

    $ (\text{a} \Rightarrow \text{b}) $ Suppose $ \ran{f} \ssnq W \implies \exists \ve{w} \in W : \nexists \ve{v} \in V : \ve{w} = f(\ve{v}) \implies f $ not surjective $ \absurd $

    $ (\text{b} \Rightarrow \text{c}) $ $ \ran{f} = W \implies \forall \ve{w} \in W \,\exists \ve{v} \in V : \ve{w} = f(\ve{v}) $; moreover, $ V = \braket{\ve{v}_1 , \dots , \ve{v}_k} \implies \forall \ve{v} \in V \,\exists \lambda_1 , \dots , \lambda_k \in \K : \ve{v} = \lambda_1 \ve{v}_1 + \dots + \lambda_k \ve{v}_k $. Then $ \forall \ve{w} \in W \,\exists \lambda_1 , \dots , \lambda_k \in \K : \ve{w} = \lambda_1 f(\ve{v}_1) + \dots + \lambda_k f(\ve{v}_k) $, i.e. $ W = \braket{f(\ve{v}_1) , \dots , f(\ve{v}_k)} $

    $ (\text{c} \Rightarrow \text{a}) $ $ W = \braket{f(\ve{v}_1) , \dots , f(\ve{v}_k)} \implies \forall \ve{w} \in W \,\exists \lambda_1 , \dots , \lambda_k \in \K : \ve{w} = \lambda_1 f(\ve{v}_1) + \dots + \lambda_k f(\ve{v}_k) = f(\lambda_1 \ve{v}_1 + \dots + \lambda_k \ve{v}_k) $, but $ V = \braket{\ve{v}_1 , \dots , \ve{v}_k} $, hence $ \forall \ve{w} \in W \,\exists \ve{v} \in V : \ve{w} = f(\ve{v}) $
  \end{proof}
\end{proofbox}

In general, even for non-surjective $ f \in \hmk $, it is true that $ V = \braket{\ve{v}_1 , \dots , \ve{v}_k} \implies \ran{f} = \braket{f(\ve{v}_1) , \dots , f(\ve{v}_k)} $ with a reasoning analogous to the previous proof.

As injections map LI vectors to LI vectors and surjections map generators to generators, we see that bijections map bases to bases.

\begin{theorem}{Rank--nullity theorem}{rank-nullity}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces and $ f \in \hmk(V,W) $. Then:
  \begin{equation}
    \dk V = \dk \ket{f} + \dk \ran{f}
  \end{equation}
\end{theorem}

\begin{proofbox}
  \begin{proof}
    As $ \ker{f} \ssq V $ and $ \ran{f} \ssq W $, they are both finitely-generated. If $ \ran{f} = \{\ve{0}_W\} $ (trivial map), then $ \ker{f} = V $ and the thesis is verified.

    Consider $ \ran{f} \neq \{\ve{0}_W\} $ and choose a basis $ \{\ve{c}_1 , \dots , \ve{c}_k\} $ of $ \ran{f} $: this means that $ \exists \ve{b}_1 , \dots , \ve{b}_k \in V : f(\ve{b}_j) = \ve{c}_j \,\,\forall j = 1, \dots, k $. Now, if $ \ker{f} \neq \{\ve{0}_V\} $ choose a basis $ \{\ve{a}_1 , \dots , \ve{a}_r\} $ of $ \ker{f} $, otherwise consider no other vectors, and set $ \bas \equiv \{\ve{a}_1 , \dots , \ve{a}_r , \ve{b}_1 , \dots , \ve{b}_k\} \ssq V $. WTS $ \bas $ is a basis of $ V $:
    \begin{itemize}
      \item consider the following linear combination:
        \begin{equation*}
          \alpha_1 \ve{a}_1 + \dots + \alpha_r \ve{a}_r + \beta_1 \ve{b}_1 + \dots + \beta_k \ve{b}_k = \ve{0}_V
        \end{equation*}
        Then, by the linearity of $ f $:
        \begin{equation*}
          \begin{split}
            \ve{0}_W = f(\ve{0}_V)
            & = f(\alpha_1 \ve{a}_1 + \dots + \alpha_r \ve{a}_r + \beta_1 \ve{b}_1 + \dots + \beta_k \ve{b}_k) \\
            & = \alpha_1 \cdot \ve{0}_W + \dots + \alpha_r \cdot \ve{0}_W + \beta_1 \ve{c}_1 + \dots + \beta_k \ve{c}_k = \beta_1 \ve{c}_1 + \dots + \beta_k \ve{c}_k
          \end{split}
        \end{equation*}
        But $ \{\ve{c}_1 , \dots , \ve{c}_k\} $ is a basis of $ \ran{f} $, hence $ \beta_1 = \dots = \beta_k $ due to linear independence. Then $ \alpha_1 \ve{a}_1 + \dots + \alpha_r \ve{a}_r = \ve{0}_V $, but $ \{\ve{a}_1 , \dots , \ve{a}_r\} $ is a basis of $ \ker{f} $, so $ \alpha_1 = \dots = \alpha_r = 0 $;
      \item $ \ve{v} \in V \implies f(\ve{v}) \in \ran{f} = \braket{f(\ve{b}_1) , \dots , f(\ve{b}_k)} $, so $ \exists \gamma_1 , \dots , \gamma_k \in \K : f(\ve{v}) = \gamma_1 f(\ve{b}_1) + \dots + \gamma_k f(\ve{b}_k) $, which rearranging and using the linearity of $ f $ becomes $ f(\ve{v} - \gamma_1 \ve{v}_1 - \dots - \gamma_k \ve{b}_k) = \ve{0}_W $, i.e. $ \ve{v} - \gamma_1 \ve{v}_1 - \dots - \gamma_k \ve{v}_k \in \ker{f} = \braket{\ve{a}_1 , \dots , \ve{a}_r} $. Then, $ \exists \delta_1 , \dots , \delta_r \in \K : \ve{v} = \gamma_1 \ve{v}_1 + \dots + \gamma_k \ve{v}_k + \delta_1 \ve{a}_1 + \dots + \delta_r \ve{a}_r $, which shows that $ V = \braket{\bas} $.
    \end{itemize}
    By \dref{def:basis}, $ \bas $ is a basis of $ V $, i.e. $ \dk V = \dk \ker{f} + \dk \ran{f} $.
  \end{proof}
\end{proofbox}

\begin{corollary}{Equidimensionality and bijections}{equidimensionality-bijections}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces and $ f \in \hmk(V,W) $. Then:
  \begin{equation*}
    \dk V = \dk W
    \qquad \implies \qquad
    f \text{ injective } \iff f \text{ surjective } \iff f \text{ bijective}
  \end{equation*}
\end{corollary}

\begin{proofbox}
  \begin{proof}
    By \pref{prop:kernel-injections}, $ f \text{ injective } \iff \ker{f} = \{\ve{0}_V\} \iff \dk \ker{f} = 0 $. By \tref{th:rank-nullity} $ \dk \ker{f} = 0 \iff \dk \ran{f} = \dk V = \dk W \iff \ran{f} = W $, and by \pref{prop:image-surjections} $ \ran{f} = W \iff f $ surjective. Hence, $ f $ is both injective and surjective, i.e. a bijection.
  \end{proof}
\end{proofbox}

We can further classify applications:
\begin{itemize}
  \item $ f \in \hmk(V,W) $ bijective is an \bctxt{isomorphism};
  \item $ f \in \hmk(V,V) \equiv \End(V) $ is an \bctxt{endomorphism};
  \item $ f \in \End(V) $ bijective is an \bctxt{automorphism}.
\end{itemize}
Isomorphisms are particularly interesting.

\begin{lemma}{Basic properties of isomorphisms}{}
  Given three $ \K $-vector spaces $ V,W,Z $ and $ f \in \hmk(V,W) , g \in \hmk(W,Z) $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item $ f $ is an isomorphism if and only if it is invertible
    \item if $ f $ and $ g $ are isomorphisms, then $ g \circ f \in \hmk(V,Z) $ is an isomorphism
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Trivial by the fact that invertibility is equivalent to bijectivity and that the composition of bijections is a bijection.
  \end{proof}
\end{proofbox}

\begin{example}{Matrices as endomorphisms}{}
  Given $ \mt{A} \in \K^{n \times n} $, then $ L_\mt{A} \in \End(\K^n) $. Moreover, if $ \mt{A} \in \GL{n,\K} $, then $ L_\mt{A} $ is an automorphism.
\end{example}

Isomorphism induce an equivalence relation between vector spaces.

\begin{definition}{Isomorphism relation}{}
  Two $ \K $-vector spaces $ V,W $ are \bcdef{isomorphic} $ V \cong W $ if $ \exists f \in \hmk(V,W) $ isomorphism.
\end{definition}

This is an equivalence relation since, if $ f $ is an isomorphism, then $ f^{-1} $ is an isomorphism too.

\begin{theorem}{Equidimensionality and isomorphisms}{}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces. Then:
  \begin{equation*}
    V \cong W
    \iff
    \dk V = \dk W
  \end{equation*}
\end{theorem}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ $ V \cong W \implies \exists f \in \hmk(V,W) $ isomorphism, which maps bases to bases, hence $ \dk V = \dk W $

    $ (\Leftarrow) $ Consider $ \bas = \{\ve{b}_1 , \dots , \ve{b}_n\} \ssq V $ basis of $ V $, so that $ \forall \ve{v} \in V \,\exists! \alpha_1 , \dots , \alpha_n \in \K : \ve{v} = \alpha_1 \ve{b}_1 + \dots + \alpha_n \ve{b}_n $. Then, define $ \varphi : V \ra \K^n : \varphi(\ve{v}) = \left( \alpha_1 , \dots , \alpha_n \right) $, which is clearly linear, so $ \varphi \in \hmk(V,\K^n) $. Moreover, $ \forall \bs{\alpha} \in \K^n \,\exists \ve{v} \in V : \ve{v} = \sum_{j = 1}^n \alpha_j \ve{b}_j $, as $ V = \braket{\bas} $, hence $ \forall \bs{\alpha} \in \K^n \,\exists \ve{v} \in V : \varphi(\ve{v}) = \bs{\alpha} $, i.e. $ \varphi $ is a surjection. Since $ \dk V = \dk \K^n $, by \cref{cor:equidimensionality-bijections} $ \varphi $ is a bijection too, thus $ V \cong \K^n $.

    Analogously, given a basis $ \mathcal{C} \ssq W $ of $ W $, we can construct an equivalent isomorphism $ \psi : W \ra \K^n $, so $ W \cong \K^n $. By the transitivity of the isomorphism relation, $ V \cong W $.
  \end{proof}
\end{proofbox}

The isomorphism relation then partitions the set of all finitely-generated vector spaces into equivalence classes composed of equidimensional spaces: for example, $ \C^n(\R) \cong \R^{2n} $ and $ \C_n[x] \cong \C^{n+1} $.

\subsection{Representative matrices}

Recalling that we can associate to each matrix $ \mt{A} \in \K^{m \times n} $ an application $ L_\mt{A} \in \hmk(\K^n,\K^m) : \ve{v} \mapsto \mt{A} \ve{v} $, it is clear that $ \ker{L_\mt{A}} $ is the solution space of the homogeneus linear system determined by $ \mt{A} \ve{x} = \ve{0} $, while $ \ran{L_\mt{A}} $ is the space of all constant terms $ \ve{b} $ which make the system $ \mt{A} \ve{x} = \ve{b} $ solvable. Moreover, the generators of $ \ran{L_\mt{A}} $ are the images of the generators of $ \K^n $: taking the Euclidean base $ \{\ve{e}_j\}_{j = 1, \dots, n} $, then:
\begin{equation*}
  L_\mt{A}(\ve{e}_j) =
  \begin{bmatrix}
    a_{11} & \dots & a_{1j} & \dots & a_{1n} \\
    \vdots & \ddots & \vdots & \ddots & \vdots \\
    a_{m1} & \dots & a_{mj} & \dots & a_{mn}
  \end{bmatrix}
  \begin{pmatrix}
    0 \\ \vdots \\ 1 \\ \vdots \\ 0
  \end{pmatrix}
  =
  \begin{pmatrix}
    a_{1j} \\ \vdots \\ a_{mj}
  \end{pmatrix}
\end{equation*}
We see, then, that the $ n $ columns of $ \mt{A} $ are the $ n $ column vectors which generate $ \ran{L_\mt{A}} $.

Now, the converse is possible too, i.e. to associate a matrix to a linear application. Consider two $ \K $-vector spaces $ V,W $ with respective bases $ \mathcal{A} = \{\ve{a}_1 , \dots , \ve{a}_n\} , \bas = \{\ve{b}_1 , \dots , \ve{b}_m\} $, and take $ f \in \hmk(V,W) $. By linearity, $ f $ is determined by its values on $ \mathcal{A} $, so suppose that:
\begin{equation}
  \begin{array}{l}
    f(\ve{a}_1) = \alpha_{11} \ve{b}_1 + \dots + \alpha_{1m} \ve{b}_m \\
    \qquad \ \ \, \vdots \\
    f(\ve{a}_n) = \alpha_{n1} \ve{b}_1 + \dots + \alpha_{nm} \ve{b}_m
  \end{array}
  \qquad \implies \qquad
  \mt{A} \equiv
  \begin{bmatrix}
    \alpha_{11} & \dots & \alpha_{1n} \\
    \vdots & \ddots & \vdots \\
    \alpha_{m1} & \dots & \alpha_{mn}
  \end{bmatrix}
  \label{eq:representative-matrix-definition}
\end{equation}
We want to show that $ f $ and $ L_\mt{A} $ are the ``same" application, i.e. we want to show that the following diagram commutes:
\begin{equation*}
  \begin{tikzcd}
    V \ar[d, "\varphi_\mathcal{A}", swap] \ar[r, "f"]
    & W \ar[d, "\varphi_\bas"] \\
    \K^n \ar[r, "L_\mt{A}", swap]
    & \K^m
  \end{tikzcd}
\end{equation*}
where $ \varphi_\mathcal{A} : V \ra \K^n $ and $ \varphi_\bas : W \ra \K^m $ are the representations of $ V $ and $ W $ on $ \K^n $ and $ \K^m $ in the respective bases, defined as:
\begin{equation*}
  V \ni \lambda_1 \ve{a}_1 + \dots + \lambda_n \ve{a}_n = \ve{v} \mapsto
  \begin{pmatrix}
    \lambda_1 \\ \vdots \\ \lambda_n
  \end{pmatrix}
  \in \K^n
  \qquad \quad
  W \ni \mu_1 \ve{b}_1 + \dots + \mu_m \ve{b}_m = \ve{w} \mapsto
  \begin{pmatrix}
    \mu_1 \\ \vdots \\ \mu_m
  \end{pmatrix}
  \in \K^m
\end{equation*}
Now, we can directly verify that $ L_\mt{A} \circ \varphi_\mathcal{A} = \varphi_\bas \circ f $, and in particular it is sufficient to show it on a basis:
\begin{equation*}
  L_\mt{A} \circ \varphi_\mathcal{A}(\ve{a}_j) = L_\mt{A}(\ve{e}_j) =
  \begin{pmatrix}
    \alpha_{1j} \\ \vdots \\ \alpha_{mj}
  \end{pmatrix}
  = \varphi_\bas(\alpha_{1j} \ve{b}_1 + \dots + \alpha_{mj} \ve{b}_m) = \varphi_\bas \circ f(\ve{a}_j)
\end{equation*}
Hence, the association between matrices and linear applications is bidirectional and well-defined, and in fact it defines an isomorphism $ \hmk(V,W) \cong \K^{m \times n} $.

\begin{definition}{Representative matrix}{}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces with respective bases $ \mathcal{A} , \bas $. Then, the \bcdef{representative matrix} of $ f \in \hmk(V,W) $ is the matrix $ \mt{M}_\bas^\mathcal{A}(f) $ determined by the isomorphism $ \hmk(V,W) \leftrightarrow \K^{m \times n} : f \leftrightarrow \mt{M}_{\bas}^\mathcal{A}(f) $ defined by \eref{eq:representative-matrix-definition}.
\end{definition}

\begin{lemma}{Basic properties of representation matrices}{}
  Given three finitely-generated $ \K $-vector spaces $ X,Y,Z $ with respective bases $ \mathcal{A} , \bas , \mathcal{C} $ and $ f \in \hmk(V,W) , g \in \hmk(W,Z) $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item $ \mt{M}_\mathcal{C}^\mathcal{A}(g \circ f) = \mt{M}_\mathcal{C}^\bas(g) \cdot \mt{M}_\bas^\mathcal{A}(f) $
    \item $ V = W \land \mathcal{A} = \bas \implies \mt{M}_\mathcal{A}^\mathcal{A}(\id_V) = \mt{I}_{\dk V} $
    \item $ f \text{ isomorphism } \implies \mt{M}_\bas^\mathcal{A}(f) \text{ invertible} \,\land\, [\mt{M}_\bas^\mathcal{A}(f)]^{-1} = \mt{M}_\mathcal{A}^\bas(f^{-1}) $
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    The first two propositions are true by the linearity of $ f $ and $ g $, while the last one is proved solving $ f^{-1} \circ f = \id_V \implies \mt{M}_\mathcal{A}^\bas(f^{-1}) \cdot \mt{M}_\bas^\mathcal{A}(f) = \id_{\dk V} $, where the first two properties where applied.
  \end{proof}
\end{proofbox}

\subsubsection{Change of bases}

To discuss how to perform a change of basis in a vector space, first we have to introduce two equivalence relations.

\begin{definition}{Equivalent matrices}{}
  Two matrices $ \mt{A} , \mt{B} \in \K^{m \times n} $ are \bcdef{equivalent} if $ \exists \mt{E} \in \GL{m,\K} , \mt{F} \in \GL{n,\K} : \mt{B} = \mt{E} \mt{A} \mt{F} $.
\end{definition}

\begin{definition}{Similar matrices}{}
  Two square matrices $ \mt{A} , \mt{B} \in \K^{n \times n} $ are \bcdef{similar} if $ \exists \mt{N} \in \GL{n,\K} : \mt{B} = \mt{N}^{-1} \mt{A} \mt{N} $.
\end{definition}

To illustrate how representation matrices change under a change of basis, consider a $ \K $-vector space $ V $ and two bases $ \mathcal{A} , \bas \ssq V $ (we denote $ V_\mathcal{A} , V_\bas $ the space with basis $ \mathcal{A} $ and $ \bas $ respectively), and take $ f \in \End{V} $. Then, consider the following commutative diagram:
\begin{equation*}
  \begin{tikzcd}
    V_\mathcal{A} \ar[r , "f"] \ar[d , "\id_V" , swap]
    & V_\mathcal{A} \ar[d , "\id_V"] \\
    V_\bas \ar[r , "f"]
    & V_\bas
  \end{tikzcd}
  \quad \implies \quad
  \underbrace{\mt{M}_\bas^\bas(f)}_{\in \, \K^{n \times n}} \cdot \underbrace{\mt{M}_\bas^\mathcal{A}(\id_V)}_{\in \, \GL{n,\K}} = \underbrace{\mt{M}_\bas^\mathcal{A}(\id_V)}_{\in \, \GL{n,\K}} \cdot \underbrace{\mt{M}_\mathcal{A}^\mathcal{A}(f)}_{\K^{n \times n}}
\end{equation*}
Hence, we see that representative matrices of the same endomorphism are similar. Moreover, we can define the change-of-basis matrix $ \mt{N}_\bas^\mathcal{A} \equiv \mt{M}_\bas^\mathcal{A}(\id_V) $, whose columns are the coefficients of the representations on $ \bas $ of the vectors of $ \mathcal{A} $. Note that, in the particular case $ f = \id_V $, the above equation proves that $ [\mt{N}_\bas^\mathcal{A}]^{-1} = \mt{N}_\mathcal{A}^\bas $.

A similar diagram can be drawn for the generalized case of $ f \in \hmk(V,W) $:
\begin{equation*}
  \begin{tikzcd}
    V_\mathcal{A} \ar[r , "f"] \ar[d , "\id_V" , swap]
    & W_\bas \ar[d , "\id_W"] \\
    V_{\mathcal{A}'} \ar[r , "f"]
    & W_{\bas'}
  \end{tikzcd}
  \quad \implies \quad
  \underbrace{\mt{M}_{\bas'}^{\mathcal{A}'}(f)}_{\in \, \K^{m \times n}} \cdot \underbrace{\mt{N}_\mathcal{A}^{\mathcal{A}'}}_{\in \, \GL{n,\K}} = \underbrace{\mt{N}_{\bas'}^\bas}_{\in \, \GL{m,\K}} \cdot \underbrace{\mt{M}_\bas^\mathcal{A}(f)}_{\K^{m \times n}}
\end{equation*}
Therefore, representative matrices of the same linear application are equivalent.









\section{Inner-product spaces}
