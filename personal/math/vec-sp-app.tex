\selectlanguage{english}

\section{Matrices}

\begin{definition}{Matrix}{}
  Given a field $ \K $ and $ n , m \in \N $, an $ n \times m $ \bcdef{matrix}\index{matrix} on $ \K $ is the object:
  \begin{equation*}
    \mt{A} =
    \begin{bmatrix}
      a_{11} & a_{12} & \dots & a_{1m} \\
      a_{21} & a_{22} & \dots & a_{2m} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{n1} & a_{n2} & \dots & a_{nm}
    \end{bmatrix}
    \equiv [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m}
    \quad : \quad
    a_{ij} \in \K \,\,\forall i = 1, \dots, n ,\, j = 1, \dots, m
  \end{equation*}
  The set of all $ n \times m $ matrices on $ \K $ is denoted by $ \K^{n \times m} $.
\end{definition}

When the dimensions of the matrix $ \mt{A} $ are unambiguous, we simply write $ \mt{A} = [a_{ij}] $. We say that an $ n \times n $ matrix is a \bctxt{square matrix}, an $ n \times 1 $ matrix is a \bctxt{column vector} and a $ 1 \times n $ matrix is a \bctxt{row vector}.

It is possible to define three operations between matrices:
\begin{itemize}
  \item sum $ + : \K^{n \times m} \times \K^{n \times m} \rightarrow \K^{n \times m} : [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} + [b_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} \mapsto [a_{ij} + b_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $
  \item product by a scalar $ \cdot : \K \times \K^{n \times m} \rightarrow \K^{n \times m} : \alpha \cdot [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} = [\alpha a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $
  \item product $ \cdot : \K^{n \times p} \times \K^{p \times m} \rightarrow \K^{n \times m} : [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, p} \cdot [b_{ij}]^{i = 1, \dots, p}_{j = 1, \dots, m} = [c_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $, $ c_{ij} = \sum_{k = 1}^p a_{ik} b_{kj} $
\end{itemize}
Note that $ \alpha a_{ij} $ is the $ \K $-product.

\begin{proposition}{}{mat-abel-gr}
  $ (\K^{n \times m} , +) $ is an abelian group.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    The matrix sum is equivalent to the $ \K $-sum of corresponding elements, which is associative and commutative. The neutral element is the zero matrix $ 0_{n \times m} = [0]^{i = 1, \dots, n}_{j = 1, \dots, m} $, while the inverse element is $ -\mt{A} = [-a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $.
  \end{proof}
\end{proofbox}

\begin{proposition}{}{}
  $ (\K^{n \times n} , + , \cdot) $ is a non-commutative ring.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    By \pref{prop:mat-abel-gr}, $ (\K^{n \times n} , +) $ is an abelian group. It is trivial to show the associativity and distributivity of the matrix product, i.e.:
    \begin{enumerate}
      \item $ \mt{A} \cdot (\mt{B} \cdot \mt{C}) = (\mt{A} \cdot \mt{B}) \cdot \mt{C} ,\, \lambda (\mt{A} \cdot \mt{B}) = (\lambda \mt{A}) \cdot \mt{B} = \mt{A} \cdot (\lambda \mt{B}) \,\,\forall \mt{A} , \mt{B} , \mt{C} \in \K^{n \times n} , \lambda \in \K $
      \item $ \mt{A} \cdot (\mt{B} + \mt{C}) = \mt{A} \cdot \mt{B} + \mt{A} \cdot \mt{C} ,\, (\mt{A} + \mt{B}) \cdot \mt{C} = \mt{A} \cdot \mt{C} + \mt{B} \cdot \mt{C} \,\,\forall \mt{A} , \mt{B} , \mt{C} \in \K^{n \times n} $
    \end{enumerate}
    Finally, the neutral element of the matrix product is the identity matrix $ \mt{I}_n = [\delta_{ij}]_{i,j = 1, \dots, n} $.
  \end{proof}
\end{proofbox}

\begin{definition}{Transposed matrix}{}
  Given a matrix $ \mt{A} \in \K^{n \times m} $, its \bcdef{transpose} is defined as $ \mt{A}\tsp \in \K^{m \times n} : [a\tsp_{ij}]^{i = 1, \dots, m}_{j = 1, \dots, n} = [a_{ji}]^{j = 1, \dots, n}_{i = 1, \dots, m} $.
\end{definition}

Square matrices can be further characterized: a square matrix $ \mt{A} \in \K^{n \times n} $ is said \bctxt{symmetric} if $ \mt{A}\tsp = \mt{A} $ or \bctxt{antisymmetric} if $ \mt{A}\tsp = - \mt{A} $, and it is \bctxt{diagonal} if $ a_{ij} = 0 \,\,\forall i \neq j \in \{1, \dots, n\} $. Moreover, we can introduce the concept of inverse matrix for square matrices.

\begin{definition}{Inverse matrix}{}
  A square matrix $ \mt{A} \in \K^{n \times n} $ is \bcdef{invertible} if $ \exists \mt{A}^{-1} \in \K^{n \times n} : \mt{A}^{-1} \cdot \mt{A} = \mt{A} \cdot \mt{A}^{-1} = \mt{I}_n $.
\end{definition}

\begin{example}{Non-invertible matrix}{}
  The matrix $ \begin{bmatrix} 2 & 0 \\ 0 & 0 \end{bmatrix} $ is non-invertible, as $ \begin{bmatrix} 2 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} = \begin{bmatrix} 2 \alpha & 2 \beta \\ 0 & 0 \end{bmatrix} \neq \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \,\forall \alpha , \beta , \gamma , \delta \in \R $.
\end{example}

\begin{definition}{General linear group}{}
  The \bcdef{general linear group}\index{$ \GL{n,\K} $} $ \GL{n,\K} $ is defined as the subset of $ \K^{n \times n} $ of all invertible matrices.
\end{definition}

Note that $ \GL{1,\K} = \K - \{0\} $.

\begin{proposition}{}{}
  $ (\GL{n,\K} , \cdot) $ is a non-abelian group.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    The neutral element is $ \mt{I}_n $, as $ \mt{I}_n^{-1} = \mt{I}_n \implies \mt{I}_n \in \GL{n,\K} $, while the existence of the inverse is granted by definition. We only have to show closure under matrix multiplication:
    \begin{equation*}
      (\mt{A} \mt{B})^{-1} = \mt{B}^{-1} \mt{A}^{-1} \impliedby \mt{I}_n = \mt{A} \cdot \mt{A}^{-1} = \mt{A} \mt{I}_n \mt{A}^{-1} = \mt{A} \mt{B} \mt{B}^{-1} \mt{A}^{-1} = (\mt{A} \mt{B}) (\mt{A} \mt{B})^{-1}
    \end{equation*}
    Hence, $ \mt{A} , \mt{B} \in \GL{n,\K} \implies \mt{A} \mt{B} \in \GL{n,\K} $.
  \end{proof}
\end{proofbox}

By this result, we conclude that the inverse matrix, when it exists, is unique.

\subsection{Linear systems of equations}

A \bctxt{linear equation} with $ n \in \N $ variables and $ \K $-coefficients is an expression of the form:
\begin{equation*}
  a_1 x_1 + \dots + a_n x_n = b
  \qquad
  a_i , b \in \K \,\,\forall i = 1, \dots, n
\end{equation*}
A \bctxt{solution} of the equation is an $ n $-tuple $ (\bar{x}_1, \dots, \bar{x}_n) \in \K^n $ which satisfies this expression.

\begin{definition}{Linear system of equations}{}
  A linear system of equations (or simply \bcdef{linear system}\index{linear system}) is a collection of $ m $ linear equations with $ n $ variables:
  \begin{equation*}
    \begin{cases}
      a_{11} x_1 + \dots + a_{1n} x_n = b_1 \\
      a_{21} x_1 + \dots + a_{2n} x_n = b_2 \\
      \qquad \qquad \quad \vdots \\
      a_{m1} x_1 + \dots + a_{mn} x_n = b_m \\
    \end{cases}
    \qquad \iff \qquad
    \mt{A} \ve{x} = \ve{b}
  \end{equation*}
  where we defined:
  \begin{equation*}
    \mt{A} =
    \begin{bmatrix}
      a_{11} & \dots & a_{1n} \\
      a_{21} & \dots & a_{2n} \\
      \vdots & \ddots & \vdots \\
      a_{m1} & \dots & a_{mn}
    \end{bmatrix}
    \in \K^{m \times n}
    \qquad \qquad
    \ve{b} =
    \begin{pmatrix}
      b_1 \\ b_2 \\ \vdots \\ b_m
    \end{pmatrix}
    \in \K^{m \times 1}
    \qquad \qquad
    \ve{x} =
    \begin{pmatrix}
      x_1 \\ x_2 \\ \vdots \\ x_n
    \end{pmatrix}
    \in \K^{n \times 1}
  \end{equation*}
\end{definition}

Two linear systems with the same set of solutions are called \bctxt{equivalent systems}: note that two equivalent systems must have the same number of variables, but not necessarily the same number of equations.

Based on the cardinality of its solution set, a linear system is said to be \bctxt{impossible} if it has no solutions, \bctxt{determined} if it has one solution and \bctxt{undetermined} if it has infinitely-many solutions. Moreover, if the solution set can be parametrized by $ k \in \N_0 $ variables, the system is of kind $ \infty^k $: a determined system is of kind $ \infty^0 $.

Linear systems can be systematically solved applying a reduction algorithm to their corresponding matrices: \bctxt{Gauss algorithm}\index{Gauss algorithm}. Starting with a general composed matrix $ [\mt{A} | \ve{b}] \in \K^{m \times (n+1)} $, first we multiply the first row by $ a_{11}^{-1} $, so that:
\begin{equation*}
  \left[
  \begin{array}{cccc|c}
    a_{11} & a_{12} & \dots & a_{1n} & b_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
  \quad \longrightarrow \quad
 \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
\end{equation*}
Then, at each row $ \mt{R}_2 , \dots , \mt{R}_m $ we apply the transformation $ \mt{R}_k \mapsto \mt{R}_k - a_{k1} \mt{R}_1 $, so that:
\begin{equation*}
  \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
  \quad \longrightarrow \quad
 \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    0 & a'_{22} & \dots & a'_{2n} & b'_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & a'_{m2} & \dots & a'_{mn} & b'_m \\
  \end{array}
  \right]
\end{equation*}
Reiterating this process to progressively smalles submatrices, the algorithm yields the general transformation:
\begin{equation*}
  \left[
  \begin{array}{cccc|c}
    a_{11} & a_{12} & \dots & a_{1n} & b_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
  \quad \longrightarrow \quad
  \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    0 & 1 & \dots & a'_{2n} & b'_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \dots & 1 & b'_m \\
  \end{array}
  \right]
\end{equation*}
As these are linear transformations, the two matrices represent equivalent linear systems: the transformed linear system is substantially easier to solve, and its solution set is a solution set of the starting linear system too.

\begin{definition}{Character}{}
  Given a matrix $ \mt{M} \in \K^{n \times m} $, its \bcdef{character} $ \mathrm{car}(\mt{M}) $ is the number of non-zero rows remaining after Gauss reduction.
\end{definition}

It can be proven that the character is independent of the operations performed during the reduction algorithm (see \secref{sssec:rank}). Moreover, it is possible to prove the Rouch√©--Capelli theorem (\tref{th:rouche-capelli}), which states that the system of equations $ \mt{A} \ve{x} = \ve{b} $ has solutions if and only if $ \mathrm{car}(\mt{A}) = \mathrm{car}([\mt{A} | \ve{b}]) $ and, in this case, that it is of type $ \infty^{n - r} $, with $ r \equiv \mathrm{car}(\mt{A}) $.

\section{Vector spaces}

\begin{definition}{Vector space}{}
  Given a set $ V \neq \emptyset $ and a field $ \K $, then $ V $ is a \bcdef{$ \K $-vector space} if there exist two operations:
  \begin{equation*}
    + : V \times V \ra V \,:\, (\ve{v} , \ve{w}) \mapsto \ve{v} + \ve{w}
    \qquad \qquad
    \cdot : \K \times V \ra V \,:\, (\lambda , \ve{v}) \mapsto \lambda \cdot \ve{v}
  \end{equation*}
  such that $ (V,+) $ is an abelian group and the following properties hold $ \forall \lambda , \mu \in \K , \ve{v} , \ve{w} \in V $:
  \begin{enumerate}
    \item $ (\lambda + \mu) \cdot (\ve{v} + \ve{w}) = \lambda \cdot \ve{v} + \mu \cdot \ve{v} + \lambda \cdot \ve{w} + \mu \cdot \ve{w} $
    \item $ (\lambda \cdot \mu) \cdot \ve{v} = \lambda \cdot (\mu \cdot \ve{v}) = \mu \cdot (\lambda \cdot \ve{v}) $
    \item $ 1_\K \cdot \ve{v} = \ve{v} $
  \end{enumerate}
\end{definition}

Note that there are three unique neutral elements: $ 0_\K \equiv 0 $, $ 1_\K \equiv 1 $ and $ 0_V \equiv \ve{0} $.
In the following, the multiplication symbol $ \cdot $ is suppressed, as the factors clarify which multiplication is occurring ($ \cdot : \K \times \K \ra \K $ or $ \cdot : \K \times V \ra V $, which have the same neutral element $ 1_\K $).

\begin{example}{Complex numbers}{}
  $ V = \C $ is a vector space both for $ \K = \R $ and $ \K = \C $, although they are different objects.
\end{example}

\begin{example}{Field as vector space}{}
  $ V = \K $ is a $ \K $-vector space. Note that, in this case, $ 0_\K \equiv 0_V $.
\end{example}

Note that, by the uniqueness of $ 0_V $, then $ \forall \ve{v} \in V \,\,\exists! -\ve{v} \in V : \ve{v} + (-\ve{v}) = 0_V $, so the following cancellation rule holds $ \forall \ve{u} , \ve{v} , \ve{w} \in V $:
\begin{equation}
  \ve{u} + \ve{v} = \ve{w} + \ve{v}
  \quad \implies \quad
  \ve{u} = \ve{w}
  \label{eq:cancellation-rule}
\end{equation}

We can now state some basic properties of vector spaces.

\begin{lemma}{Basic properties of vector spaces}{}
  Given a $ \K $-vector space $ V $, then $ \forall \lambda \in \K , \ve{v} \in V $:
  \begin{multicols}{2}
    \begin{enumerate}[label = {\alph*.}]
      \item $ 0_\K \cdot \ve{v} = 0_V $
      \item $ (-\lambda) \cdot \ve{v} = - (\lambda \cdot \ve{v}) $
      \item $ \lambda \cdot 0_V = 0_V $
      \item $ \lambda \cdot \ve{v} = 0_V \iff \lambda = 0_\K \lor \ve{v} = 0_V $
    \end{enumerate}
  \end{multicols}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item Consider $ c \in \K - \{0_\K\} $; then $ c \ve{v} + 0_V = c \ve{v} = (c + 0_\K) \ve{v} = c \ve{v} + 0_\K \cdot \ve{v} $, which by \eref{eq:cancellation-rule} proves $ 0_\K \cdot \ve{v} = 0_V $.
      \item $ \lambda \ve{v} + (-\lambda) \ve{v} = (\lambda - \lambda) \ve{v} = 0_\K \cdot \ve{v} = 0_V $, which by the uniqueness of the negative element proves $ (-\lambda) \ve{v} = - (\lambda \ve{v}) $.
      \item $ \lambda \cdot 0_V = \lambda (\ve{v} - \ve{v}) = \lambda \ve{v} + \lambda \cdot (-1_\K) \cdot \ve{v} = \lambda \ve{v} + (-\lambda) \ve{v} = \lambda \ve{v} - (\lambda \ve{v}) = 0_V $
      \item $ \lambda = 0_\K $ is trivial, so consider $ \lambda \neq 0_\K $; then $ \exists ! \lambda^{-1} \in \K : \lambda^{-1} \cdot \lambda = 1_\K $, so $ 0_V = \lambda^{-1} \cdot 0_V = \lambda^{-1} \cdot (\lambda \ve{v}) = (\lambda^{-1} \cdot \lambda) \ve{v} = 1_\K \cdot \ve{v} = \ve{v} $, i.e. $ \ve{v} = 0_V $.
    \end{enumerate}
    \ensp
  \end{proof}
\end{proofbox}

\subsection{Subspaces}

\begin{definition}{Subspace}{}
  Given a $ \K $-vector space $ V $ and a subset $ U \subseteq V : U \neq \emptyset $, then $ U $ is a \bcdef{subspace}\index{subspace} of $ V $ if it is closed under $ + : U \times U \ra U $ and $ \cdot : \K \times U \ra U $.
\end{definition}

\begin{lemma}{}{}
  If $ U $ is a subspace of $ V(\K) $, then $ 0_V \in U $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    By definition $ U \neq \emptyset \implies \exists \ve{v} \in U $. By the closure condition $ \lambda \ve{v} \in U \,\,\forall \lambda \in \K $, hence taking $ \lambda = 0_\K $ proves the thesis.
  \end{proof}
\end{proofbox}

A typical strategy to prove that $ U $ is a subspace of $ V(\K) $ is showing the closure properties, while to prove that it is \emph{not} a subspace we usually show that $ 0_V \notin U $.

\begin{example}{Polynomial subspaces}{}
  Given $ V = \K[x] $, then $ U = \K_n[x] $ is a subspace $ \forall n \in \N_0 $.
\end{example}

An important concept to analyze vector spaces is that of linear combination. Given two sets $ \{\lambda_k\}_{k = 1, \dots, n} \subset \K $ and $ \{\ve{v}_k\}_{k = 1, \dots, n} \subset V $, their \bctxt{linear combination}\index{linear combination} is:
\begin{equation}
  \sum_{k = 1}^n \lambda_k \ve{v}_k = \lambda_1 \ve{v}_1 + \dots \lambda_n \ve{v}_n \in V
\end{equation}

\begin{proposition}{Subspaces and linear combinations}{subspaces-linear-combinations}
  Given a $ \K $-vector space $ V $ and $ U \subset V : U \neq \emptyset $, then $ U $ is a subspace of $ V $ if and only if it is closed under linear combinations, that is:
  \begin{equation*}
    \{\lambda_k\}_{k = 1, \dots, n} \subset \K , \{\ve{v}_k\}_{k = 1, \dots, n} \subset U
    \quad \implies \quad
    \sum_{k = 1}^n \lambda_k \ve{v}_k \in U
  \end{equation*}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    First, note that the general case of linear combinations of $ n $ vectors can be reduced to the case of $ 2 $ vectors.

    $ (\Rightarrow) $ Being $ U $ a subspace, it is closed under $ + : U \times U \ra U $ and $ \cdot : \K \times U \ra U $; then, by definition $ \lambda , \mu \in \K , \ve{v} , \ve{w} \in U \implies \lambda \ve{v} + \mu \ve{w} \in U $.

    $ (\Leftarrow) $ Given $ \lambda \in \K $ and $ \ve{v} , \ve{w} \in V $, then $ \ve{v} + \ve{w} = 1_\K \ve{v} + 1_\K \ve{w} $ and $ \lambda \ve{v} = \lambda \ve{v} + 0_\K \ve{w} $, hence closure under linear combinations implies closure under $ + : U \times U \ra U $ and $ \cdot : \K \times U \ra U $.
  \end{proof}
\end{proofbox}

Generally, it is easier to show closure under linear combinations rather than under addition and scalar multiplication.

\begin{lemma}{Intersection of subspaces}{subspace-intersection}
  Given two subspaces of $ V_1 , V_2 $ of $ V(\K) $, then $ V_1 \cap V_2 $ is still a subset of $ V(\K) $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Being $ V_1 , V_2 $ subspaces, both $ V_1 $ and $ V_2 $ are closed under linear combinations, so $ V_1 \cap V_2 $ is too, as $ \ve{v} \in V_1 \cap V_2 \implies \ve{v} \in V_1 \land \ve{v} \in V_2 $.
  \end{proof}
\end{proofbox}

On the other hand, in general $ V_1 \cup V_2 $ is not a subspace. As a counterexample, consider e.g. $ V = \Vect_0(\R^3) $, the plane $ \spi : z = 0 $ and the line $ r : (x,y,z) = (0,0,t) , t \in \R $; then, consider the subspaces $ V_1 = \Vect_0(\pi) , V_2 = \Vect_0(r) $: their union is clearly not closed under addition, as:
\begin{equation*}
  \begin{pmatrix}
    1 \\ 1 \\ 0
  \end{pmatrix} \in V_1
  ,
  \begin{pmatrix}
    0 \\ 0 \\ 1
  \end{pmatrix} \in V_2
  \qquad \qquad
  \begin{pmatrix}
    1 \\ 1 \\ 0
  \end{pmatrix}
  +
  \begin{pmatrix}
    0 \\ 0 \\ 1
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 \\ 1 \\ 1
  \end{pmatrix} \notin V_1 \cup V_2
\end{equation*}

\begin{definition}{Sum of subspaces}{}
  Given a $ \K $-vector space $ V $ and two subspaces $ V_1 , V_2 $, their \bcdef{sum}\index{subspace!sum of} is defined as:
  \begin{equation*}
    V_1 + V_2 \defeq \{\ve{w} \in V : \ve{w} = \ve{u} + \ve{v}, \ve{u} \in V_1, \ve{v} \in V_2\}
  \end{equation*}
  This is a \bcdef{direct sum}\index{direct sum!of subspaces}, denoted by $ V_1 \oplus V_2 $, if every $ \ve{w} \in V_1 + V_2 $ has a unique representation as $ \ve{w} = \ve{u} + \ve{v} , \ve{u} \in V_1 , \ve{v} \in V_2 $.
\end{definition}

Trivially $ V_1 , V_2 \subseteq V_1 + V_2 $.

\begin{lemma}{Direct sum as disjoint sum}{}
  Given two subspaces $ V_1 , V_2 $ of $ V(\K) $, then $ V_1 + V_2 = V_1 \oplus V_2 \iff V_1 \cap V_2 = \{\ve{0}\} $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ Suppose $ \exists \ve{v} \in V_1 \cap V_2 : \ve{v} \neq \ve{0} $; then $ \ve{v} = \ve{v} + \ve{0} = \ve{0} + \ve{v} $, i.e. the expression of $ \ve{v} \in V_1 + V_2 $, but the expression of $ \ve{v} \in V_1 \oplus V_2 $ must be unique, hence $ \ve{v} = \ve{0} \absurd $

    $ (\Leftarrow) $ Suppose $ \exists \ve{w} \in V_1 + V_2 : \ve{w} = \ve{u}_1 + \ve{v}_1 = \ve{u}_2 + \ve{v}_2 , \ve{u}_1 \neq \ve{u}_2 \in V_1 , \ve{v}_1 \neq \ve{v}_2 \in V_2 $; then $ V_1 \ni \ve{u}_1 - \ve{u}_2 = \ve{v}_2 - \ve{v}_1 \in V_2 \implies \ve{v}_2 - \ve{v}_1 \in V_1 $, so $ \ve{v}_2 - \ve{v}_1 \in V_1 \cap V_2 $, but $ V_1 \cap V_2 = \{\ve{0}\} $, hence $ \ve{v}_2 = \ve{v}_1 $ and idem for $ \ve{u}_1 = \ve{u}_2 \absurd $
  \end{proof}
\end{proofbox}

The sum of subspaces preserves the subspace structure, contrary to the simple union.

\begin{proposition}{Sum as subspace}{}
  Given a $ \K $-vector space and two subspaces $ V_1 , V_2 $, their sum $ V_1 + V_2 $ is still a subspace of $ V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Consider $ \ve{a} , \ve{b} \in V_1 + V_2 $ and define $ \ve{u}_{a,b} \in V_1 , \ve{v}_{a,b} \in V_2 : \ve{a} = \ve{u}_a + \ve{v}_a \land \ve{b} = \ve{u}_b + \ve{v}_b $: as $ V_1 , V_2 $ are subspaces, they are closed under linear combinations, so, given $ \lambda ,\mu \in \K $, then $ \lambda \ve{a} + \mu \ve{b} = (\lambda \ve{u}_a + \mu \ve{u}_b) + (\lambda \ve{v}_a + \mu \ve{v}_b) \equiv \ve{u} + \ve{v} \in V_1 + V_2 $, where $ \ve{u} \in V_1 $ and $ \ve{v} \in V_2 $, which shows that $ V_1 + V_2 $ too is closed under linear combinations and a subspace by \pref{prop:subspaces-linear-combinations}.
  \end{proof}
\end{proofbox}

\subsection{Bases}

To give a more explicit description of vector spaces, we have to define the concept of basis and its properties.

\subsubsection{Generators}

\begin{definition}{Linear dependence}{}
  Given a $ \K $-vector space $ V $ and a set $ \{\ve{v}_j\}_{j = 1, \dots, k} \equiv S \subseteq V $, then the vectors of $ S $ are:
  \begin{itemize}
    \item \bcdef{linearly dependent} (LD) if $ \exists \{\lambda_j\}_{j = 1, \dots, k} \subset \K - \{0\} : \lambda_1 \ve{v}_1 + \dots \lambda_k \ve{v}_k = \ve{0} $
    \item \bcdef{linearly independent}\index{linear independence!of vectors} (LI) if $ \lambda_1 \ve{v}_1 + \dots \lambda_k \ve{v}_k = \ve{0} \iff \lambda_j = 0 \,\,\forall j = 1, \dots, k $
  \end{itemize}
\end{definition}

The generalization to infinite sets is trivial: $ \{\ve{v}_\alpha\}_{\alpha \in \mathcal{I}} \equiv S \subset V(\K) $ is LI if every finite subset of $ S $ is LI, while it is LD if there exists at least one non-empty subset which is LD.

\begin{example}{Complex numbers}{}
  $ \{1, \img\} $ are LD in $ \C(\C) $, as $ 1 \cdot 1 + \img \cdot \img = 0 $, while they are LI in $ \C(\R) $.
\end{example}

\begin{example}{Polynomials}{}
  $ \{1, x, \dots, x^n, \dots\} $ are LI in $ \K[x] $.
\end{example}

We can prove some basic properties of linear dependence.

\begin{lemma}{Basic properties of linear dependence}{}
  Given a $ \K $-vector space $ V $ and $ S \subseteq V : S \neq \emptyset $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item Given $ S \subseteq T \subseteq V $, then $ S \text{ LD} \implies T \text{ LD} $
    \item $ S = \{\ve{v}\} \text{ LD} \implies \ve{v} = \ve{0} $;
    \item $ S = \{\ve{v}_1 , \ve{v}_2\} \text{ LD} \implies \exists \lambda \in \K : \ve{v}_1 = \lambda \ve{v}_2 $
    \item If $ S = \{\ve{v}_1 , \dots , \ve{v}_n\} $ LD, then at least one vector is a linear combination of the others;
    \item If $ S $ LI and $ S \cup \{\ve{w}\} $ LD, then $ \ve{w} $ is a linear combination of the vectors of $ S $;
    \item If $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n = \ve{0} $ and $ \lambda_n \neq 0 $, then $ \ve{v}_n $ is a linear combination of $ \{\ve{v}_1 , \dots , \ve{v}_{n-1}\} $.
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item $ S \subseteq T \implies \ve{v} \in T \,\,\forall \ve{v} \in S $, hence $ \{\ve{v}_i\}_{i = 1, \dots, n} \subset S \text{ LD} \implies \{\ve{v}_i\}_{i = 1, \dots, n} \subset T \text{ LD} $.
      \item $ \lambda \ve{v} = \ve{0} \iff \lambda = 0 \lor \ve{v} = \ve{0} $, so $ \ve{v} = \ve{0} \implies S \text{ LD} $, while $ S \text{ LD} \implies \lambda \neq 0 \implies \ve{v} = 0 $.
      \item $ \{\ve{v}_1 , \ve{v}_2\} \text{ LD} \implies \exists \lambda , \mu \in \K - \{0\} : \lambda \ve{v}_1 + \mu \ve{v}_2 = \ve{0} \iff \ve{v}_1 = \lambda^{-1} \mu \ve{v}_2 $
      \item If $ \{\ve{v}_j\}_{j = 1, \dots, n} $ LD, then by definition $ \exists \{\lambda_j\}_{j = 1, \dots, n} \subset \K - \{0\} : \sum_{j = 1}^n \lambda_j \ve{v}_j = \ve{0} $, hence WLOG $ \ve{v}_1 $ can be isolated as $ \ve{v}_1 = - \lambda_1^{-1} \sum_{j = 2}^n \lambda_j \ve{v}_j $.
      \item $ \{\ve{v}_1 , \dots , \ve{v}_n , \ve{w}\} \text{ LD} \implies \exists \lambda_1 , \dots , \lambda_n , \alpha \in \K - \{0\} : \sum_{j = 1}^n \lambda_j \ve{v}_j + \alpha \ve{w} = \ve{0} $, so $ \ve{w} $ can be isolated as $ \ve{w} = - \alpha^{-1} \sum_{j = 1}^n \lambda_j \ve{v}_j $.
      \item $ \sum_{j = 1}^n \lambda_j \ve{v}_j = \ve{0} \land \lambda_n \neq 0 \implies \ve{v}_n = - \lambda_n^{-1} \sum_{j = 1}^{n-1} \lambda_j \ve{v}_j $
    \end{enumerate}
    \ensp
  \end{proof}
\end{proofbox}

We can now introduce the notion of generators.

\begin{definition}{Generated subset}{}
  Given a $ \K $-vector space $ V $ and $ \{\ve{v}_\alpha\}_{\alpha \in \mathcal{I}} \equiv S \subseteq V $, the \bcdef{subset generated by $ S $} is the set:
  \begin{equation*}
    \lspan{S} \defeq \{\ve{v} \in V : \exists \lambda_1 , \dots , \lambda_n \in \K, \ve{v}_{\alpha_1} , \dots , \ve{v}_{\alpha_n} \in S : \ve{v} = \lambda_1 \ve{v}_{\alpha_1} + \dots + \lambda_n \ve{\alpha_n}\}
  \end{equation*}
  The elements of $ S $ are called \bcdef{generators} of $ \lspan{S} $.
\end{definition}

We often denote $ \lspan{S} \equiv \braket{S} $: this subset contains all vectors of $ V $ which can be expressed as linear combinations of vectors of $ S $.

\begin{proposition}{Generated subspace}{}
  Given a $ \K $-vector space and $ S \subseteq V : S \neq \emptyset $, then $ \braket{S} $ is a subspace of $ V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Let $ S = \{\ve{s}_\alpha\}_{\alpha \in \mathcal{I}} $ and $ \ve{v} , \ve{w} \in S : \ve{v} = \sum_{j = 1}^k \lambda_j \ve{s}_{\alpha_j} , \ve{w} = \sum_{j = 1}^n \mu_j \ve{s}_{\beta_j} $, with coefficients $ \{\lambda_j\}_{j = 1, \dots, k} , \{\mu_j\}_{j = 1, \dots, n} \subset \K - \{0\} $. Adding vectors with vanishing coefficients, we can rewrite $ \ve{v} $ and $ \ve{w} $ in terms of the same vectors:
    \begin{equation*}
      \ve{v} = \sum_{j = 1}^m a_j \ve{s}_{\gamma_j}
      \qquad \qquad
      \ve{w} = \sum_{j = 1}^m b_j \ve{s}_{\gamma_j}
      \quad \implies \quad
      \zeta \ve{v} + \xi \ve{w} = \sum_{j = 1}^m \left( \zeta a_j + \xi b_j \right) \ve{s}_{\gamma_j} \in \braket{S}
    \end{equation*}
    This shows that $ \braket{S} $ is closed under linear combination, hence the thesis.
  \end{proof}
\end{proofbox}

Note that, give a subspace $ U \subseteq V(\K) $, then at most $ U = \braket{U} $, hence every subspace admits a family of generators. If $ U $ has a finite number of generators, then it is a \bctxt{finitely-generated subspace}: for example, $ \K_n[x] = \braket{1, \dots, x^n} $, $ \C(\C) = \braket{1} $ and $ \C(\R) = \braket{1, \img} $ are finitely-generated.
We can state two trivial properties of generated subsets.

\begin{lemma}{}{}
  Given $ S \subseteq V(\K) $ and $ U = \braket{S} $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item Given $ S \subseteq T \subseteq V $, then $ U = \braket{T} $;
    \item If $ U = \braket{\ve{s}_1 , \dots , \ve{s}_n} $ and $ \ve{s}_n \in \braket{\ve{s}_1 , \ve{s}_{n-1}} $, then $ U = \braket{\ve{s}_1 , \dots , \ve{s}_{n-1}} $.
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item If $ S \subseteq T $, then each linear combination in $ S $ is a linear combination in $ T $ too, hence $ \braket{S} = \braket{T} $.
      \item Given $ \ve{v} = \lambda_1 \ve{s}_1 + \dots + \lambda_n \ve{s}_n \in U $ and $ \ve{s}_n = \mu_1 \ve{s}_1 + \dots + \mu_{n-1} \ve{s}_{n-1} $, then $ \ve{v} = \left( \lambda_1 + \mu_1 \right) \ve{s}_1 + \dots + \left( \lambda_{n-1} + \mu_{n-1} \right) \ve{s}_{n-1} $, hence the thesis.
    \end{enumerate}
    \ensp
  \end{proof}
\end{proofbox}

\subsubsection{Bases of generic vector spaces}

\begin{definition}{Basis of a vector space}{basis}
  Given a $ \K $-vector space $ V $, a \bcdef{basis} of $ V $ is a LI subset $ \bas \subseteq V : V = \braket{\bas} $.
\end{definition}

Every non-trivial vector space (i.e. $ V \neq \{\ve{0}\} $) admits the existence of a basis, but the proof is non-trivial as it relies on Zorn's Lemma (or equivalently to the Axiom of Choice).

\begin{theorem}{Basis theorem}{basis}
  Every non-trivial vector space admits a basis.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    First, we prove that every LI subset of $ V $ can be extended to a basis of $ V $. Let $ A \subseteq V $ be a non-empty LI subset of $ V $, and define $ \mathscr{S} $ the collection of all LI supersets of $ A $.

    \begin{lemma}{}{}
      Given a chain $ \{A_\alpha\}_{\alpha \in \mathcal{I}} \subseteq \mathscr{S} : A_1 \subseteq A_2 \subseteq \dots $, then $ \bigcup_{\alpha \in \mathcal{I}} A_\alpha \in \mathscr{S} $.
    \end{lemma}

    \begin{proofbox}
      \begin{proof}
        Set $ \mathcal{A} \equiv \bigcup_{\alpha \in \mathcal{I}} A_\alpha $. If $ A \subseteq A_\alpha \,\,\forall \alpha \in \mathcal{I} $, then trivially $ A \subseteq \mathcal{A} $. To prove the linear independence, consider a linear combination $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n $ in $ \mathcal{A} $, with $ n \in \N $, and choose an $ A_{\alpha_n} $ large enough so that $ v_1 , \dots , v_n \in A_{\alpha_n} $. Then, $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n = \ve{0} \implies \lambda_1 , \dots , \lambda_n = 0 $, as $ A_{\alpha_n} $ is LI by definition. Since $ n \in \N $ is generic, $ \mathcal{A} $ is LI.
      \end{proof}
    \end{proofbox}

    It is then clear that $ \mathscr{S} $ satisfies the hypotheses of Zorn's Lemma (\lref{lemma:zorn}), therefore it has a maximal element $ \bas $. Now, suppose $ \braket{\bas} \neq V $, i.e. $ \exists \ve{b} \in V - \braket{\bas} $, and consider the linear combination $ \mu \ve{b} + \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{b}_n = \ve{0} $, with $ \ve{v}_1 , \dots , \ve{v}_n \in \bas $ and $ n \in \N $: then $ - \mu \ve{b} \in \braket{\bas} $, but $ \ve{b} \notin \braket{\bas} $, so $ \mu = 0 $ (as $ \ve{b} \neq \ve{0} \in \braket{\bas} $). Consequently, $ \lambda_1 = \dots = \lambda_n = 0 $ as $ \bas $ is LI, thus $ \bas \cup \{\ve{b}\} $ is LI and a superset of $ \bas \in \mathscr{S} $, which contradicts $ \bas $ being a maximal element of $ \mathscr{S} \absurd $

    Having showed that every LI subset $ A \subseteq V $ can be extended to a basis $ \bas $ of $ V $, the thesis is trivially found taking $ A = \emptyset $, which is a subset of every non-trivial vector space.
  \end{proof}
\end{proofbox}

This, though trivial for finite-dimensional spaces, is quite impressive for infinite-dimensional ones (for dimensionality, see \secref{sssec:dimensionality}).

\begin{proposition}{}{}
  Given a $ \K $-vector space $ V $, then $ S \subseteq V $ is a basis of $ V $ if and only if every element of $ V $ has a unique representation as a linear combination of elements of $ S $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Note that two representations are equal if they differ only by vanishing coefficients.

    $ (\Rightarrow) $ As $ V = \braket{S} $, then every $ \ve{v} \in V $ can be written as a linear combination of elements of $ S $. Suppose that $ \ve{v} $ has two representations:
    \begin{equation*}
      \ve{v} = \lambda_1 \ve{s}_1 + \dots \lambda_n \ve{s}_n
      \qquad \qquad
      \ve{v} = \mu_1 \ve{t}_1 + \dots + \mu_m \ve{t}_m
    \end{equation*}
    with $ \{\ve{s}_j\}_{j = 1, \dots, n} , \{\ve{t}_k\}_{k = 1, \dots, m} \subseteq S $ and $ \{\lambda_j\}_{j = 1, \dots, n} , \{\mu_k\}_{k = 1, \dots, m} \subseteq \K $. Now, we can extend both representations by adding vanishing coefficients, so that both include the same vectors of $ S $:
    \begin{equation*}
      \ve{v} = \zeta_1 \ve{v}_1 + \dots + \zeta_r \ve{v}_r
      \qquad \qquad
      \ve{v} = \xi_1 \ve{v}_1 + \dots + \xi_r \ve{v}_r
    \end{equation*}
    with $ \{\ve{v}_j\}_{j = 1, \dots, r} \subseteq S $ and $ \{\zeta_j\}_{j = 1, \dots, r} , \{\xi_j\}_{j = 1, \dots, r} \subseteq \K $. Subtracting these two expressions:
    \begin{equation*}
      \ve{0} = \left( \zeta_1 - \xi_1 \right) \ve{v}_1 + \dots + \left( \zeta_r - \xi_r \right) \ve{v}_r
    \end{equation*}
    But $ S $ is LI, hence $ \zeta_j = \xi_j \,\,\forall j = 1, \dots, r $, i.e. the two representations are equal.

    $ (\Leftarrow) $ As every $ \ve{v} \in V $ can be written as a linear combination of elements of $ S $, then $ V = \braket{S} $. We only have to prove that $ S $ is LI. Consider $ \ve{0} \in V $: by hypothesis, it has a unique representation as a linear combination of vectors in $ S $, and a possible representation is $ \ve{0} = 0 \cdot \ve{s} $ for some $ \ve{s} \in S $, i.e. the trivial representation with all vanishing coefficients. Now, consider a linear combination in $ S $:
    \begin{equation*}
      \lambda_1 \ve{s}_1 + \dots + \lambda_n \ve{s}_n = \ve{0}
    \end{equation*}
    with $ n \in \N $. This too is a representation of $ \ve{0} $, hence $ \lambda_j = 0 \,\,\forall j = 1, \dots, n $ by the uniqueness of the representation. As $ n \in \N $ is generic, this is the definition of $ S $ being LI.
  \end{proof}
\end{proofbox}

\subsubsection{Bases of finitely-generated vector spaces}

We now turn our attention to finitely-generated vector spaces, i.e. $ V = \braket{\ve{v}_1 , \dots , \ve{v}_n} $ with $ n \in \N $.

\begin{proposition}{}{}
  Given a $ \K $-vector space $ V = \braket{\ve{v}_1 , \dots , \ve{v}_n} $, then $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ contains a basis of $ V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    If $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ is LI, then it is a basis of $ V $, so consider $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ LD, i.e. $ \exists \ve{v} \in \braket{\{\ve{v}_1 , \dots , \ve{v}_n\} - \{\ve{v}\}} $. WLOG, consider $ \ve{v} = \ve{v}_n $, so that $ V = \braket{\ve{v}_1 , \dots , \ve{v}_{n-1}} $: reiterating this procedure, all LD vectors are eliminated, leaving a basis of $ V $, as at most only a single vector $ \ve{v}_1 $ remains ($ \ve{v}_1 \neq \ve{0} $ as it is LI).
  \end{proof}
\end{proofbox}

A direct corollary is that every finitely-generated vector space admits a finite basis, found by the elimination algorithm highlighted in the previous proof.

\begin{definition}{MSLIV}{}
  Given a $ \K $-vector space $ V $, then a LI subset $ \{\ve{v}_1 , \dots , \ve{v}_n\} \subseteq V $ is a \bcdef{maximal set of linearly-independent vectors} (MSLIV) if $ \{\ve{v}_1 , \dots , \ve{v}_n\} \cup \{\ve{v}\} $ is LD $ \forall \ve{v} \in V $.
\end{definition}

We extend this notion considering $ V = \{\ve{v}_1 , \dots , \ve{v}_n\} $: then, a LI subset $ \{\ve{v}_{j_1} , \dots , \ve{v}_{j_r}\} \subseteq \{\ve{v}_1 , \dots , \ve{v}_n\} $, with $ r \leq n $, is a \bctxt{maximal subset of linearly-independent vectors} (MSLIV) if $ \{\ve{v}_{j_1} , \dots , \ve{v}_{j_r}\} \cup \{\ve{v}_j\} $ is LD $ \forall j \in \{1, \dots, n\} - \{j_1, \dots, j_r\} $. Trivially, a maximal subset of LI vectors is also a maximal set of LI vectors in $ V $, so the redundant acronym MSLIV is justified.
We can now prove that bases and MSLIVs are equivalent notions.

\begin{theorem}{Bases as MSLIVs}{basis-msliv}
  Given a non-trivial $ \K $-vector space $ V = \braket{\ve{v}_1 , \dots , \ve{v}_n} $, then $ \bas \subseteq V $ is a basis if and only if it is a MSLIV.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    $ (\Leftarrow) $ WLOG let $ \{\ve{v}_1 , \dots , \ve{v}_r\} $, with $ r \leq n $, be a MSLIV of $ \{\ve{v}_1 , \dots , \ve{v}_n\} $: then WTS $ V = \braket{\ve{v}_1 , \dots , \ve{v}_r} $. If $ r = n $ the proof is complete, so consider $ r < n $ and $ \ve{v}_j : r < j \leq n $: by definition $ \{\ve{v}_1 , \dots , \ve{v}_r\} \cup \{\ve{v}_j\} $ is LD, i.e. $ \exists \{\lambda_{j_k}\}_{k = 1, \dots, r} \subseteq \K : \ve{v}_i = \lambda_{j_1} \ve{v}_1 + \dots + \lambda_{j_r} \ve{v}_r $, which means that $ \ve{v}_i \in \braket{\ve{v}_1 , \dots , \ve{v}_r} \implies V = \braket{\{\ve{v}_1 , \dots , \ve{v}_n\} - \{\ve{v}_i\}} $. This holds $ \forall i \in [r+1, n] \subseteq \N $, hence $ V = \braket{\ve{v}_1 , \dots , \ve{v}_r} $.

    $ (\Rightarrow) $ Let $ \bas = \{\ve{v}_1 , \dots , \ve{v}_n\} $ be a basis of $ V $and $ \{\ve{w}_1 , \dots , \ve{w}_m\} \ssq V : m > n $, and suppose this is LI. By definition $ \exists \lambda_1 , \dots , \lambda_n \in \K : \ve{w}_1 = \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n $, but $ \ve{w}_1 $ is LI, therefore $ \exists j \in [1, \dots, n] \subseteq \N : \lambda_j \neq 0 $. WLOG $ j = 1 $, hence $ \ve{v}_1 \in \braket{\ve{w}_1 , \ve{v}_2 , \dots , \ve{v}_n} $. Iterating, we can substitute $ \ve{v}_1 , \dots , \ve{v}_n $ with $ \ve{w}_1 , \dots , \ve{w}_n $: indeed, supposing that $ \{v_1 , \dots , \ve{v}_r\} $ have been substituted with $ \ve{w}_1 , \dots , \ve{w}_r $, with $ 1 \leq r < n $, then $ \ve{v}_{r+1} $ can be substituted with $ \ve{w}_{r+1} $ as $ V = \braket{\ve{w}_1 , \dots , \ve{w}_r , \ve{v}_{r+1} , \dots , \ve{v}_n} \implies \exists \alpha_1 , \dots , \alpha_r , \beta_{r+1} , \dots , \beta_n \in \K : \ve{w}_{r+1} = \alpha_1 \ve{w}_1 + \dots + \alpha_r \ve{w}_r + \beta_{r+1} \ve{v}_{r+1} + \dots + \beta_n \ve{v}_n $, but $ \{\ve{w}_1 , \dots , \ve{w}_{r+1}\} $ are LI, thus $ \exists j \in [r+1 , n] \subseteq \N : \beta_j \neq 0 $, and WLOG $ j = r+1 $ by reordering indices. Performing the reiteration $ V = \braket{\ve{w}_1 , \dots , \ve{w}_n} $, so $ \ve{w}_{n+1} $ is a linear combination of $ \{\ve{w}_1 , \dots , \ve{w}_n\} \absurd $
  \end{proof}
\end{proofbox}

There is still another equivalent concept to introduce.

\begin{definition}{MSG}{}
  Given a $ \K $-vector space $ V $, then $ \{\ve{v}_1 , \dots , \ve{v}_n\} \ssq V $ is a \bcdef{minimal set of generators} (MSG) if $ V = \braket{\ve{v}_1 , \dots , \ve{v}_n} $ and $ \{\ve{v}_1 , \dots , \ve{v}_n\} - \{\ve{v}_j\} $ does not generate $ V \,\,\forall j = 1, \dots , n $.
\end{definition}

\begin{theorem}{Bases ad MSGs}{basis-msg}
  Given a non-trivial $ \K $-vector space $ V $, then $ \bas \ssq V $ is a basis of $ V $ if and only if it is a MSG.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    $ (\Leftarrow) $ Let $ \{\ve{v}_1 , \dots , \ve{v}_n\} \ssq V $ be a MSG: then WTS $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ is LI. Consider a linear combination $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n = \ve{0} $ and suppose $ \lambda_1 \neq 0 $: this allows to express $ \ve{v}_1 $ as a linear combination of $ \{\ve{v}_2 , \dots , \ve{v}_n\} $, but then $ V = \braket{\ve{v}_2 , \dots , \ve{v}_n} \absurd $

    $ (\Rightarrow) $ Suppose $ \bas = \{\ve{v}_1 , \dots , \ve{v}_n\} $ is not a MSG, and WLOG $ V = \braket{\ve{v}_2 , \dots , \ve{v}_n} $: then $ \ve{v}_1 $ can be expressed as linear combination of $ \{\ve{v}_2 , \dots , \ve{v}_n\} $, i.e. $ \bas $ is LD $ \absurd $
  \end{proof}
\end{proofbox}

This shows that bases, MSLIVs and MSGs are all equivalent notions.

\subsubsection{Dimensionality}
\label{sssec:dimensionality}

To properly define the concept of dimensionality of a vector space, we first have to prove that all bases are equivalent.

\begin{theorem}{Equicardinality of bases}{basis-equicardinality}
  Given a non-trivial $ \K $-vector space $ V $ and two bases $ \bas_1 = \{\ve{v}_1 , \dots , \ve{v}_n\} , \bas_2 = \{\ve{w}_1 , \dots , \ve{w}_m\} $, then $ n = m $.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    As $ \bas_1 $ is a MSLIV by \tref{th:basis-msliv}, then every subset of $ n + 1 $ vectors in $ V $ is LD, hence $ m \leq n $ as $ \bas_2 $ must be LI. The vice versa applies too, hence $ n = m $.
  \end{proof}
\end{proofbox}

By this theorem, all bases of finitely-generated spaces are equivalent, since the equicardinality ensures that we can define a bijection $ f : \bas_1 \leftrightarrow \bas_2 \,\,\forall \bas_1 , \bas_2 $ bases of $ V $.

Moreover, this result hints to the fact that the cardinality of the bases of $ V $ is a fundamental property of the vector space, linked to hits dimensionality, so we give a proper definition of this quantity.

\begin{definition}{Dimension}{}
  Given a $ \K $-vector space $ V $, then we define its \bcdef{dimension} as:
  \begin{equation*}
    \dim_\K V \defeq
    \begin{cases}
      0 & V = \{\ve{0}\} \\
      n & \abs{\bas} = n \,\,\forall \bas \text{ basis of } V \\
      \infty & V \text{ not finitely-generated}
    \end{cases}
  \end{equation*}
\end{definition}

The dimension of a vector space is a well-defined quantity by \tref{th:basis} and \tref{th:basis-equicardinality}.

\begin{example}{Various spaces}{}
  Trivially, $ \dim_\K \K^n = n $, so $ \dim_\C \C^n = n $ and $ \dim_\R \C^n = 2n $, while $ \dim_\R \R^\R = \infty $.
\end{example}

We can now give some trivial properties of dimensionality.

\begin{lemma}{Basic property of dimension}{}
  Given an $ n $-dimensional $ \K $-vector space $ V $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item $ \{\ve{v}_1 , \dots , \ve{v}_m\} \ssq V $ is LD $ \forall m > n $;
    \item $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ LI is a basis of $ V $;
    \item $ \{\ve{v}_1 , \dots , \ve{v}_n\} $ set of generators of $ V $ is a basis of $ V $.
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    These results are corollaries of \tref{th:basis-msliv} and \tref{th:basis-msg}.
  \end{proof}
\end{proofbox}

\begin{proposition}{Dimension of subspaces}{}
  Given $ \dim_\K V = n $ and a subspace $ U \ssq V $, then $ \dim_\K U \equiv k \leq n $ and $ k = n \iff U = V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    The case $ U = \{\ve{0}\} $ is trivial, so consider $ U \neq \{\ve{0}\} $. Let $ \ve{u}_1 \in U $ LI and add $ \ve{u}_2 , \ve{u}_3 , \dots \in U $ to get $ \{\ve{u}_1 , \ve{u}_2\} , \{\ve{u}_1 , \ve{u}_2 , \ve{u}_3\} , \dots $: a LD subset is reached in at most $ n $ steps. Let WLOG $ \{\ve{u}_1 , \dots , \ve{u}_k\} $ the MSLIV of $ \{\ve{u}_1 , \dots , \ve{u}_n\} $, with $ k \leq n $: by \tref{th:basis-msliv}, this is a basis of $ U $, hence $ k = \dim_\K U \leq n $.

    $ U = V \implies k = n $ is trivial, while $ k = n \implies \{\ve{u}_1 , \dots , \ve{u}_n\} $ is a MSLIV of $ V $, hence a basis of $ V $, so $ V = \braket{\ve{u}_1 , \dots , \ve{u}_n} = V $.
  \end{proof}
\end{proofbox}

A consequence of this theorem is the fact that LI subset $ \{\ve{v}_1 , \dots , \ve{v}_r\} \ssq V $, with $ r < n $, can always be completed to a basis, i.e. $ \exists \ve{w}_{r+1} , \dots , \ve{w}_n \in V : \{\ve{v}_1 , \dots, \ve{v}_r , \ve{w}_{r+1} , \ve{w}_n\} $ is a basis of $ V $.

\begin{theorem}{Grassmann's Theorem}{}
  Given a $ \K $-vector space $ V $ and finitely-generated subspaces $ X,Y \ssq V $, then:
  \begin{equation}
    \dim_\K X + \dim_\K Y = \dim_\K \left( X + Y \right) + \dim_\K \left( X \cap Y \right)
  \end{equation}
\end{theorem}

\begin{proofbox}
  \begin{proof}
    Let $ \bas_X = \{\ve{x}_1 , \dots , \ve{x}_r\} , \bas_Y = \{\ve{y}_1 , \dots , \ve{y}_s\} $ be bases of $ X , Y $ and $ m \equiv \dim_\K \left( X \cap Y \right) $.
    If $ m = 0 $, then $ X \cap Y = \{\ve{0}\} $, while if $ m \ge 1 $ let $ \bas_{XY} = \{\ve{v}_1 , \dots , \ve{v}_m\} $ be a basis of $ X \cap Y $, which is a finitely-generated subspace by \lref{lemma:subspace-intersection}. Then, completing the bases, $ \exists \ve{x}_{m+1} , \dots, \ve{x}_r \in X : \{\ve{v}_1 , \dots , \ve{v}_m , \ve{x}_{m+1} , \dots , \ve{x}_r\} $ is a basis of $ X $ and $ \exists \ve{y}_{m+1} , \dots , \ve{y}_s \in Y : \{\ve{v}_1 , \dots , \ve{v}_m , \ve{y}_{m+1} , \dots , \ve{y}_s\} $ is a basis of $ Y $ (WLOG same vectors as in $ \bas_X $ and $ \bas_Y $). Now, WTS $ \dim_\K \left( X + Y \right) = r + s - m $, so consider $ \bas = \{\ve{v}_1 , \dots , \ve{v}_m , \ve{x}_{m+1} , \dots , \ve{x}_r , \ve{y}_{m+1} , \dots , \ve{y}_s\} $:
    \begin{itemize}
      \item $ X + Y \defeq \{\ve{v} = \ve{x} + \ve{y} : \ve{x} \in X , \ve{y} \in Y\} $, but $ \ve{x} \in \braket{\ve{v}_1 , \dots , \ve{v}_m , \ve{x}_{m+1} , \dots , \ve{x}_r} $ and $ \ve{y} \in \braket{\ve{v}_1 , \dots , \ve{v}_m , \ve{y}_{m+1} , \dots , \ve{y}_s} $, so $ \ve{x} + \ve{y} \in \braket{\ve{v}_1 , \dots , \ve{v}_m , \ve{x}_{m+1} , \dots , \ve{x}_r , \ve{y}_{m+1} , \dots , \ve{y}_s} $, i.e. $ X + Y = \braket{\bas} $;
      \item consider the following linear combination:
        \begin{equation*}
          \alpha_1 \ve{v}_1 + \dots + \alpha_m \ve{v}_m + \beta_{m+1} \ve{x}_{m+1} + \dots + \beta_r \ve{x}_r + \gamma_{m+1} \ve{y}_{m+1} + \dots + \gamma_s \ve{y}_s = \ve{0}
        \end{equation*}
        and rearrange it as:
        \begin{equation*}
          \underbrace{\alpha_1 \ve{v}_1 + \dots + \alpha_m \ve{v}_m + \beta_{m+1} \ve{x}_{m+1} + \dots + \beta_r \ve{x}_r}_{\in \, X} = \underbrace{- \gamma_{m+1} \ve{y}_{m+1} - \dots - \gamma_s \ve{y}_s}_{\in \, Y}
        \end{equation*}
        Therefore, both expressions are in $ X \cap Y = \braket{\ve{v}_1 , \dots , \ve{v}_m} $, hence \exists $ \delta_1, \dots, \delta_m \in \K $ such that:
        \begin{equation*}
          \delta_1 \ve{v}_1 + \dots + \delta_m \ve{v}_m + \gamma_{m+1} \ve{y}_{m+1} + \dots + \gamma_s \ve{y}_s = \ve{0}
        \end{equation*}
        But $ \bas_Y $ is a basis of $ Y $, i.e. LI, so $ \delta_1 = \dots = \delta_m = \gamma_{m+1} = \dots = \gamma_s = 0 $, thus:
        \begin{equation*}
          \alpha_1 \ve{v}_1 + \dots + \alpha_m \ve{v}_m + \beta_{m+1} \ve{x}_{m+1} + \dots + \beta_r \ve{x}_r = \ve{0}
        \end{equation*}
        But $ \bas_X $ is a basis of $ X $, i.e. LI, so $ \alpha_1 = \dots = \alpha_m = \beta_{m+1} = \dots = \beta_r = 0 $. This shows that $ \bas $ is LI.
    \end{itemize}
    By \dref{def:basis}, $ \bas $ is a basis of $ X + Y $, i.e. $ \dim_\K \left( X + Y \right) = r + s - m $.
  \end{proof}
\end{proofbox}

\begin{example}{Eucldean geometry}{}
  Consider $ V = \Vect_0(\R^3) $ and $ \alpha , \beta $ planes such that $ \ve{0} \in \alpha , \beta $: they then determine a line $ r \equiv \alpha \cap \beta \ni \{\ve{0}\} $. Setting $ X = \Vect_0(\alpha) $, $ Y = \Vect_0(\beta) $ and $ X \cap Y = \Vect_0(r) $, we correctly have $ 2 + 2 = 3 + 1 $.
\end{example}

\section{Linear applications}

A fundamental tool in mathematics are linear applications, which arise in every one of its fields of study.

\begin{definition}{Linear application}{}
  Given $ \K $-vector spaces $ V,W $, an application $ f : V \ra W $ is \bcdef{$ \K $-linear} if:
  \begin{equation*}
    f(\lambda \ve{v} + \mu \ve{w}) = \lambda f(\ve{v}) + \mu f(\ve{w})
    \quad
    \forall \lambda , \mu \in \K , \ve{v} , \ve{w} \in V
  \end{equation*}
\end{definition}

This condition means that $ \K $-linear applications preserve linear combinations.

\begin{example}{Matrices}{}
  Given $ \mt{A} \in \K^{m \times n} $, we can associate to it an application $ L_\mt{A} : \K^n \ra \K^m : \ve{v} \mapsto \mt{A} \ve{v} $, which is $ \K $-linear by the linearity of the matrix product. Note that $ L_{\mt{I}_n} = \id_{\K^n} $.

  Moreover, given $ \mt{A} \in \K^{m \times n} $ and $ \mt{B} \in \K^{n \times p} $, then $ L_\mt{A} \circ L_\mt{B} = L_{\mt{A} \cdot \mt{B}} : \K^p \ra \K^m $ by the following commutative diagram:
  \begin{equation*}
    \begin{tikzcd}
      \K^m \arrow[rd, "L_{\mt{A} \cdot \mt{B}}", swap] \arrow[r, "L_{\mt{A}}"]
      & \K^n \arrow[d, "L_{\mt{B}}"] \\
      & \K^p
    \end{tikzcd}
  \end{equation*}
\end{example}

We can now state some properties of linear applications.

\begin{lemma}{Basic properties of linear applications}{}
  Given $ \K $-vector spaces $ V,W,Z $ and $ \K $-linear applications $ f : V \ra W , g : W \ra Z $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item $ f(\ve{0}_V) = \ve{0}_W $
    \item $ g \circ f : V \ra Z $ is $ \K $-linear;
    \item If $ f $ is bijective, then $ f^{-1} : W \ra V $ is $ \K $-linear.
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item $ f(\ve{0}_V) = f(0_\K \cdot \ve{v}) = 0_\K \cdot f(\ve{v}) = \ve{0}_W $
      \item $ g \circ f (\lambda \ve{u} + \mu \ve{v}) = g(\lambda f(\ve{u}) + \mu f(\ve{v})) = \lambda g(f(\ve{u})) + \mu g(f(\ve{v})) $
      \item $ f(\lambda f^{-1}(\ve{u}) + \mu f^{-1}(\ve{v})) = \lambda \ve{u} + \mu \ve{v} \implies f^{-1}(\lambda \ve{u} + \mu \ve{v}) = \lambda f^{-1}(\ve{u}) + \mu f^{-1}(\ve{v}) $
    \end{enumerate}
    \ensp
  \end{proof}
\end{proofbox}

We can also prove an existence-uniqueness theorem for linear applications.

\begin{theorem}{Existence and uniqueness}{}
  Let $ V,W $ be $ \K $-vector spaces, $ \bas = \{\ve{b}_1 , \dots , \ve{b}_n\} $ a basis of $ V $ and $ \{\ve{w}_1 , \dots , \ve{w}_n\} \ssq W $ an ordered set of vectors. Then $ \exists ! \varphi : V \ra W : \varphi(\ve{b}_j) = \ve{w}_j \,\,\forall j = 1, \dots , n $ which is $ \K $-linear.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    Let $ \ve{v} \in V $; then $ \exists \alpha_1 , \dots , \alpha_n \in \K : \ve{v} = \alpha_1 \ve{b}_1 + \dots + \alpha_n \ve{b}_n $ fixed since $ \bas $ is a basis. Now, define $ \varphi : V \ra W : \varphi(\ve{v}) = \alpha_1 \ve{w}_1 + \dots \alpha_n \ve{w}_n $: clearly $ \varphi(\ve{b}_j) = \ve{w}_j \,\,\forall j = 1, \dots, n $, and also $ \varphi $ is unique since both $ \{\alpha_j\}_{j = 1, \dots, n} \ssq \K $ and $ \{\ve{w}_j\}_{j = 1 , \dots, n} \ssq W $ are fixed. Finally, $ \varphi $ is $ \K $-linear, since $ f(\lambda \ve{v}_1 + \mu \ve{v}_2) = \left( \lambda \alpha_1 + \mu \beta_1 \right) \ve{w}_1 + \dots + \left( \lambda \alpha_n + \mu \beta_n \right) \ve{w}_n = \lambda f(\ve{v}_1) + \mu f(\ve{v}_2) $.
  \end{proof}
\end{proofbox}

In general, fixed $ \dim_\K V = n $, then given two sets $ \{\ve{v}_1 , \dots , \ve{v}_k\} \ssq V $ and $ \{\ve{w}_1 , \dots , \ve{w}_k\} \ssq W $, with $ k \in \N $, then the existence of $ \varphi : V \ra W : \varphi(\ve{v}_j) = \ve{w}_j \,\,\forall j = 1, \dots , k $ is only granted if $ \{\ve{v}_1 , \dots , \ve{v}_k\} $ is LI: in this case, if $ n = k $ then $ \varphi $ is unique too, by the previous theorem, while if $ k < n $ in general we can define multiple $ \varphi $ with such property, as we can complete $ \{\ve{v}_1 , \dots , \ve {v}_k\} $ to a basis of $ V $, which can then be mapped to arbitrary vectors in $ W $. On the other hand, if $ \{\ve{v}_1 , \dots , \ve{v}_k\} $ is LD, then $ \varphi $ can be defined only if $ \{\ve{w}_1 , \dots , \ve{w}_k\} $ satisfies the same linear-dependence relations, otherwise linearity cannot be satisfied.

Given two $ \K $-vector space $ V $ and $ W $, we denote the set of all $ \K $-linear applications $ f : V \ra W $ as $ \hmk(V,W) $: this has a natural structure of $ \K $-vector space with $ (f + g)(\ve{v}) \equiv f(\ve{v}) + g(\ve{v}) $ and $ (\lambda \cdot f)(\ve{v}) = \lambda \cdot f(\ve{v}) $.

\begin{definition}{Kernel and image}{}
  Given $ f \in \hmk(V,W) $, its \bcdef{kernel} is defined as $ \ker{f} \defeq \{\ve{v} \in V : f(\ve{v}) = \ve{0}_W\} \ssq V $, while its \bcdef{image} (or range) is defined as $ \ran{f} \defeq \{\ve{w} \in W : \exists \ve{v} \in V : \ve{w} = f(\ve{v})\} \ssq W $.
\end{definition}

\begin{lemma}{Kernel and image as subspaces}{}
  Given $ f \in \hmk(V,W) $, then $ \ker{f} $ is a subspace of $ V $ and $ \ran{f} $ is a subspace of $ W $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    By the linearity of $ f $, given $ \ve{v} , \ve{v}' \in V $ and $ \ve{w} , \ve{w}' \in W $:
    \begin{equation*}
      f(\lambda \ve{v} + \mu \ve{v}') = \lambda f(\ve{v}) + \mu f(\ve{v}') = \lambda \cdot \ve{0}_W + \mu \ve{0}_W = \ve{0}_\ve{w}
    \end{equation*}
    \begin{equation*}
      \ran{f} \ni \lambda \ve{w} + \mu \ve{w}' = \lambda f(\ve{v}) + \mu f(\ve{v}') = f(\lambda \ve{v} + \mu \ve{v}')
    \end{equation*}
    Thus, both $ \ker{f} $ and $ \ran{f} $ are closed under linear combinations, i.e. vector spaces.
  \end{proof}
\end{proofbox}

We can further carachterize the kernel and the image of a linear application.

\begin{proposition}{Kernel and injections}{kernel-injections}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces and $ f \in \hmk(V,W) $. Then the following conditions are equivalent:
  \begin{enumerate}[label = {\alph*.}]
    \item $ f $ is injective;
    \item $ \ker{f} = \{\ve{0}_V\} $
    \item $ \{\ve{v}_1 , \dots , \ve{v}_k\} \ssq V \text{ LI } \implies \{f(\ve{v}_1 , \dots , f(\ve{v}_k)\} \ssq W \text{ LI} $
  \end{enumerate}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Consider the following implications:

    $ (\text{a} \Rightarrow \text{b}) $ Suppose $ \exists \ve{v} \in \ker{f} : \ve{v} \neq \ve{0}_V $; then $ f(\ve{v}) = \ve{0}_W = f(\ve{0}_V) $, but $ f $ is injective $ \absurd $

    $ (\text{b} \Rightarrow \text{c}) $ Let $ \{\ve{v}_1 , \dots , \ve{v}_k\} \ssq V $ LI and consider $ \ve{0}_W = \lambda_1 f(\ve{v}_1) + \dots + \lambda_k f(\ve{v}_k) = f(\lambda_1 \ve{v}_1 + \dots + \lambda_k \ve{v}_k) $, hence $ \lambda_1 \ve{v}_1 + \dots + \lambda_k \ve{v}_k = \ve{0}_V $ as $ \ker{f} = \{\ve{0}_V\} $, therefore $ \lambda_1 = \dots = \lambda_k = 0 $ as $ \{\ve{v}_1 , \dots , \ve{v}_k\} $ LI.

    $ (\text{c} \Rightarrow \text{a}) $ Given $ \ve{v}_1 , \ve{v}_2 \in V $, by linearity $ f(\ve{v}_1) = f(\ve{v}_2) \implies f(\ve{v}_1 - \ve{v}_2) = \ve{0}_W $, so suppose $ \ve{v}_1 \neq \ve{v}_2 $: then $ \ve{v} \equiv \ve{v}_1 - \ve{v}_2 \neq \ve{0}_V $, i.e. LI, but $ f(\ve{v}) = \ve{0}_W $, i.e. LD $ \absurd $
  \end{proof}
\end{proofbox}

\begin{proposition}{Image and surjections}{image-surjections}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces and $ f \in \hmk(V,W) $. Then the following conditions are equivalent:
  \begin{enumerate}[label = {\alph*.}]
    \item $ f $ is surjective;
    \item $ \ran{f} = W $
    \item $ V = \braket{\ve{v}_1 , \dots , \ve{v}_k} \implies W = \braket{f(\ve{v}_1) , \dots , f(\ve{v}_k)} $
  \end{enumerate}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Consider the following implications:

    $ (\text{a} \Rightarrow \text{b}) $ Suppose $ \ran{f} \ssnq W \implies \exists \ve{w} \in W : \nexists \ve{v} \in V : \ve{w} = f(\ve{v}) \implies f $ not surjective $ \absurd $

    $ (\text{b} \Rightarrow \text{c}) $ $ \ran{f} = W \implies \forall \ve{w} \in W \,\exists \ve{v} \in V : \ve{w} = f(\ve{v}) $; moreover, $ V = \braket{\ve{v}_1 , \dots , \ve{v}_k} \implies \forall \ve{v} \in V \,\exists \lambda_1 , \dots , \lambda_k \in \K : \ve{v} = \lambda_1 \ve{v}_1 + \dots + \lambda_k \ve{v}_k $. Then $ \forall \ve{w} \in W \,\exists \lambda_1 , \dots , \lambda_k \in \K : \ve{w} = \lambda_1 f(\ve{v}_1) + \dots + \lambda_k f(\ve{v}_k) $, i.e. $ W = \braket{f(\ve{v}_1) , \dots , f(\ve{v}_k)} $.

    $ (\text{c} \Rightarrow \text{a}) $ $ W = \braket{f(\ve{v}_1) , \dots , f(\ve{v}_k)} \implies \forall \ve{w} \in W \,\exists \lambda_1 , \dots , \lambda_k \in \K : \ve{w} = \lambda_1 f(\ve{v}_1) + \dots + \lambda_k f(\ve{v}_k) = f(\lambda_1 \ve{v}_1 + \dots + \lambda_k \ve{v}_k) $, but $ V = \braket{\ve{v}_1 , \dots , \ve{v}_k} $, so $ \forall \ve{w} \in W \,\exists \ve{v} \in V : \ve{w} = f(\ve{v}) $.
  \end{proof}
\end{proofbox}

In general, even for non-surjective $ f \in \hmk $, it is true that $ V = \braket{\ve{v}_1 , \dots , \ve{v}_k} \implies \ran{f} = \braket{f(\ve{v}_1) , \dots , f(\ve{v}_k)} $ with a reasoning analogous to the previous proof.

As injections map LI vectors to LI vectors and surjections map generators to generators, we see that bijections map bases to bases.

\begin{theorem}{Rank--nullity theorem}{rank-nullity}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces and $ f \in \hmk(V,W) $. Then:
  \begin{equation}
    \dk V = \dk \ker{f} + \dk \ran{f}
    \label{eq:rank-nullity}
  \end{equation}
\end{theorem}

\begin{proofbox}
  \begin{proof}
    As $ \ker{f} \ssq V $ and $ \ran{f} \ssq W $, they are both finitely-generated. If $ \ran{f} = \{\ve{0}_W\} $ (trivial map), then $ \ker{f} = V $ and the thesis is verified.

    Consider $ \ran{f} \neq \{\ve{0}_W\} $ and choose a basis $ \{\ve{c}_1 , \dots , \ve{c}_k\} $ of $ \ran{f} $: this means that $ \exists \ve{b}_1 , \dots , \ve{b}_k \in V : f(\ve{b}_j) = \ve{c}_j \,\,\forall j = 1, \dots, k $. Now, if $ \ker{f} \neq \{\ve{0}_V\} $ choose a basis $ \{\ve{a}_1 , \dots , \ve{a}_r\} $ of $ \ker{f} $, otherwise consider no other vectors, and set $ \bas \equiv \{\ve{a}_1 , \dots , \ve{a}_r , \ve{b}_1 , \dots , \ve{b}_k\} \ssq V $. WTS $ \bas $ is a basis of $ V $:
    \begin{itemize}
      \item consider the following linear combination:
        \begin{equation*}
          \alpha_1 \ve{a}_1 + \dots + \alpha_r \ve{a}_r + \beta_1 \ve{b}_1 + \dots + \beta_k \ve{b}_k = \ve{0}_V
        \end{equation*}
        Then, by the linearity of $ f $:
        \begin{equation*}
          \begin{split}
            \ve{0}_W = f(\ve{0}_V)
            & = f(\alpha_1 \ve{a}_1 + \dots + \alpha_r \ve{a}_r + \beta_1 \ve{b}_1 + \dots + \beta_k \ve{b}_k) \\
            & = \alpha_1 \cdot \ve{0}_W + \dots + \alpha_r \cdot \ve{0}_W + \beta_1 \ve{c}_1 + \dots + \beta_k \ve{c}_k = \beta_1 \ve{c}_1 + \dots + \beta_k \ve{c}_k
          \end{split}
        \end{equation*}
        But $ \{\ve{c}_1 , \dots , \ve{c}_k\} $ is a basis of $ \ran{f} $, hence $ \beta_1 = \dots = \beta_k $ due to linear independence. Then $ \alpha_1 \ve{a}_1 + \dots + \alpha_r \ve{a}_r = \ve{0}_V $, but $ \{\ve{a}_1 , \dots , \ve{a}_r\} $ is a basis of $ \ker{f} $, so $ \alpha_1 = \dots = \alpha_r = 0 $;
      \item $ \ve{v} \in V \implies f(\ve{v}) \in \ran{f} = \braket{f(\ve{b}_1) , \dots , f(\ve{b}_k)} $, so $ \exists \gamma_1 , \dots , \gamma_k \in \K : f(\ve{v}) = \gamma_1 f(\ve{b}_1) + \dots + \gamma_k f(\ve{b}_k) $, which rearranging and using the linearity of $ f $ becomes $ f(\ve{v} - \gamma_1 \ve{v}_1 - \dots - \gamma_k \ve{b}_k) = \ve{0}_W $, i.e. $ \ve{v} - \gamma_1 \ve{v}_1 - \dots - \gamma_k \ve{v}_k \in \ker{f} = \braket{\ve{a}_1 , \dots , \ve{a}_r} $. Then, $ \exists \delta_1 , \dots , \delta_r \in \K : \ve{v} = \gamma_1 \ve{v}_1 + \dots + \gamma_k \ve{v}_k + \delta_1 \ve{a}_1 + \dots + \delta_r \ve{a}_r $, which shows that $ V = \braket{\bas} $.
    \end{itemize}
    By \dref{def:basis}, $ \bas $ is a basis of $ V $, i.e. $ \dk V = \dk \ker{f} + \dk \ran{f} $.
  \end{proof}
\end{proofbox}

The name of this theorem will be clear in \secref{sssec:rank}.

\begin{corollary}{Equidimensionality and bijections}{equidimensionality-bijections}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces and $ f \in \hmk(V,W) $. Then:
  \begin{equation*}
    \dk V = \dk W
    \qquad \implies \qquad
    f \text{ injective } \iff f \text{ surjective } \iff f \text{ bijective}
  \end{equation*}
\end{corollary}

\begin{proofbox}
  \begin{proof}
    By \pref{prop:kernel-injections}, $ f \text{ injective } \iff \ker{f} = \{\ve{0}_V\} \iff \dk \ker{f} = 0 $. By \tref{th:rank-nullity} $ \dk \ker{f} = 0 \iff \dk \ran{f} = \dk V = \dk W \iff \ran{f} = W $, and by \pref{prop:image-surjections} $ \ran{f} = W \iff f $ surjective. Hence, $ f $ is both injective and surjective, i.e. a bijection.
  \end{proof}
\end{proofbox}

We can further classify applications:
\begin{itemize}
  \item $ f \in \hmk(V,W) $ is a \bctxt{homomorphism};
  \item $ f \in \hmk(V,W) $ bijective is an \bctxt{isomorphism};
  \item $ f \in \hmk(V,V) \equiv \End(V) $ is an \bctxt{endomorphism};
  \item $ f \in \End(V) $ bijective is an \bctxt{automorphism}.
\end{itemize}
Isomorphisms are particularly interesting.

\begin{lemma}{Basic properties of isomorphisms}{}
  Given three $ \K $-vector spaces $ V,W,Z $ and $ f \in \hmk(V,W) , g \in \hmk(W,Z) $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item $ f $ is an isomorphism if and only if it is invertible;
    \item If $ f $ and $ g $ are isomorphisms, then $ g \circ f \in \hmk(V,Z) $ is an isomorphism.
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Trivial by the fact that invertibility is equivalent to bijectivity and that the composition of bijections is a bijection.
  \end{proof}
\end{proofbox}

\begin{example}{Matrices as endomorphisms}{}
  Given $ \mt{A} \in \K^{n \times n} $, then $ L_\mt{A} \in \End(\K^n) $. Moreover, if $ \mt{A} \in \GL{n,\K} $, then $ L_\mt{A} $ is an automorphism.
\end{example}

Isomorphism induce an equivalence relation between vector spaces.

\begin{definition}{Isomorphism relation}{}
  Two $ \K $-vector spaces $ V,W $ are \bcdef{isomorphic} $ V \cong W $ if $ \exists f \in \hmk(V,W) $ isomorphism.
\end{definition}

This is an equivalence relation since, if $ f $ is an isomorphism, then $ f^{-1} $ is an isomorphism too.

\begin{theorem}{Equidimensionality and isomorphisms}{equidimensionality-isomorphism}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces. Then:
  \begin{equation*}
    V \cong W
    \iff
    \dk V = \dk W
  \end{equation*}
\end{theorem}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ $ V \cong W \implies \exists f \in \hmk(V,W) $ isomorphism, which maps bases to bases, hence $ \dk V = \dk W $

    $ (\Leftarrow) $ Consider $ \bas = \{\ve{b}_1 , \dots , \ve{b}_n\} \ssq V $ basis of $ V $, so that $ \forall \ve{v} \in V \,\exists! \alpha_1 , \dots , \alpha_n \in \K : \ve{v} = \alpha_1 \ve{b}_1 + \dots + \alpha_n \ve{b}_n $. Then, define $ \varphi : V \ra \K^n : \varphi(\ve{v}) = \left( \alpha_1 , \dots , \alpha_n \right) $, which is clearly linear, so $ \varphi \in \hmk(V,\K^n) $. Moreover, $ \forall \bs{\alpha} \in \K^n \,\exists \ve{v} \in V : \ve{v} = \sum_{j = 1}^n \alpha_j \ve{b}_j $, as $ V = \braket{\bas} $, hence $ \forall \bs{\alpha} \in \K^n \,\exists \ve{v} \in V : \varphi(\ve{v}) = \bs{\alpha} $, i.e. $ \varphi $ is a surjection. Since $ \dk V = \dk \K^n $, by \cref{cor:equidimensionality-bijections} $ \varphi $ is a bijection too, thus $ V \cong \K^n $.

    Analogously, given a basis $ \mathcal{C} \ssq W $ of $ W $, we can construct an equivalent isomorphism $ \psi : W \ra \K^n $, so $ W \cong \K^n $. By the transitivity of the isomorphism relation, $ V \cong W $.
  \end{proof}
\end{proofbox}

The isomorphism relation then partitions the set of all finitely-generated vector spaces into equivalence classes composed of equidimensional spaces: for example, $ \C^n(\R) \cong \R^{2n} $ and $ \C_n[x] \cong \C^{n+1} $.

\subsection{Representative matrices}

Recalling that we can associate to each matrix $ \mt{A} \in \K^{m \times n} $ an application $ L_\mt{A} \in \hmk(\K^n,\K^m) : \ve{v} \mapsto \mt{A} \ve{v} $, it is clear that $ \ker{L_\mt{A}} $ is the solution space of the homogeneus linear system determined by $ \mt{A} \ve{x} = \ve{0} $, while $ \ran{L_\mt{A}} $ is the space of all constant terms $ \ve{b} $ which make the system $ \mt{A} \ve{x} = \ve{b} $ solvable. Moreover, the generators of $ \ran{L_\mt{A}} $ are the images of the generators of $ \K^n $: taking the Euclidean base $ \{\ve{e}_j\}_{j = 1, \dots, n} $, then:
\begin{equation*}
  L_\mt{A}(\ve{e}_j) =
  \begin{bmatrix}
    a_{11} & \dots & a_{1j} & \dots & a_{1n} \\
    \vdots & \ddots & \vdots & \ddots & \vdots \\
    a_{m1} & \dots & a_{mj} & \dots & a_{mn}
  \end{bmatrix}
  \begin{pmatrix}
    0 \\ \vdots \\ 1 \\ \vdots \\ 0
  \end{pmatrix}
  =
  \begin{pmatrix}
    a_{1j} \\ \vdots \\ a_{mj}
  \end{pmatrix}
\end{equation*}
We see, then, that the $ n $ columns of $ \mt{A} $ are the $ n $ column vectors which generate $ \ran{L_\mt{A}} $.

Now, the converse is possible too, i.e. to associate a matrix to a linear application. Consider two $ \K $-vector spaces $ V,W $ with respective bases $ \mathcal{A} = \{\ve{a}_1 , \dots , \ve{a}_n\} , \bas = \{\ve{b}_1 , \dots , \ve{b}_m\} $, and take $ f \in \hmk(V,W) $. By linearity, $ f $ is determined by its values on $ \mathcal{A} $, so suppose that:
\begin{equation}
  \begin{array}{l}
    f(\ve{a}_1) = \alpha_{11} \ve{b}_1 + \dots + \alpha_{1m} \ve{b}_m \\
    \qquad \ \ \, \vdots \\
    f(\ve{a}_n) = \alpha_{n1} \ve{b}_1 + \dots + \alpha_{nm} \ve{b}_m
  \end{array}
  \qquad \implies \qquad
  \mt{A} \equiv
  \begin{bmatrix}
    \alpha_{11} & \dots & \alpha_{1n} \\
    \vdots & \ddots & \vdots \\
    \alpha_{m1} & \dots & \alpha_{mn}
  \end{bmatrix}
  \label{eq:representative-matrix-definition}
\end{equation}
We want to show that $ f $ and $ L_\mt{A} $ are the ``same" application, i.e. we want to show that the following diagram commutes:
\begin{equation*}
  \begin{tikzcd}
    V \ar[d, "\varphi_\mathcal{A}", swap] \ar[r, "f"]
    & W \ar[d, "\varphi_\bas"] \\
    \K^n \ar[r, "L_\mt{A}", swap]
    & \K^m
  \end{tikzcd}
\end{equation*}
where $ \varphi_\mathcal{A} : V \ra \K^n $ and $ \varphi_\bas : W \ra \K^m $ are the representations of $ V $ and $ W $ on $ \K^n $ and $ \K^m $ in the respective bases, defined as:
\begin{equation*}
  V \ni \lambda_1 \ve{a}_1 + \dots + \lambda_n \ve{a}_n = \ve{v} \mapsto
  \begin{pmatrix}
    \lambda_1 \\ \vdots \\ \lambda_n
  \end{pmatrix}
  \in \K^n
  \qquad \quad
  W \ni \mu_1 \ve{b}_1 + \dots + \mu_m \ve{b}_m = \ve{w} \mapsto
  \begin{pmatrix}
    \mu_1 \\ \vdots \\ \mu_m
  \end{pmatrix}
  \in \K^m
\end{equation*}
Now, we can directly verify that $ L_\mt{A} \circ \varphi_\mathcal{A} = \varphi_\bas \circ f $, and in particular it is sufficient to show it on a basis:
\begin{equation*}
  L_\mt{A} \circ \varphi_\mathcal{A}(\ve{a}_j) = L_\mt{A}(\ve{e}_j) =
  \begin{pmatrix}
    \alpha_{1j} \\ \vdots \\ \alpha_{mj}
  \end{pmatrix}
  = \varphi_\bas(\alpha_{1j} \ve{b}_1 + \dots + \alpha_{mj} \ve{b}_m) = \varphi_\bas \circ f(\ve{a}_j)
\end{equation*}
Hence, the association between matrices and linear applications is bidirectional and well-defined, and in fact it defines an isomorphism $ \hmk(V,W) \cong \K^{m \times n} $.

\begin{definition}{Representative matrix}{representative-matrix}
  Let $ V,W $ be finitely-generated $ \K $-vector spaces with respective bases $ \mathcal{A} , \bas $. Then, the \bcdef{representative matrix} of $ f \in \hmk(V,W) $ is the matrix $ \mt{M}_\bas^\mathcal{A}(f) $ determined by the isomorphism $ \hmk(V,W) \leftrightarrow \K^{m \times n} : f \leftrightarrow \mt{M}_{\bas}^\mathcal{A}(f) $ defined by \eref{eq:representative-matrix-definition}.
\end{definition}

\begin{lemma}{Basic properties of representative matrices}{representative-matrix-properties}
  Given three finitely-generated $ \K $-vector spaces $ X,Y,Z $ with respective bases $ \mathcal{A} , \bas , \mathcal{C} $ and $ f \in \hmk(V,W) , g \in \hmk(W,Z) $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item $ \mt{M}_\mathcal{C}^\mathcal{A}(g \circ f) = \mt{M}_\mathcal{C}^\bas(g) \cdot \mt{M}_\bas^\mathcal{A}(f) $
    \item $ V = W \land \mathcal{A} = \bas \implies \mt{M}_\mathcal{A}^\mathcal{A}(\id_V) = \mt{I}_{\dk V} $
    \item $ f \text{ isomorphism } \implies \mt{M}_\bas^\mathcal{A}(f) \text{ invertible} \,\land\, [\mt{M}_\bas^\mathcal{A}(f)]^{-1} = \mt{M}_\mathcal{A}^\bas(f^{-1}) $
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    The first two propositions are true by the linearity of $ f $ and $ g $, while the last one is proved solving $ f^{-1} \circ f = \id_V \implies \mt{M}_\mathcal{A}^\bas(f^{-1}) \cdot \mt{M}_\bas^\mathcal{A}(f) = \id_{\dk V} $, where the first two properties where applied.
  \end{proof}
\end{proofbox}

When the bases $ \mathcal{A} $ and $ \bas $ are both the respective canonical bases of $ V $ and $ W $, then we denote the representative matrix of $ f $ simply as $ \mt{M}_f $.

\subsubsection{Change of bases}

To discuss how to perform a change of basis in a vector space, first we have to introduce two equivalence relations.

\begin{definition}{Equivalent matrices}{}
  Two matrices $ \mt{A} , \mt{B} \in \K^{m \times n} $ are \bcdef{equivalent} if $ \exists \mt{E} \in \GL{m,\K} , \mt{F} \in \GL{n,\K} : \mt{B} = \mt{E} \mt{A} \mt{F} $.
\end{definition}

\begin{definition}{Similar matrices}{}
  Two square matrices $ \mt{A} , \mt{B} \in \K^{n \times n} $ are \bcdef{similar} if $ \exists \mt{N} \in \GL{n,\K} : \mt{B} = \mt{N}^{-1} \mt{A} \mt{N} $.
\end{definition}

To illustrate how representation matrices change under a change of basis, consider a $ \K $-vector space $ V $ and two bases $ \mathcal{A} , \bas \ssq V $ (we denote $ V_\mathcal{A} , V_\bas $ the space with basis $ \mathcal{A} $ and $ \bas $ respectively), and take $ f \in \End{V} $. Then, consider the following commutative diagram:
\begin{equation*}
  \begin{tikzcd}
    V_\mathcal{A} \ar[r , "f"] \ar[d , "\id_V" , swap]
    & V_\mathcal{A} \ar[d , "\id_V"] \\
    V_\bas \ar[r , "f"]
    & V_\bas
  \end{tikzcd}
  \quad \implies \quad
  \underbrace{\mt{M}_\bas^\bas(f)}_{\in \, \K^{n \times n}} \cdot \underbrace{\mt{M}_\bas^\mathcal{A}(\id_V)}_{\in \, \GL{n,\K}} = \underbrace{\mt{M}_\bas^\mathcal{A}(\id_V)}_{\in \, \GL{n,\K}} \cdot \underbrace{\mt{M}_\mathcal{A}^\mathcal{A}(f)}_{\K^{n \times n}}
\end{equation*}
Hence, we see that representative matrices of the same endomorphism are similar. Moreover, we can define the change-of-basis matrix $ \mt{N}_\bas^\mathcal{A} \equiv \mt{M}_\bas^\mathcal{A}(\id_V) $, whose columns are the coefficients of the representations on $ \bas $ of the vectors of $ \mathcal{A} $. Note that, in the particular case $ f = \id_V $, the above equation proves that $ [\mt{N}_\bas^\mathcal{A}]^{-1} = \mt{N}_\mathcal{A}^\bas $.

A similar diagram can be drawn for the generalized case of $ f \in \hmk(V,W) $:
\begin{equation*}
  \begin{tikzcd}
    V_\mathcal{A} \ar[r , "f"] \ar[d , "\id_V" , swap]
    & W_\bas \ar[d , "\id_W"] \\
    V_{\mathcal{A}'} \ar[r , "f"]
    & W_{\bas'}
  \end{tikzcd}
  \quad \implies \quad
  \underbrace{\mt{M}_{\bas'}^{\mathcal{A}'}(f)}_{\in \, \K^{m \times n}} \cdot \underbrace{\mt{N}_\mathcal{A}^{\mathcal{A}'}}_{\in \, \GL{n,\K}} = \underbrace{\mt{N}_{\bas'}^\bas}_{\in \, \GL{m,\K}} \cdot \underbrace{\mt{M}_\bas^\mathcal{A}(f)}_{\K^{m \times n}}
\end{equation*}
Therefore, representative matrices of the same linear application are equivalent.

\subsection{Determinant and rank}

In order to continue our analysis of linear applications, we need to introduce two important notions: the determinant and the rank of a matrix.

\subsubsection{Determinant}

\begin{definition}{Determinant}{determinant}
  Given a square matrix $ \mt{A} \in \K^{n \times n} : \mt{A} = [a_{ij}]_{i,j = 1, \dots, n} $, its \bcdef{determinant} is defined as:
  \begin{equation}
    \det{\mt{A}} \defeq \sum_{\sigma \in \sy{n}} \sgn(\sigma) \prod_{i = 1}^n a_{i \sigma(i)}
    \label{eq:determinant-definition}
  \end{equation}
\end{definition}

Note that the determinant has $ n! $ terms, each containing one and only one element from each row and each column of $ \mt{A} $; moreover, it can be interpreted as an application $ \det : \K^{n \times n} \ra \K $. We can now prove some trivial properties.

\begin{lemma}{Basic properties of determinants}{determinant-basic}
  Let $ \mt{A} \in \K^{n \times n} $. Then:
  \begin{enumerate}[label = {\alph*.}]
    \item $ \det \mt{A}\tsp = \det \mt{A} $
    \item Swapping two rows or two columns, the determinant changes sign;
    \item If two rows or two columns are equal, then $ \det \mt{A} = 0 $;
    \item Keeping $ n - 1 $ columns (or rows) fixed, the determinant is a $ \K $-linear application with respect to the other column (or row);
    \item $ \det(\lambda \mt{A}) = \lambda^n \det \mt{A} \,\,\forall \lambda \in \K $
    \item $ \det \mt{I}_n = 1 $
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item Transposition exchanges columns and rows without altering their structure, but each term of the determinant has one and only one element from each row and each column, hence it is unchanged.
      \item Swapping two rows or two columns is achieved through a permutation $ \rho \in \sy{n} : \sgn{\rho} = -1 $, thus, as $ \sgn(\sigma \circ \rho) = \sgn(\sigma) \sgn(\rho) = - \sgn \sigma $, the determinant changes sign.
      \item By the previous property, exchanging two equal rows or columns $ \det \mt{A} = - \det \mt{A} $, hence $ \det \mt{A} = 0 $.
      \item Linearity follows from the fact that each term of the determinant contains one and only one element of each row and each column.
      \item Follows from the previous property, recalling that $ \lambda \mt{A} $ means multiplying each row (or column) of $ \mt{A} $ by $ \lambda $, and each term of the determinant has $ n $ elements of $ \mt{A} $.
      \item Trivial by direct computation.
    \end{enumerate}
    \ensp
  \end{proof}
\end{proofbox}

In particular, property (d) shows that the determinant is a $ \K $-multilinear applications, as it is linear with respect to each row (or column) of $ \mt{A} $, while property (b) is easily generalized to:
\begin{equation}
  \det(\ve{a}_{\sigma(1)} , \dots , \ve{a}_{\sigma(n)}) = \sgn(\sigma) \det \mt{A}
  \label{eq:determinant-permutation}
\end{equation}
where $ \ve{a}_1 , \dots , \ve{a}_n $ can denote either the rows ($ \in \K^{1 \times n} $) or the columns ($ \in \K^{n \times 1} $) of $ \mt{A} $.
Furthermore, we can prove a powerful theorem for computing determinants.

\begin{theorem}{Binet theorem}{}
  Given $ \mt{A} , \mt{B} \in \K^{n \times n} $, then:
  \begin{equation}
    \det(\mt{A} \mt{B}) = \det(\mt{A}) \det(\mt{B})
    \label{eq:binet-theorem}
  \end{equation}
\end{theorem}

\begin{proofbox}
  \begin{proof}
    Denote the rows of $ \mt{A} $ and $ \mt{B} $ as $ \{\ve{a}_1 , \dots , \ve{a}_n\} , \{\ve{b}_1 , \dots , \ve{b}_n\} \in \K^{n \times 1} $ respectively, and set the Euclidean base of $ \K^{n \times 1} $ as $ \ve{e}_j = (0 , \dots , 1 , \dots , 0) $, so that the rows of $ \mt{A} \mt{B} $ are:
    \begin{equation*}
      \ve{r}_i = \ve{a}_i \mt{B} = \sum_{j = 1}^n a_{ij} \ve{e}_j \mt{B} = \sum_{j = 1}^{n} a_{ij} \ve{b}_j
    \end{equation*}
    Then, by the multilinearity of the determinant:
    \begin{equation*}
      \det(\mt{A} \mt{B})
      = \det \left( \sum_{j_1 = 1}^n a_{1 j_1} \ve{b}_{j_1} , \dots , \sum_{j_n = 1}^{n} a_{n j_n} \ve{b}_{j_n} \right)
      = \sum_{j_1 = 1}^n \dots \sum_{j_n = 1}^n a_{1 j_1} \dots a_{n j_n} \det(\ve{b}_{j_1} , \dots , \ve{b}_{j_n})
    \end{equation*}
    By \lref{lemma:determinant-basic}, if $ j_i = j_k $ for some $ i \neq k $, then the determinant vanishes, so the summation is restricted to $ (j_1 , \dots , j_n) = (\sigma(1) , \dots , \sigma(n) $, with $ \sigma \in \sy{n} $, i.e.:
    \begin{equation*}
      \det(\mt{A} \mt{B}) = \sum_{\sigma \in \sy{n}} a_{1 \sigma(1)} \dots a_{n \sigma(n)} \det(\ve{b}_{\sigma(1)} , \dots , \ve{b}_{\sigma(n)}) = \sum_{\sigma \in \sy{n}} \sgn(\sigma) a_{1 \sigma(1)} \dots a_{n \sigma(n)} \det \mt{B}
    \end{equation*}
    where \eref{eq:determinant-permutation} was used. By \dref{def:determinant}, the proof is complete.
  \end{proof}
\end{proofbox}

The determinant can also be used to establish the linear (in)dependence of a set of vectors.

\begin{proposition}{}{determinant-ld}
  Let $ \mt{A} \in \K^{n \times n} $ and $ \{\ve{a}_1 , \dots , \ve{a}_n\} \in \K^{n \times 1} $ be its columns. Then $ \{\ve{a}_1 , \dots , \ve{a}_n\} $ is LD if and only if $ \det \mt{A} = 0 $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ WLOG $ \exists \lambda_2 , \dots , \lambda_n \in \K : \ve{a}_1 = \lambda_2 \ve{a}_2 + \dots + \lambda_n \ve{a}_n $, so, by the multilinearity of the determinant:
    \begin{equation*}
      \begin{split}
        \det \mt{A}
        & = \det(\lambda_2 \ve{a}_2 + \dots + \lambda_n \ve{a}_n , \ve{a}_2 , \dots , \ve{a}_n) \\
        & = \lambda_2 \det(\ve{a}_2 , \ve{a}_2 , \dots , \ve{a}_n) + \dots + \lambda_n \det(\ve{a}_n , \ve{a}_2 , \dots , \ve{a}_n)
        = \sum_{i = 1}^n \lambda_i \det(\ve{a}_i , \dots , \ve{a}_i , \dots) = 0
      \end{split}
    \end{equation*}

    $ (\Leftarrow) $ Suppose $ \{\ve{a}_1 , \dots , \ve{a}_n\} $ LI, i.e. they form a basis of $ \K^{n \times 1} $. Let $ \mt{B} \equiv [\beta_{ij}] \in \GL{n,\K} $ be the matrix representing the change of basis to the Euclidean basis, i.e.:
    \begin{equation*}
      \ve{e}_i = \beta_{1 i} \ve{a}_1 + \dots + \beta_{n i} \ve{a}_n
    \end{equation*}
    and consider $ \mt{C} \equiv \mt{A} \mt{B} \in \K^{n \times n} $, whose columns are:
    \begin{equation*}
      \ve{c}_i = \sum_{j = 1}^n \ve{a}_j \beta_{ji} = \beta_{1i} \ve{a}_1 + \dots + \beta_{n i} \ve{a}_n = \ve{e}_i
    \end{equation*}
    Hence, $ \mt{C} = \mt{I}_n $, but by Binet's theorem $ 1 = \det \mt{C} = \det \mt{A} \det \mt{B} = 0 \cdot \det \mt{B} = 0 \absurd $
  \end{proof}
\end{proofbox}

Since the determinant is invariant under transposition, this proposition holds considering rows too.
We can now prove an alternative way to compute determinants.

\begin{definition}{Submatrices and minors}{}
  Let $ \mt{M} \in \K^{m \times n} $. Then any matrix obtained by eliminating any number of rows and/or columns from $ \mt{M} $ is a \bcdef{submatrix} of $ \mt{M} $, and the determinant of any square submatrix is a \bcdef{minor} of $ \mt{M} $.
\end{definition}

Given a square matrix $ \ve{A} = [a_{ij}] \in \K^{n \times n} $, then we can associate to each element $ a_{ij} $ the square submatrix $ \mt{M}_{ij} \in \K^{(n-1) \times (n-1)} $ obtained eliminating the $ i^\text{th} $ row and the $ j^\text{th} $ columns from $ \mt{A} $: the quantity $ \tilde{a}_{ij} \equiv (-1)^{i + j} \det \mt{M}_{ij} $ is denoted as the \bctxt{cofactor} of $ a_{ij} $, and we define the cofactor matrix $ \cof{\mt{A}} \equiv [\tilde{a}_{ij}] \in \K^{n \times n} $.

\begin{example}{$ 2 \times 2 $ matrices}{}
  Consider $ \mt{A} \in \K^{2 \times 2} $. The cofactors then are:
  \begin{align*}
    \tilde{a}_{11} &= (-1)^{1+1} \det [a_{22}] = a_{22} & \tilde{a}_{12} &= (-1)^{1+2} \det [a_{21}] = - a_{21} \\
    \tilde{a}_{21} &= (-1)^{2+1} \det [a_{12}] = - a_{12} & \tilde{a}_22 &= (-1)^{2+2} \det [a_{11}] = a_{11}
  \end{align*}
  So, in general:
  \begin{equation*}
    \mt{A} =
    \begin{bmatrix}
      a & b \\ c & d
    \end{bmatrix}
    \quad \implies \quad
    \cof{\mt{A}} =
    \begin{bmatrix}
      d & -c \\ -b & a
    \end{bmatrix}
  \end{equation*}
\end{example}

Cofactors allow us to compute determinants without explicitly dealing with permutations.

\begin{theorem}{Laplace's Theorem}{}
  Let $ \mt{A} = [a_{ij}] \in \K^{n \times n} $. Then:
  \begin{equation}
    \sum_{k = 1}^n a_{ik} \tilde{a}_{jk} = \sum_{k = 1}^n a_{ki} \tilde{a}_{kj} = \delta_{ij} \det \mt{A}
    \quad \forall i,j \in \{1, \dots, n\}
    \label{eq:laplace-expansion}
  \end{equation}
\end{theorem}

\begin{proofbox}
  \begin{proof}
    The proofs of both equalities are analogous, so WLOG we prove the second one.

    $ (i = j) $ Consider $ k,i \in \{1, \dots, n\} $ fixed and denote as $ \mt{M}_{ki} = [m_{p,q}]_{p,q = 1, \dots, n-1} $ the submatrix obtained by eliminating the $ k^\text{th} $ row and the $ i^\text{th} $ column. Then, each term in the expansion of $ \det \mt{A} $ which contains $ a_{ki} $ can be rewritten as:
    \begin{equation*}
      \sgn(\sigma) a_{1 \sigma(1)} \dots a_{ki} \dots a_{n \sigma(n)} = \sgn(\tau) a_{ki} m_{1 , \tau(1)} \dots m_{n-1 , \tau(n-1)}
    \end{equation*}
    WTS that $ \{\sigma \in \sy{n} : \sigma(k) = i\} \leftrightarrow \sy{n-1} $ is a bijection. Firs, the map $ \sigma \mapsto \tau $ is:
    \begin{equation*}
      \tau =
      \begin{pmatrix}
        1 & \dots & k-1 & k & \dots & n-1 \\
        \gamma_i \circ \sigma(1) & \dots & \gamma_i \circ \sigma(k-1) & \gamma_i \circ \sigma(k+1) & \dots & \gamma_i \circ \sigma(n-1)
      \end{pmatrix}
    \end{equation*}
    where $ \gamma_i \equiv (n , n-1 , \dots , i+1 , i) $ is a cycle which decrements all indices larger than $ i $. Then, to construct the map $ \tau \mapsto \sigma $, consider $ \tau' \in \sy{n} : \tau'(p) = \tau(p) \,\,\forall p \in \{1, \dots, n-1\} \land \tau'(n) = n $, so:
    \begin{equation*}
      \tau' =
      \begin{pmatrix}
        1 & \dots & k-1 & k & \dots & n-1 & n \\
        \gamma_i \circ \sigma(1) & \dots & \gamma_i \circ \sigma(k-1) & \gamma_i \circ \sigma(k+1) & \dots & \gamma_i \circ \sigma(n-1) & n
      \end{pmatrix}
    \end{equation*}
    Now, consider the following compositions:
    \begin{equation*}
      \tau' \circ \gamma_k =
      \begin{pmatrix}
        1 & \dots & k-1 & k & k+1 & \dots & n-1 & n \\
        \gamma_i \circ \sigma(1) & \dots & \gamma_i \circ \sigma(k-1) & n & \gamma_i \circ \sigma(k+1) & \dots & \gamma_i \circ \sigma(n-1) & \gamma_i \circ \sigma(n)
      \end{pmatrix}
    \end{equation*}
    \begin{equation*}
      \gamma_i \circ \sigma =
      \begin{pmatrix}
        1 & \dots & k-1 & k & k+1 & \dots & n-1 & n \\
        \gamma_i \circ \sigma(1) & \dots & \gamma_i \circ \sigma(k-1) & n & \gamma_i \circ \sigma(k+1) & \dots & \gamma_i \circ \sigma(n-1) & \gamma_i \circ \sigma(n)
      \end{pmatrix}
    \end{equation*}
    Therefore, these are the same permutation, i.e. $ \sigma = \gamma_i^{-1} \circ \tau' \circ \gamma_k $, proving the bijection $ \sigma \leftrightarrow \tau $. As the cycle $ \gamma_k $ can be written as $ n - k $ transpositions, $ \sgn{\gamma_k} = (-1)^{n-k} $, hence:
    \begin{equation*}
      \sgn{\sigma} = (-1)^{2n - k - i} \sgn{\tau'} = (-1)^{k+i} \sgn{\tau}
    \end{equation*}
    Putting everything together:
    \begin{equation*}
      \sigma(\sigma) a_{1 \sigma(1)} \dots a_{ki} \dots a_{n \sigma(n)} = a_{ki} (-1)^{k + i} \sgn(\tau) m_{1 , \tau(1)} \dots m_{n-1 , \tau(n-1)}
    \end{equation*}
    Then, the expansion of $ \det \mt{A} $ can be rewritten as:
    \begin{equation*}
      \begin{split}
        \det \mt{A}
        & = \sum_{\sigma \in \sy{n}} \sgn(\sigma) a_{1 \sigma(1)} \dots a_{n \sigma(n)} = \sum_{k = 1}^n \sum_{\sigma \in \sy{n} :\, \sigma(k) = i} \sgn(\sigma) a_{1 \sigma(1)} \dots a_{ki} \dots a_{n \sigma(n)} \\
        & = \sum_{k = 1}^n a_{ki} (-1)^{k+i} \sum_{\tau \in \sy{n-1}} \sgn(\tau) m_{1 , \tau(1)} \dots m_{n-1 , \tau(n-1)} \eqdef \sum_{k = 1}^n a_{ki} (-1)^{k+i} \det \mt{M}_{ki}
      \end{split}
    \end{equation*}
    which is valid $ \forall i \in \{1 , \dots , n\} $. Using the definition of cofactor $ \tilde{a}_{ki} \equiv (-1)^{k+i} \det \mt{M}_{ki} $ concludes the proof.

    $ (i \neq j) $ Consider $ i \neq j \in \{1 , \dots , n\} $ and define a matrix $ \mt{B} $ by replacing the $ j^\text{th} $ column of $ \mt{A} $ with the $ i^\text{th} $ one: then $ \det \mt{B} = 0 $. Applying the just-proved expansion to $ \mt{B} $ yields:
    \begin{equation*}
      0 = \det \mt{B} = \sum_{k = 1}^n b_{kj} \tilde{b}_{kj}
    \end{equation*}
    Now, note that $ b_{kj} = a_{ki} $ by definition, while $ \tilde{b}_{kj} = \tilde{a}_{kj} $, as $ \mt{A} $ and $ \mt{B} $ only differ by the $ j^\text{th} $ column, which does not affect the definition of the cofactors of the $ j^\text{th} $ column's elements. Hence, the proof is complete.
  \end{proof}
\end{proofbox}

Moreover, cofactors also allow us to compute inverse matrices in a straightforward way.

\begin{proposition}{Inverse matrix from cofactors}{}
  Let $ \mt{A} \in \K^{n \times n} $. Then, $ \mt{A} $ is invertible if and only if $ \det \mt{A} \neq 0 $, and:
  \begin{equation}
    \mt{A}^{-1} = \frac{1}{\det \mt{A}} \left( \cof{\mt{A}} \right)\tsp
    \label{eq:inverse-matrix-cofactors}
  \end{equation}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ Invertibility means that $ \exists \mt{A}^{-1} \in \K^{n \times n} : \mt{A}^{-1} \mt{A} = \mt{A} \mt{A}^{-1} = \mt{I}_n $, so $ \det(\mt{A}^{-1} \mt{A}) = 1 $, but $ \det(\mt{A}^{-1} \mt{A}) = \det(\mt{A}^{-1}) \det(\mt{A}) $ by Binet's theorem, hence $ \det \mt{A} \neq 0 $.

    $ (\Leftarrow) $ To prove this implication, consider $ \mt{A} = [a_{ij}]_{i,j = 1, \dots, n} $. Then, we prove the following lemma.

    \begin{lemma}[before upper = {\tcbtitle}]{}{}
      \begin{equation*}
        \mt{A} \left( \cof{\mt{A}} \right)\tsp = \left( \cof{\mt{A}} \right)\tsp \mt{A} = \det(\mt{A}) \mt{I}_n
      \end{equation*}
    \end{lemma}

    \begin{proofbox}
      \begin{proof}
        The general elements of $ \mt{A} \left( \cof{\mt{A}} \right)\tsp $ and $ \left( \cof{\mt{A}} \right)\tsp \mt{A} $ are:
        \begin{equation*}
          \left[ \mt{A} \left( \cof{\mt{A}} \right)\tsp \right]_{ij} = \sum_{k = 1}^n a_{ik} \tilde{a}_{jk} = \delta_{ij} \det \mt{A}
          \qquad \qquad
          \left[ \left( \cof{\mt{A}} \right)\tsp \mt{A} \right]_{ij} = \sum_{k = 1}^n a_{kj} \tilde{a}_{ki} = \delta_{ij} \det \mt{A}
        \end{equation*}
        where \eref{eq:laplace-expansion} was used. This completes the proof.
      \end{proof}
    \end{proofbox}

    Using this lemma, if $ \det \mt{A} \neq 0 $, then it is clear that $ \mt{A}^{-1} = \left( \det \mt{A} \right)^{-1} \left( \cof{\mt{A}} \right)\tsp $.
  \end{proof}
\end{proofbox}

\begin{propcorollary}{Cramer's Theorem}{}
  Consider a linear system $ \mt{A} \ve{x} = \ve{b} $, with $ \mt{A} \in \K^{n \times n} $ and $ \ve{b} \in \K^{n \times 1} $. Then:
  \begin{equation*}
    \det \mt{A} \neq 0
    \quad \implies \quad
    \exists ! \ve{x} \in \K^{n \times 1} \text{ solution} : x_i = \frac{1}{\det \mt{A}} \det \left( \ve{a}_1 , \dots , \ve{a}_{i-1} , \ve{b} , \ve{a}_{i+1} , \dots , \ve{a}_n \right)
  \end{equation*}
  where $ \ve{a}_1 , \dots , \ve{a}_n \in \K^{n \times 1} $ are the columns of $ \mt{A} $.
\end{propcorollary}

\begin{proofbox}
  \begin{proof}
    $ \det \mt{A} \neq 0 \iff \exists \mt{A}^{-1} \in \K^{n \times n} $ by the previous proposition. Then:
    \begin{equation*}
      \ve{x} = \mt{A}^{-1} \ve{b} = \frac{1}{\det \mt{A}} \left( \cof{\mt{A}} \right)\tsp \ve{b}
    \end{equation*}
    Its elements are:
    \begin{equation*}
      x_i = \frac{1}{\det \mt{A}} \sum_{k = 1}^n \tilde{a}_{ij} b_j
    \end{equation*}
    which, by \eref{eq:laplace-expansion}, is the Laplace expansion of the matrix obtained by substituting the $ i^\text{th} $ column of $ \mt{A} $ with $ \ve{b} $, as the cofactors relative to this column are not changed by this substitution.
  \end{proof}
\end{proofbox}

These results clearly show the importance of cofactors and determinants in Linear Algebra.

\subsubsection{Rank}
\label{sssec:rank}

\begin{definition}{Rank}{rank}
  Given $ \mt{A} \in \K^{m \times n} $ with rows $ \ve{r}_1 , \dots , \ve{r}_m \in \K^{1 \times n} $ and columns $ \ve{c}_1 , \dots , \ve{c}_n \in \K^{m \times 1} $, the \bcdef{rank by rows} of $ \mt{A} $ is defined as $ \rk_\text{r}(\mt{A}) \defeq \dk \braket{\ve{r}_1 , \dots , \ve{r}_m} $, and the \bcdef{rank by columns} as $ \rk_\text{c}(\mt{A}) \defeq \dk \braket{\ve{c}_1 , \dots , \ve{c}_n} $.
\end{definition}

The rank by rows (or columns) is just the number of LI rows (or columns). We can show that the two ranks are the same.

\begin{proposition}{}{}
  Given $ \mt{A} \in \K^{m \times n} $, then $ \rk_\text{r}(\mt{A}) = \rk_\text{c}(\mt{A}) $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Set $ r \equiv \rk_\text{r}(\mt{A}) $ and $ c \equiv \rk_\text{c}(\mt{A}) $. If $ r = 0 $, then $ \mt{A} \equiv \mt{0}_{m \times n} $ and $ c = 0 $ too, so consider $ r > 0 $. A LD relation between $ \ve{c}_1 , \dots , \ve{c}_n $ can be written as:
    \begin{equation*}
      x_1 \ve{c}_1 + \dots + x_n \ve{c}_n = \ve{0}
      \quad \implies \quad
      \exists \ve{x} \equiv
      \begin{pmatrix}
        x_1 \\ \vdots \\ x_n
      \end{pmatrix}
      \neq \ve{0}
      \,\,:\,\,
      \mt{A} \ve{x} = \ve{0}
    \end{equation*}
    Now, consider the associated linear application $ L_\mt{A} \in \hmk(\K^n , \K^m) $: then $ \dk \ran{L_\mt{A}} = n - \dk \ker{L_\mt{A}} $ by \eref{eq:rank-nullity}. But $ \dk \ran{L_\mt{A}} $ is the number of LI columns of $ \mt{A} $ (as the columns of $ \mt{A} $ are the images of the Euclidean basis of $ \K^n $), so $ \dk \ran{L_\mt{A}} = c $, while $ \dk \ker{L_\mt{A}} = \dk \{\ve{x} \in \K^{n \times 1} : \mt{A} \ve{x} = \ve{0}\} $ by the above equation.

    WLOG let $ \ve{r}_1 , \dots , \ve{r}_r $ the $ r $ LI rows of $ \mt{A} $, so $ \ve{r}_{r+1} , \dots , \ve{r}_m $ are linear combinations of $ \{\ve{r}_1 , \dots , \ve{r}_r\} $. Then, the linear system reduces to $ \mt{A}' \ve{x} = \ve{0} $, where $ \mt{A}' \in \K^{r \times n} $ is only composed of $ \ve{r}_1 , \dots , \ve{r}_r $, hence $ \rk_\text{c}(\mt{A}') = \rk_\text{c}(\mt{A}) = c $ since they represent equivalent systems. But the columns of $ \mt{A}' $ are vectors in $ \K^{r \times 1} $, thus $ c \leq r $.

    The same reasoning can be applied to $ \mt{A}\tsp $, finding $ r \leq c $, therefore $ r = c $.
  \end{proof}
\end{proofbox}

We then set the \bctxt{rank} of $ \mt{A} $ to be $ \rk(\mt{A}) \equiv \rk_\text{r}(\mt{A}) = \rk_\text{c}(\mt{A}) $, and also $ \rk(\mt{A}) = \mathrm{car}(\mt{A}) $, that is the number of the so-called ``pivots" of $ \mt{A} $.
We can now prove some trivial properties of the rank.

\begin{lemma}[before upper = {\tcbtitle}]{Basic properties of rank}{}
  \begin{enumerate}[label = {\alph*.}]
    \item If $ \mt{A} \in \K^{m \times n} $ and $ \mt{B} $ is a submatrix of $ \mt{A} $, then $ \rk \mt{B} \leq \rk \mt{A} $;
    \item If $ \mt{A} \in \K^{n \times n} $, then $ \mt{A} $ is invertible if and only if $ \rk \mt{A} = n $.
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item Let $ \mt{A} = [a_{ij}] \in \K^{m \times n} $ and $ \mt{B} $ be formed by rows $ i_1 , \dots , i_p $ and columns $ j_1 , \dots , j_q $ of $ \mt{A} $. Moreover, define $ \mt{C} $ as the submatrix of $ \mt{A} $ formed by the same rows of $ \mt{B} $ and all the columns of $ \mt{A} $: then, obviously $ \rk_\text{c}(\mt{B}) \leq \rk_\text{c}(\mt{C}) $ and $ \rk_\text{r}(\mt{C}) \leq \rk_\text{r}(\mt{A}) $, hence $ \rk \mt{B} \leq \rk \mt{A} $.
      \item $ \mt{A} $ is invertible if and only if $ \det \mt{A} \neq 0 $, which, by \pref{prop:determinant-ld}, is equivalent to all the columns of $ \mt{A} $ being LI, i.e. $ \rk \mt{A} = n $.
    \end{enumerate}
    \ensp
  \end{proof}
\end{proofbox}

With the notion of rank defined, we can prove the Rouch√©--Capelli theorem.

\begin{theorem}{Rouch√©--Capelli Theorem}{rouche-capelli}
  Let $ \mt{A} \in \K^{m \times n} $ and $ \ve{b} \in \K^{m \times 1} $. Then, the lineas system $ \mt{A} \ve{x} = \ve{b} $ has solutions if and only if $ \rk \mt{A} = \rk \left[ \mt{A} | \ve{b} \right] $ and, in this case, it is of kind $ \infty^{n - r} $, with $ r \equiv \rk \mt{A} $.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    Consider the associated linear application $ L_\mt{A} \in \hmk(\K^n , \K^m) $: then, the system has solutions if and only if $ \ve{b} \in \ran{L_\mt{A}} = \braket{\ve{c}_1 , \dots , \ve{c}_n} $, where $ \ve{c}_1 , \dots , \ve{c}_n \in \K^{m \times 1} $ are the columns of $ \mt{A} $. But $ \ve{b} \in \braket{\ve{c}_1 , \dots , \ve{c}_n} \iff \braket{\ve{c}_1 , \dots , \ve{c}_n} = \braket{\ve{c}_1 , \dots , \ve{c}_n , \ve{b}} $, which is equivalent to $ \dk \braket{\ve{c}_1 , \dots , \ve{c}_n} = \dk \braket{\ve{c}_1 , \dots , \ve{c}_n , \ve{b}} $, i.e. $ \rk \mt{A} = \rk \left[ \mt{A} | \ve{b} \right] $.

    Now, assume the system has solutions and $ \rk \mt{A} = \rk \left[ \mt{A} | \ve{b} \right] \equiv r $. WLOG, let $ \ve{r}_1 , \dots , \ve{r}_r $ be the $ r $ LI rows of $ \mt{A} $, so that the equations from the $ (r+1)^\text{th} $ to the $ m^\text{th} $ can be eliminated from the linear system. Then:
    \begin{equation*}
      \mt{A} \ve{x} = \ve{b}
      \quad \iff \quad
      \begin{cases}
        a_{11} x_1 + \dots + a_{1 r} x_r + a_{1 (r+1)} x_{r+1} + \dots + a_{1 n} x_n = b_1 \\
        \qquad \qquad \vdots \\
        a_{r 1} x_1 + \dots + a_{r r} x_r + a_{r (r+1)} x_{r+1} + \dots + a_{r n} x_n = b_r \\
      \end{cases}
    \end{equation*}
    $ x_{r+1} , \dots , x_n $ can be WLOG interpreted as free parameters, which can then be absorbed into the constant terms: in this way, the system reduces to $ \mt{A}' \ve{x}' = \ve{b}' $, with $ \mt{A} \in \K^{r \times r} $ and $ \ve{x}' , \ve{b} \in \K^{r \times 1} $. By hypothesis $ \rk \mt{A} = r $, hence it is invertible, and the system has a unique solution $ \ve{x}' = \mt{A}^{-1} \ve{b} $ which depends on $ n - r $ parameters, i.e. of kind $ \infty^{n - r} $.
  \end{proof}
\end{proofbox}

Finally, rank can be used to prove some properties of linear applications too.

\begin{proposition}{}{application-rank}
  Given two finite-dimensional $ \K $-vector spaces $ V , W $ and an application $ f \in \hmk(V,W) $ with representative matrix $ \mt{M}_f \in \K^{m \times n} $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item $ \dk \ran{f} = \rk \mt{M}_f $
    \item $ \dk \ker{f} = n - \rk \mt{M}_f $
    \item If $ n = m $ and $ \rk \mt{M}_f = n $, then $ f $ is an isomorphism.
  \end{enumerate}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    As $ \dk \ran{f} = \dk \braket{f(\ve{e}_1) , \dots , f(\ve{e}_n)} $, with $ \{\ve{e}_1 , \dots , \ve{e}_n\} $ the canonical basis of $ V $, it is clear that $ \dk \ran{f} = \rk \mt{M}_f $ by \dref{def:rank}. Then, $ \dk \ker{f} = n - \rk \mt{M}_f $ by the rank--nullity theorem (\tref{th:rank-nullity}).

    Now, consider $ n = m $, i.e. $ \dk V = \dk W $. If $ \rk \mt{M}_f = n $, then $ \rk \ran{f} = \dk W $ and $ \dk \ker{f} = 0 $, so $ f $ is both a surjection and an injection, i.e. a bijection.
  \end{proof}
\end{proofbox}

\subsection{Eigenvalues and eigenvectors}

Consider two $ \K $-vector spaces $ V,W $ of dimensions $ \dk V = n , \dk W = m $ and an application $ f \in \hmk(V,W) $. Then, by \pref{prop:application-rank}, is possible to chose two bases $ \bas \ssq V , \mathcal{C} \ssq W $ such that:
\begin{equation*}
  \mt{M}_\mathcal{C}^\bas(f) =
  \begin{bmatrix}
    \mt{I}_k & \mt{0}_{n-k} \\
    \mt{0}_{m-k} & \mt{0}_{(m-k) \times (n-k)}
  \end{bmatrix}
\end{equation*}
with $ k \equiv \dk \ran{f} $. To show that this is possible, consider $ k < n $ (i.e. $ \ker{f} \neq \{\ve{0}\} $), so that, by \tref{th:rank-nullity}, $ \dk \ker{f} = n - k $, and let $ \{\ve{v}_{k+1} , \dots , \ve{v}_n\} \ssq V $ be a basis of $ \ker{f} $ and WLOG $ \bas = \{\ve{v}_1 , \dots , \ve{v}_k , \ve{v}_{k+1} , \dots , \ve{v}_n\} $ its extension to a basis of $ V $: then, $ \{f(\ve{v}_1) , \dots , f(\ve{v}_k)\} \ssq W $ is a basis of $ \ran{f} $, and WLOG $ \mathcal{C} = \{f(\ve{v}_1) , \dots , f(\ve{v}_k) , \ve{w}_{k+1} , \dots , \ve{w}_m\} $ its extension to a basis of $ W $. It is trivial to see that $ \mt{M}_\mathcal{C}^\bas(f) $ has the desired form, for the so-defined bases.

A matrix of this form resembles a diagonal matrix, and it is in fact diagonal if $ n = m $. A particular such case is $ W = V $, i.e. that of endomorphisms that can be diagonalized, which we now analyze. Note that our discussion in this section is limited to finite-dimensional vector spaces, for which the concepts used are well-defined.

\begin{definition}{Diagonalizable endomorphism}{}
  Let $ V $ be a $ \K $-vector space. Then, $ f \in \End{V} $ is \bcdef{diagonalizable} if $ \exists \bas \ssq V $ basis of $ V $, called \bcdef{diagonalizing basis}, such that:
  \begin{equation*}
    \mt{M}_\bas^\bas(f) =
    \begin{bmatrix}
      \lambda_1 & 0 & \dots & 0 \\
      0 & \lambda_2 & \dots & 0 \\
      \vdots & \vdots & \ddots & \vdots \\
      0 & 0 & \dots & \lambda_n
    \end{bmatrix}
  \end{equation*}
  for some $ \lambda_1 , \dots , \lambda_n \in \K $.
\end{definition}

\begin{lemma}{Diagonalizable matrix}{}
  Given $ \mt{A} \in \K^{n \times n} $, then $ \mt{A} $ is diagonalizable if and only if it is similar to a diagonal matrix.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ $ \mt{A} \in \K^{n \times n} $ diagonalizable is equivalent to $ L_\mt{A} \in \End{\K^n} $ diagonalizable, which means that $ \exists \bas \ssq \K^n : \mt{M}_\bas^\bas(L_\mt{A}) $ is diagonal. Denoting the canonical basis of $ \K^n $ as $ \mathcal{E} $, then $ \mt{A} \equiv \mt{M}_\mathcal{E}^\mathcal{E}(L_\mt{A}) $ and $ \mt{M}_\bas^\bas(L_\mt{A}) = [\mt{N}_\mathcal{E}^\bas]^{-1} \mt{M}_\mathcal{E}^\mathcal{E}(L_\mt{A}) \mt{N}_\mathcal{E}^\bas $, which shows that $ \mt{A} $ is similar to a diagonal matrix.

    $ (\Leftarrow) $ Let $ \mt{S} \in \GL{n,\K} : \mt{S}^{-1} \mt{A} \mt{S} = \mt{D} $, with $ \mt{D} \in \K^{n \times n} $ diagonal. Consider $ \ve{v}_1 , \dots , \ve{v}_n \in \K^{n \times 1} $ the columns of $ \mt{S} $: as $ \det \mt{S} \neq 0 $, they form a basis $ \bas = \{\ve{v}_1 , \dots , \ve{v}_n\} \ssq \K^n $ since they are LI, and $ \mt{S} = \mt{N}_\mathcal{E}^\bas $ (with $ \mathcal{E} $ canonical basis of $ \K^n $). Then $ \mt{M}_\bas^\bas(L_\mt{A}) = [\mt{N}_\mathcal{E}^\bas]^{-1} \mt{M}_\mathcal{E}^\mathcal{E}(L_\mt{A}) \mt{N}_\mathcal{E}^\bas = \mt{S}^{-1} \mt{A} \mt{S} = \mt{D} $, i.e. $ L_\mt{A} $ is diagonalizable with diagonalizing basis $ \bas $.
  \end{proof}
\end{proofbox}

It is possible to characterize the diagonalizing basis in order to get an algorithm to establish whether an endomorphism is diagonalizable.

\begin{definition}{Eigenvectors and eigenvalues}{}
  Given a $ \K $-vector space $ V $ and $ f \in \End{V} $, then $ \ve{v} \in V : \ve{v} \neq \ve{0} $ is an \bcdef{eigenvector} of $ f $ if $ \exists \lambda \in \K : f(\ve{v}) = \lambda \ve{v} $, called \bcdef{eigenvalue} relative to $ \ve{v} $.
\end{definition}

The null vector $ \ve{0} $ is excluded from the formal definition of eigevector since it can be regarded as an eigenvector with $ \K $-finitely-many eigenvalues.

\begin{proposition}{Diagonalizing basis}{diagonalizing-basis}
  Given a $ \K $-vector space $ V $ and $ f \in \End{V} $, then $ f $ is diagonalizable if and only if $ \exists \bas \ssq V $ basis of $ V $ composed of eigenvectors of $ f $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ If $ \mt{M}_\bas^\bas(f) = \diag(\lambda_1 , \dots , \lambda_n) $, then, given $ \bas = \{\ve{v}_1 , \dots , \ve{v}_n\} $, clearly $ f(\ve{v}_i) = 0 \cdot \ve{v}_1 + \dots + \lambda_i \ve{v}_i + \dots + 0 \cdot \ve{v}_n = \lambda_i \ve{v}_i \,\,\forall i \in \{1 , \dots , n\} $.

    $ (\Leftarrow) $ Given $ \bas = \{\ve{v}_1 , \dots , \ve{v}_n\} : f(\ve{v}_i) = \lambda_i \ve{v}_i \,\,\forall i \in \{1 , \dots , n\} $, then by \dref{def:representative-matrix} $ \mt{M}_\bas^\bas(f) = \diag(\lambda_1 , \dots , \lambda_n) $.
  \end{proof}
\end{proofbox}

Moreover, the eigenvalues of $ f \in \End{V} $ determine subspaces of $ V $.

\begin{proposition}{Eigenspaces}{}
  Let $ V $ be a $ \K $-vector space and $ f \in \End{V} $. Then, given an eigenvalue $ \lambda \in \K $ of $ f $, the relative \bcprop{eigenspace} $ V_\lambda(f) \defeq \{\ve{v} \in V : f(\ve{v}) = \lambda \ve{v}\} $ is a subspace of $ V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Trivially $ \{\ve{0}\} \in V_\lambda(\ve{v}) \,\,\forall \lambda \in \K $. Then, given $ \ve{v}_1 , \ve{v}_2 \in V_\lambda(f) $:
    \begin{equation*}
      f(\mu_1 \ve{v}_1 + \mu_2 \ve{v}_2) = \mu_1 f(\ve{v}_1) + \mu_2 f(\ve{v}_2) = \mu_1 \lambda \ve{v}_1 + \mu_2 \lambda \ve{v}_2 = \lambda (\mu_1 \ve{v}_1 + \mu_2 \ve{v}_2)
    \end{equation*}
    which shows that $ V_\lambda(f) \ssq V $ is closed under linear combinations, i.e. a subspace of $ V $.
  \end{proof}
\end{proofbox}

\begin{example}{Euclidean geometry}{}
  Set $ V = \Vect_0(\R^2) $. Then, if $ f $ is a reflection with respect to the line $ r $, then it has two eigenspaces: $ V_1(f) = \{\ve{v} \in V : \ve{v} \parallel r\} $ and $ V_{-1}(f) = \{\ve{v} \in V : \ve{v} \perp r\} $. On the other hand, if $ g $ is a rotation by an angle $ \alpha \in [0, 2\pi) $, then there are three possible cases:
  \begin{itemize}
    \item $ \alpha = 0 $, i.e. $ g = \id_V $ and $ V_1(g) = V $;
    \item $ \alpha = \pi $, i.e. $ g = - \id_V $ and $ V_{-1}(g) = V $;
    \item $ \alpha \in (0,\pi) \cup (\pi,2\pi) $, and it has no eigenvalues.
  \end{itemize}
\end{example}

We can prove that these eigenspaces are ``linearly-independent" from one another.

\begin{theorem}{LI eigenvectors}{eigenvalues-li}
  Consider a $ \K $-vector space $ V $ and $ f \in \End{V} $. If $ \lambda_1 \neq \dots \neq \lambda_k $ are distinct eigenvalues of $ f $, then their relative eigenvectors $ \ve{v}_1 , \dots , \ve{v}_k $ are LI.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    We use induction on $ k $:
    \begin{itemize}
      \item $ k = 1 $ is true as $ \ve{v}_1 \neq \ve{0} $ is LI.
      \item Assume that $ \ve{v}_1 , \dots , \ve{v}_{k-1} $ are LI.
      \item Consider $ a_1 \ve{v}_1 + \dots + a_k \ve{v}_k = \ve{0} $, with $ a_1 , \dots , a_k \in \K $. Then $ f(a_1 \ve{v}_1 + \dots + a_k \ve{v}_k) = \ve{0} $, i.e. $ a_1 \lambda_1 \ve{v}_1 + \dots a_k \lambda_k \ve{v}_k = \ve{0} $, but $ \ve{0} = \lambda_k \ve{0} = a_1 \lambda_k \ve{v}_1 + \dots + a_k \lambda_k \ve{v}_k $, so the linear combination reduces to $ a_1 (\lambda_1 - \lambda_k) \ve{v}_1 + \dots + a_{k-1} (\lambda_{k-1} - \lambda_k) \ve{v}_k = \ve{0} $. The eigenvalues are distinct by hypothesis and the eigenvectors are LI by the inductive step, hence $ a_1 = \dots = a_{k-1} = 0 $, and $ a_k = 0 $ as $ \ve{v}_k \neq \ve{0} $, i.e. $ \ve{v}_1 , \dots , \ve{v}_k $ are LI.
    \end{itemize}
    \ensp
  \end{proof}
\end{proofbox}

By \pref{prop:diagonalizing-basis}, if $ f $ has $ n = \dk V $ distinct eigenvalues, then it is diagonalizable, as they form a diagonalizing basis of $ V $ by \tref{th:eigenvalues-li}.

\subsubsection{Computation of eigenvalues}

In order to systematically compute the eigenvalues of $ f \in \End{V} $, note that the condition for $ \lambda \in \K $ to be an eigenvalue is $ \exists \ve{v} \in V : f(\ve{v}) = \lambda \ve{v} \iff (f - \lambda \id_V)(\ve{v}) = \ve{0} $, hence the relative eigenspace can be written as $ V_\lambda(f) = \ker(f - \lambda \id_V) $.

Translating this in matrix terms, given a basis $ \bas \ssq V $ and $ \mt{A} \equiv \mt{M}_\bas^\bas(f) $, then $ \lambda $ is an eigenvalue of $ f $ if $ V_\lambda(f) \neq \{\ve{0}\} $, which means that $ f - \lambda \id_V $ must not be an injection (by \pref{prop:kernel-injections}): by \pref{prop:application-rank} this is equivalent to $ \rk(\mt{A} - \lambda \mt{I}_n) < n $, i.e. $ \det(\mt{A} - \lambda \mt{I}_n) = 0 $ by \pref{prop:determinant-ld}.

\begin{definition}{Characteristic polynomial}{}
  Given $ \mt{A} \in \K^{n \times n} $, its \bcdef{characteristic polynomial} is $ p_\mt{A}(t) \defeq \det(\mt{A} - t \mt{I}_n) \in \K_n[t] $.
\end{definition}

We see then that the eigenvalues of $ f $ are the roots of $ p_\mt{A}(\lambda) $, with $ \mt{A} \equiv \mt{M}_\bas^\bas(f) $. Note that the characteristic polynomial is basis-independent, and so are the eigenvalues of $ f $.

\begin{lemma}{Basis-independence of eigenvalues}{characteristic-polynomial-basis-independence}
  Given $ V $ a $ \K $-vector space, $ \bas , \mathcal{C} \ssq V $ two bases and $ f \in \End{V} $, setting $ \mt{B} \equiv \mt{M}_\bas^\bas(f) $ and $ \mt{C} \equiv \mt{M}_\mathcal{C}^\mathcal{C}(f) $, then $ p_\mt{B}(\lambda) = p_\mt{C}(\lambda) $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Let $ \mt{N} \equiv \mt{N}_\mathcal{C}^\bas \in \GL{n,\K} $, so that $ \mt{B} = \mt{N}^{-1} \mt{C} \mt{N} $. Then:
    \begin{equation*}
      \det(\mt{B} - \lambda \mt{I}_n) = \det(\mt{N}^{-1} \mt{C} \mt{N} - \lambda \mt{N}^{-1} \mt{I}_n \mt{N}) = \det(\mt{N}^{-1}) \det(\mt{C} - \lambda \mt{I}_n) \det(\mt{N}) = \det(\mt{C} - \lambda \mt{I}_n)
    \end{equation*}
    where we used Binet's theorem. This shows that $ p_\mt{B}(\lambda) = p_\mt{C}(\lambda) $.
  \end{proof}
\end{proofbox}

A particular corollary, obtained setting $ \lambda = 0 $, is that similar matrices have the same determinant.

The basis-independence of the characteristic polynomial allows us to define the notions of determinant $ \det f $ and characteristic polynomial $ p_f(\lambda) $ directly for the endomorphism $ f $, since they are well-defined by \lref{lemma:characteristic-polynomial-basis-independence}. In particular, since $ p_f(\lambda) \in \K_n[\lambda] $ with $ n = \dk V $, if $ V $ is finite-dimensional, then $ f $ as finitely-many eigenvalues.

\begin{example}{Complex and real endomorphisms}{}
  If $ \K = \C $, then $ p_f(\lambda) $ has $ n $ roots by the fundamental theorem of algebra. On the other hand, if $ \K = \R $, then only the real roots of $ p_f(\lambda) $ are eigenvalues of $ f $.
\end{example}

In general, then, the eigevalues $ \lambda $ of $ f $ are found by solving $ p_f(\lambda) = 0 $, and then the eigenspaces $ V_\lambda(f) $ are found by solving the systems $ (\mt{A} - \lambda \mt{I}_n) \ve{x} = \ve{0} $.

\subsubsection{Multiplicity of eigevalues}

From Ruffini's theorem, if $ \alpha \in \K $ is a root of $ p(t) \in \K[t] $, then $ \exists q(t) \in \K[t] : p(t) = (t - \alpha) q(t) $, so in general we define the \bctxt{multiplicity} of $ \alpha$ as $ m_p(\alpha) \defeq \max_{\N_0} \{k \in \N_0 : \exists q(t) \in \K[t] : p(t) = (t-\alpha)^k q(t)\} $, i.e. the exponent of the highest power of $ (t - \alpha) $ that divides $ p(t) $. This concept can be extended to eigenvalues in a two-fold way.

\begin{definition}{Multiplicity}{}
  Given a $ \K $-vector space $ V $ and $ f \in \End{V} $, for an eigenvalue $ \lambda \in \K $ of $ f $ the \bcdef{algebraic multiplicity} is $ \am(\lambda) \defeq m_{p_f}(\lambda) $ and the \bcdef{geometric multiplicity} is $ \gm(\lambda) \defeq \dk V_\lambda(f) $.
\end{definition}

Though different in nature, we can prove an important relation between the two multiplicities.

\begin{lemma}{}{}
  Given a $ \K $-vector space $ V $, $ f \in \End{V} $ and an eigenvalue $ \lambda \in \K $, then $ 1 \le \gm(\lambda) \le \am(\lambda) $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    As $ \lambda $ is an eigenvalue of $ f $, $ V_\lambda(f) \neq \{\ve{0}\} $, hence $ \gm(\lambda) \ge 1 $. Now, let $ s \equiv \gm(\lambda) $ and $ \{\ve{v}_1 , \dots , \ve{v}_s\} \ssq V_\lambda(f) $ a basis of $ V_\lambda(f) $, and extend it to a basis $ \bas = \{\ve{v}_1 , \dots , \ve{v}_s , \ve{w}_{s+1} , \dots , \ve{w}_n\} $ of $ V $. By definition $ f(\ve{v}_i) = \lambda \ve{v}_i \,\,\forall i = 1, \dots, s $, hence:
    \begin{equation*}
      \mt{M}_\bas^\bas(f) =
      \begin{bmatrix}
        \lambda \mt{I}_s & * \\
        \mt{0}_{s \times (n-s)} & *
      \end{bmatrix}
    \end{equation*}
    Clearly, then, $ p_f(t) = (t - \lambda)^s q(t) $, with $ q(t) \in \K_{n-s}[t] $ determined by $ \{\ve{w}_{s+1} , \dots , \ve{w}_n\} $, hence $ \am(\lambda) \ge s $, which is the thesis.
  \end{proof}
\end{proofbox}

The diagonalizability of an endomorphism is linked to the reducibility of its characteristic polynomial and the multiplicity of its eigenvalues.

\begin{theorem}{Diagonalizability and multiplicity}{}
  Let $ V $ be a $ \K $-vector space and $ f \in \End{V} $. Then, $ f $ is diagonalizable if and only if $ p_f(\lambda) $ is fully reducible in $ \K $ and $ \am(\lambda) = \gm(\lambda) \,\,\forall \lambda \in \K $ eigenvalue of $ f $.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ Let $ \bas \ssq V $ be the diagonalizing basis of $ f $: then, given  $ \lambda_1 , \dots , \lambda_r \in \K $ distinct eigenvalues of $ f $, $ \mt{M}_\bas^\bas(f) = \diag(\lambda_1 , \dots , \lambda_1 , \dots , \lambda_r , \dots \lambda_r) $ with $ m_i \equiv \am(\lambda_i) \,\,\forall i \in \{1, \dots, r\} $, and $ p_f(t) = (\lambda_1 - t)^{m_1} \dots (\lambda_r - t)^{m_r} $ with $ m_1 + \dots + m_r = n \equiv \dk V $. This shows that $ p_f(t) $ is fully reducible in $ \K $.

    Now, consider $ V_{\lambda_i}(f) = \ker(f - \lambda_i \id_V) $: by \pref{prop:application-rank} $ \dk V_{\lambda_i}(f) = n - \rk(\mt{M}_\bas^\bas(f) - \lambda_i \mt{I}_n) $, but:
    \begin{equation*}
      \mt{M}_\bas^\bas(f) - \lambda_i \mt{I}_n = \diag(\underbrace{\lambda_1 - \lambda_i , \dots , \lambda_1 - \lambda_i}_{m_1} , \dots , \underbrace{0 , \dots , 0}_{m_i} , \dots , \underbrace{\lambda_r - \lambda_i , \dots , \lambda_r - \lambda_i}_{m_r})
    \end{equation*}
    hence $ \rk(\mt{M}_\bas^\bas(f) - \lambda_i \mt{I}_n) = \sum_{j = 1}^n m_j - m_i = n - m_i $, i.e. $ \gm(\lambda_i) = m_i \equiv \am(\lambda_i) $.

    $ (\Leftarrow) $ Let $ \lambda_1 , \dots , \lambda_r \in \K $ be the distinct eigenvalues of $ f $, with $ m_i \equiv \gm(\lambda_i) \,\,\forall i \in \{1, \dots , r\} $, and be $ \bas_i = \{\ve{v}_{i,1} , \dots , \ve{v}_{i,m_i}\} \ssq V_{\lambda_i}(f) $ a basis of $ V_{\lambda_i}(f) $. By hypothesis, the total number of basis-eigenvectors is $ m_1 + \dots + m_r = \am(\lambda_1) + \dots + \am(\lambda_r) = n $, as $ p_f(t) $ is fully reducible in $ \K $, so consider the following linear combination:
    \begin{equation*}
      \underbrace{\mu_{1,1} \ve{v}_{1,1} + \dots + \mu_{1,m_1} \ve{v}_{1,m_1}}_{\ve{w}_1} + \dots + \underbrace{\mu_{r,1} \ve{v}_{r,1} + \dots + \mu_{r,m_r} \ve{v}_{r,m_r}}_{\ve{w}_r} = \ve{0}
    \end{equation*}
    Since $ \ve{w}_i \in V_{\lambda_i}(f) \,\,\forall i \in \{1, \dots, r\} $, they are LI by \tref{th:eigenvalues-li}, so $ \ve{w}_1 = \dots = \ve{w}_r = \ve{0} $. Then, $ \mu_{i,1} \ve{v}_{i,1} + \dots + \mu_{i,m_i} \ve{v}_{i,m_i} = \ve{0} \,\,\forall i \in \{1, \dots, r\} $, but $ \bas_i $ is a basis of $ V_{\lambda_i}(f) $, thus $ \mu_{i,1} = \dots = \mu_{i,m_i} = 0 \,\,\forall i \in \{1, \dots, r\} $: this means that $ \bas = \bas_1 \cup \dots \cup \bas_r $ is a basis of $ V $, hence $ f $ is diagonalizable with diagonalizing basis $ \bas $.
  \end{proof}
\end{proofbox}

\section{Inner-product spaces}

A particular class of vector spaces is of great interest in mathematics: these are inner-product spaces, in which we can introduce the notion of ``distance".

\subsection{Dual spaces}

In order to define inner-product spaces, we first have to analyze dual spaces, which we will generalize in the next chapter.

\begin{definition}{Dual space}{}
  Given a $ \K $-vector space $ V $, its \bcdef{dual space} is $ V^* \defeq \hmk(V,\K) $, whose elements are \bcdef{linear forms} (or functionals) on $ V $.
\end{definition}

Clearly, $ V^* $ is a $ \K $-vector space too, since $ \hmk(V,\K) $ has a natural structure of $ \K $-vector space due to linearity.

\begin{lemma}{Dual basis}{}
  Given a finite-dimensional $ \K $-vector space $ V $ with a basis $ \bas = \{\ve{b}_1 , \dots , \ve{b}_n\} \ssq V $, then $ \dk V^* = \dk V $ and $ \bas^* \equiv \{b^*_1 , \dots , b^*_n\} : b^*_i(\ve{b}_j) = \delta_{ij} \,\,\forall i,j \in \{1, \dots , n\} $ is a basis of $ V^* $, called \bclemma{dual basis} relative to $ \bas $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Fix the canonical basis $ \mathcal{E} = \{1_\K\} \ssq \K $ of $ \K $: then, the application $ \varphi : V^* \ra \K^{1 \times n} : \varphi(\omega) = \mt{M}_\mathcal{E}^\bas(\omega) $ is an isomorphism, so $ \dk V = n $ by \tref{th:equidimensionality-isomorphism}.

    To prove that $ \bas^* \ssq V^* $ is a basis, WTS $ \bas^* $ is LI. Take $ \ve{v} \in V : \ve{v} = \sum_{i = 1}^n v_i \ve{b}_i $, so that:
    \begin{equation*}
      b_i^*(\ve{v}) = \sum_{j = 1}^n v_j b_i^*(\ve{b}_j) = \sum_{j = 1}^n v_j \delta_{ij} = v_i
      \quad \implies \quad
      \ve{v} = \sum_{i = 1}^n b_i^*(\ve{v}) \ve{b}_i
    \end{equation*}
    Now, consider a linear combination $ \lambda_1 b_1^* + \dots + \lambda_n b_n^* = 0 \in V^* $, and rewrite:
    \begin{equation*}
      0_\K = 0(\ve{b}_i) = \sum_{j = 1}^n \lambda_j b_j^*(\ve{b}_i) = \sum_{j = 1}^n \lambda_j \delta_{ij} = \lambda_i
      \quad \forall i \in \{1, \dots, n\}
    \end{equation*}
    This concludes the proof.
  \end{proof}
\end{proofbox}

Given the isomorphisms $ V \cong \K^n \cong V^* $, it is possible to represent both $ V $ and $ V^* $ on $ \K^n $: in particular, $ V $ is usually represente on $ \K^{n \times 1} $, while $ V^* $ on $ \K^{1 \times n} $, so that $ \omega(\ve{v}) \in \K \,\,\forall \ve{v} \in V , \omega \in V^* $.

\begin{definition}{Transposed homomosphism}{}
  Given two $ \K $-vector spaces $ V,W $ and $ f \in \hmk(V,W) $, its \bcdef{transposed homomorphism} is the application $ f\tsp : W^* \rightarrow V^* $ define as $ W^* \ni \omega \mapsto f\tsp(\omega) \defeq \omega \circ f \in V^* $.
\end{definition}

As composition preserves linearity, $ f\tsp \in \hmk(W^*,V^*) $, making the following diagrams commutative:
\begin{equation*}
  \begin{tikzcd}
    V \ar[r , "f"] \ar[dr , "f\tsp(\omega)", swap] & W \ar[d , "\omega"] \\ & \K
  \end{tikzcd}
  \qquad \iff \qquad
  \begin{tikzcd}
    V \ar[d , "\varphi_V", swap, leftrightarrow] \ar[r , "f"] & W \ar[d , "\varphi_W" , leftrightarrow] \\
    V^* & W^* \ar[l , "f\tsp"]
  \end{tikzcd}
\end{equation*}

\begin{proposition}{}{}
  Consider two finite-dimensional $ \K $-vector spaces $ V,W $ with bases $ \bas \ssq V , \mathcal{C} \ssq W $, and $ f \in \hmk(V,W) $. Then:
  \begin{equation}
    \mt{M}_{\bas^*}^{\mathcal{C}^*}(f\tsp) = [\mt{M}_\mathcal{C}^\bas(f)]\tsp
    \label{eq:transposed-homomorphism-matrix}
  \end{equation}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Set $ n \equiv \dk V $ and $ m \equiv \dk W $. Clearly $ \mt{M}_\mathcal{C}^\bas(f) \in \K^{m \times n} $ and $ \mt{M}_{\bas^*}^{\mathcal{C}^*}(f\tsp) \in \K^{n \times m} $, so, fixing the canonical basis $ \mathcal{E} = \{1_\K\} \ssq \K $ of $ \K $ and given $ \omega \in W^* $, $ \mt{M}_\mathcal{E}^\mathcal{C}(\omega) = \left[ \beta_1 , \dots , \beta_m \right] \in \K^{1 \times m} $, but $ \omega = \omega_1 c_1^* + \dots + \omega_m c_m^* $ on the basis $ \mathcal{C}^* $, thus, given $ \ve{w} \in W $:
    \begin{equation*}
      \begin{split}
        \omega(\ve{w})
        & = \mt{M}_\mathcal{E}^\mathcal{C}(\omega) \ve{w} =
        \begin{bmatrix}
          \beta_1 & \dots & \beta_m
        \end{bmatrix}
        \begin{bmatrix}
          w_1 \\ \vdots \\ w_m
        \end{bmatrix}
        = \sum_{i = 1}^m \beta_i w_i \\
        & = \sum_{i = 1}^m \omega_i c_i^*(\ve{w}) = \sum_{i,j = 1}^m \omega_i w_j c_i^*(\ve{c}_j) = \sum_{i,j = 1}^m \omega_i w_j \delta_{ij} = \sum_{i = 1}^m \omega_i w_i
      \end{split}
    \end{equation*}
    Hence, $ [\mt{M}_\mathcal{E}^\mathcal{C}(\omega)]\tsp $ is precisely the representation of $ \omega \in W^* $ on $ \K^{m \times 1} $ with basis $ \mathcal{C}^* $.

    Now, since $ f\tsp(\omega) = \omega \circ f \in V^* $, by \lref{lemma:representative-matrix-properties} $ \mt{M}_\mathcal{E}^\bas(f\tsp(\omega)) = \mt{M}_\mathcal{E}^\mathcal{C}(\omega) \mt{M}_\mathcal{C}^\bas(f) $, therefore, by the same reasoning, the representation of $ f\tsp(\omega) \in V^* $ on $ \K^{n \times 1} $ with basis $ \bas^* $ is $ [\mt{M}_\mathcal{E}^\bas(f\tsp(\omega))]\tsp = [\mt{M}_\mathcal{C}^\bas(f)]\tsp [\mt{M}_\mathcal{E}^\mathcal{C}(\omega)]\tsp $, but $ f\tsp \in \hmk(W^* , V^*) $ has representative matrix $ \mt{M}_{\bas^*}^{\mathcal{C}^*}(f\tsp) $, thus:
    \begin{equation*}
      f\tsp(\omega) = \mt{M}_{\mathcal{C}^*}^{\bas^*}(f\tsp) \bs{\omega} = \mt{M}_{\mathcal{C}^*}^{\bas^*}(f\tsp) [\mt{M}_\mathcal{E}^\bas(\omega)]\tsp
    \end{equation*}
    Comparing the two expressions for $ f\tsp(\omega) $, the proof is complete.
  \end{proof}
\end{proofbox}

\subsubsection{Bilinear forms}

We can generalize linear forms to multilinear forms, i.e. maps which are linear with respect to each of their arguments. Here, we consider maps with two arguments.

\begin{definition}{Bilinear forms}{}
  Given a $ \K $-vector space $ V $, a \bcdef{bilinear form} is an application $ b : V \times V \ra \K $ which is linear with respect to both its arguments.
\end{definition}

By linearity, it is trivial to see that $ b(\ve{0} , \ve{v}) = b(\ve{v} , \ve{0}) = 0_\K $. Moreover, we can associate to every $ \mt{A} \in \K^{n \times n} $ a bilinear form on $ b_\mt{A} : \K^n \times \K^n \ra \K $ defined as $ b_\mt{A}(\ve{x} , \ve{y}) = \ve{x}\tsp \mt{A} \ve{y} $, i.e., setting $ \mt{A} = [a_{ij}] $, $ b_\mt{A}(\ve{x} , \ve{y}) = \sum_{i,j = 1}^n a_{ij} x_i y_j $.
Analogously, we can in general define the representative matrix of $ b : V \times V \ra \K $ on a basis $ \bas \{\ve{v}_1 , \dots , \ve{v}_n\} \ssq V $ as $ \mt{M}_\bas(b) \equiv [b(\ve{v}_i , \ve{v}_j)]_{i,j = 1, \dots, n} $, which defines the actions of $ b $ on $ V $ as:
\begin{equation*}
  b(\ve{v} , \ve{w}) = \sum_{i,j = 1}^n v_i w_j b(\ve{v}_i , \ve{w}_j) = \ve{v}\tsp \mt{M}_\bas(b) \ve{w}
\end{equation*}
where the abuse of notation $ \ve{v},\ve{w} \in V $ and $ \ve{v},\ve{w} \in \K^n $ is justified by the isomorphism $ V \cong \K^n $.
To see how the representative matrices in different bases are related, we need to introduce another equivalence relation for square matrices.

\begin{definition}{Congruent matrices}{}
  Given $ \mt{A} , \mt{B} \in \K^{n \times n} $, they are \bcdef{congruent} if $ \exists \mt{C} \in \GL{n,\K} : \mt{B} = \mt{C}\tsp \mt{A} \mt{C} $.
\end{definition}

Note that congruent matrices have the same rank. Indeed, $ \rk \mt{A} = \dk \ran{L_\mt{A}} $, but $ \mt{C} \in \GL{n,\K} $ is invertible, hence it defines an automorphism of $ \K^{n \times n} $: then, $ \rk(\mt{C} \mt{A}) = \rk \mt{A} $ by \tref{th:equidimensionality-isomorphism}, and $ \rk(\mt{A} \mt{C}) = \rk \mt{A} $ since $ \rk \mt{A}\tsp = \rk \mt{A} $.

\begin{theorem}{Congruence and bilinear forms}{}
  Given a $ \K $-vector space $ V $, then two matrices represent the same bilinear form $ b : V \times V \ra \K $ if and only if they are congruent.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ Consider $ \bas , \bas' \ssq V $ bases of $ V $, and let $ \mt{C} \equiv \mt{N}_{\bas'}^\bas \in \GL{n,\K} $ be the basis-change matrix, so that $ \ve{v}' = \mt{C} \ve{v} \,\,\forall \ve{v} \in V $ (with an abuse of notation since $ V \cong \K^n $). Then:
    \begin{equation*}
      b(\ve{v} , \ve{w}) = \ve{v}\tsp \mt{M}_\bas(b) \ve{w} = (\mt{C} \ve{v})\tsp \mt{M}_{\bas'}(b) (\mt{C} \ve{w}) = \ve{v}\tsp (\mt{C}\tsp \mt{M}_{\bas'}(b) \mt{C}) \ve{w}
      \,\,\ \implies \,\,\
      \mt{M}_\bas(b) = \mt{C}\tsp \mt{M}_{\bas'}(b) \mt{C}
    \end{equation*}

    $ (\Leftarrow) $ Consider two congruent matrices $ \mt{A} , \mt{B} \in \K^{n \times n} : \exists \mt{C} \in \GL{n,\K} : \mt{B} = \mt{C}\tsp \mt{A} \mt{C} $. Then, $ \mt{C} $ determines a change of basis on $ \K^{n \times n} $, so that, setting $ b_\mt{A}(\ve{v}' , \ve{w}') = {\ve{v}'}\tsp \mt{A} \ve{w}' $:
    \begin{equation*}
      b_\mt{B}(\ve{v} , \ve{w}) = \ve{v}\tsp \mt{B} \ve{w} = \ve{v}\tsp \mt{C}\tsp \mt{A} \mt{C} \mt{w} = (\mt{C} \ve{v})\tsp \mt{A} (\mt{C} \ve{w}) = {\ve{v}'}\tsp \mt{A} \ve{w} = b_\mt{A}(\ve{v}' , \ve{w}')
    \end{equation*}
    Hence, $ \mt{A} $ and $ \mt{B} $ represent the same bilinear form in different bases of $ V $.
  \end{proof}
\end{proofbox}

By this theorem, it is clear that symmetry is a well-defined property for bilinear forms, since it is preserved by congruence.

\begin{lemma}{Symmetric bilinear form}{}
  Let $ V $ be a $ \K $-vector space and $ b : V \times V \ra \K $ a bilinear form on $ \K $. Then $ b $ is \bctxt{symmetric}, i.e. $ b(\ve{v} , \ve{w}) = b(\ve{w} , \ve{v}) \,\,\forall \ve{v},\ve{w} \in V $, if and only if its representative matrix is symmetric.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Set $ \mt{M}_\bas(b) = [b_{ij}] $. Then $ b(\ve{v} , \ve{w}) = b(\ve{w} , \ve{v}) \iff \sum_{i,j = 1}^n v_i w_j b_{ij} = \sum_{i,j = 1}^n v_i w_j b_{ji} \iff b_{ij} = b_{ji} \,\,\forall i,j \in \{1, \dots, n\} $.
  \end{proof}
\end{proofbox}

\subsection{Euclidean vector spaces}

In this subsection, we always consider $ V $ to be a $ \R $-vector space of dimension $ \dim_\R V = n \in \N $.

\begin{definition}{Positive-definite bilinear form}{}
  A symmetric bilinear form $ b : V \times V \ra \R $ is \bcdef{positive-definite} if $ b(\ve{v} , \ve{v}) \ge 0 \,\,\forall \ve{v} \in V $ and $ b(\ve{v} , \ve{v}) = 0 \iff \ve{v} = \ve{0} $.
\end{definition}

We refer to symmetric positive-definite bilinear forms as \bctxt{inner products}, and we set the notation $ b(\ve{v} , \ve{w}) \equiv \braket{\ve{v} , \ve{w}} $.

\begin{example}{Canonical inner product}{}
  Consider $ V = \R^n $. Then, the \bcex{canonical inner product} is $ \braket{\ve{v} , \ve{w}} \equiv b_{\mt{I}_n}(\ve{v} , \ve{w}) = \ve{v}\tsp \ve{w} $.
\end{example}

\begin{definition}{Euclidean vector space}{}
  $ V $ is a \bcdef{Euclidean vector space} (EVS) if it is possible to define an inner product on it, and it is denoted as $ \evs $.
\end{definition}

In an EVS it is possible to define two important notions (with $ \ve{v} , \ve{w} \in V $):
\begin{multicols}{2}
  \begin{itemize}
    \item \bctxt{norm}: $ \norm{\ve{v}} \defeq \sqrt{\braket{\ve{v},\ve{v}}} $
    \item \bctxt{distance}: $ \dist(\ve{v} , \ve{w}) \defeq \norm{\ve{v} - \ve{w}} $
  \end{itemize}
\end{multicols}

For the resto of this subsection, we assume that $ V $ has the structure of EVS.

\begin{lemma}{Cauchy--Schwarz inequality}{}
  Given $ \ve{v} , \ve{w} \in V $, then:
  \begin{equation}
    \abs{\braket{\ve{v} , \ve{w}}}^2 \leq \norm{\ve{v}}^2 \norm{\ve{w}}^2
    \quad \land \quad
    \abs{\braket{\ve{v} , \ve{w}}}^2 = \norm{\ve{v}}^2 \norm{\ve{w}}^2
    \iff
    \ve{v} , \ve{w} \text{ LI}
    \label{eq:cauchy-schwarz-inequality}
  \end{equation}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    The case $ \ve{v} = \ve{0} \lor \ve{w} = \ve{0} $ is trivial, so WLOG $ \ve{v} , \ve{w} \neq \ve{0} $.

    Given $ \ve{v} , \ve{w} \in V $, consider $ p(t) \equiv \braket{t \ve{v} + \ve{w} , t \ve{v} + \ve{w}} \in \R_2[t] $: by definition $ p(t) \ge 0 \,\,\forall t \in \R $, hence, by linearity and symmetry:
    \begin{equation*}
      p(t) = t^2 \norm{\ve{v}}^2 + 2 t \braket{\ve{v} , \ve{w}} + \norm{\ve{w}}^2 \ge 0
      \quad \forall t \in \R
    \end{equation*}
    To be true, this requires that the discriminant of the polynomial is non-positive, i.e.:
    \begin{equation*}
      \Delta_p \equiv 4 \left( \braket{\ve{v} , \ve{w}}^2 - \norm{\ve{v}}^2 \norm{\ve{w}}^2 \right) \leq 0
    \end{equation*}
    which is the thesis. Moreover, assume $ \Delta_p = 0 $: then $ p(t) $ has a single root $ \lambda $ with multiplicity $ 2 $, but $ p(t) = \norm{t \ve{v} + \ve{w}}^2 $, hence $ p(\lambda) = 0 \iff \lambda \ve{v} + \ve{w} = \ve{0} $, i.e. $ \ve{v} , \ve{w} $ LI.

    On the other hand, if $ \ve{v} $ and $ \ve{w} $ are LI, then $ \exists \lambda \in \R : \ve{v} = \lambda \ve{w} $, so by linearity $ \braket{\ve{v} , \ve{w}}^2 = \lambda^2 \braket{\ve{v} , \ve{v}}^2 = \lambda^2 \norm{\ve{v}}^4 = \norm{\ve{v}}^2 \norm{\ve{w}}^2 $.
  \end{proof}
\end{proofbox}

We can now state some properties of the norm.

\begin{lemma}{Basic properties of norm}{}
  Given $ \ve{v} , \ve{w} \in V $ and $ \lambda \in \R $, then:
  \begin{multicols}{2}
    \begin{enumerate}[label = {\alph*.}]
      \item $ \abs{\braket{\ve{v} , \ve{w}}} \leq \norm{\ve{v}} \norm{\ve{w}} $
      \item $ \norm{\ve{v}} \ge 0 \land \norm{\ve{v}} = 0 \iff \ve{v} = \ve{0} $
      \item $ \norm{\lambda \ve{v}} = \abs{\lambda} \norm{\ve{v}} $
      \item $ \norm{\ve{v} + \ve{w}} \leq \norm{\ve{v}} + \norm{\ve{w}} $
    \end{enumerate}
  \end{multicols}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Property (a) is a direct consequence of the Cauchy--Schwarz inequality, while (b) and (c) are trivial by the definition of inner product. Now, consider $ \ve{v} , \ve{w} \in V $:
    \begin{equation*}
      \begin{split}
        \norm{\ve{v} + \ve{w}}^2
        & = \braket{\ve{v} + \ve{w} , \ve{v} + \ve{w}} = \norm{\ve{v}}^2 + 2 \braket{\ve{v} , \ve{w}} + \norm{\ve{w}}^2 \\
        & \leq \norm{\ve{v}}^2 + 2 \norm{\ve{v}} \norm{\ve{w}} + \norm{\ve{w}}^2 = \left( \norm{\ve{v}} + \norm{\ve{w}} \right)^2
      \end{split}
    \end{equation*}
    where property (a) was used. This concludes the proof.
  \end{proof}
\end{proofbox}

We now introduce some more notation: if $ \ve{v} \in V : \norm{\ve{v}} = 1 $, then $ \ve{v} $ is a \bctxt{versor} (or unit vector). Moreover, given $ \ve{v} , \ve{w} \in V - \{\ve{0}\} $, then the \bctxt{angle} $ \vartheta \in [0, \pi] $ between them is defined as:
\begin{equation}
  \cos \theta = \frac{\braket{\ve{v} , \ve{w}}}{\norm{\ve{v}} \norm{\ve{w}}}
  \label{eq:angle-definition}
\end{equation}
which is well-defined by the Cauchy--Schwarz inequality. Two vectors $ \ve{v} , \ve{w} \in V $ are \bctxt{orthogonal} if $ \braket{\ve{v} , \ve{w}} = 0 $, i.e. if their angle is $ \vartheta = \frac{\pi}{2} $, and we denote this by $ \ve{v} \perp \ve{w} $.

\begin{definition}{Orthogonal subspace}{}
  Given a subspace $ U \ssq V $, its orthogonal space is $ U^\perp \defeq \{\ve{v} \in V : \braket{\ve{v} , \ve{u}} = 0 \,\,\forall \ve{u} \in U\} $.
\end{definition}

\begin{lemma}{}{}
  Given a subspace $ U \ssq V $ and a basis $ \{\ve{s}_1 , \dots , \ve{s}_k\} \ssq U $, then $ U^\perp = \{\ve{v} \in V : \ve{v} \perp \ve{s}_i \,\,\forall i = 1, \dots , k\} $ and it is a subspace of $ V $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Consider $ \ve{v} , \ve{w} \in U^\perp $ and $ \ve{u} \in U $: then $ \braket{\ve{u} , \lambda \ve{v} + \mu \ve{w}} = \lambda \braket{\ve{u} , \ve{v}} + \mu \braket{\ve{u} , \ve{w}} = 0 $, hence $ U^\perp $ is closed under linear combinations, i.e. a subspace of $ V $.

    Now, since $ U = \braket{\ve{s}_1 , \dots , \ve{s}_k} $, $ \forall \ve{u} \in U \,\exists! u_1 , \dots , u_k \in \R : \ve{u} = u_1 \ve{s}_1 + \dots + u_k \ve{s}_k $, so:
    \begin{equation*}
      0 = \braket{\ve{v} , \ve{u}} = \sum_{i = 1}^k u_i \braket{\ve{v} , \ve{s}_i}
      \quad \forall \ve{v} \in U^\perp
      \quad \iff \quad
      \braket{\ve{v} , \ve{s}_i} = 0
      \quad \forall i \in \{1 , \dots , k\}
    \end{equation*}
    which is the thesis.
  \end{proof}
\end{proofbox}

\subsubsection{Orthonormal bases}

It is possible to link orthogonality and linear independence in a straightforward way.

\begin{lemma}{Orthogonality and LI}{orthogonality-li}
  Given $ S = \{\ve{v}_1 , \dots , \ve{v}_k\} \ssq V - \{\ve{0}\} $, if $ \ve{v}_i \perp \ve{v}_j \,\,\forall i \neq j \in \{1, \dots, k\} $, then $ S $ is LI.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Consider $ \lambda_1 \ve{v}_1 + \dots + \lambda_k \ve{v}_k = \ve{0} $, so that:
    \begin{equation*}
      0 = \braket{\lambda_1 \ve{v}_1 + \dots + \lambda_k \ve{v}_k , \ve{v}_i} = \lambda_i \norm{\ve{v}_i}^2
      \quad \forall i \in \{1, \dots, k\}
    \end{equation*}
    Since $ \ve{v}_i \neq \ve{0} $, then $ \lambda_i = 0 \,\,\forall i \in \{1, \dots, k\} $, which is the thesis.
  \end{proof}
\end{proofbox}

An important consequence of this lemma is that a set of mutually-orthogonal vectors cannot have more than $ n = \dim_\R V $ vectors. Now, we can characterize a particular classes of bases of Euclidean spaces: orthonormal bases.

\begin{definition}{Orthonormal basis}{}
  A basis $ \bas = \{\ve{b}_1 , \dots , \ve{b}_n\} \ssq V $ is \bcdef{orthonormal} if it is composed by versors such that $ \ve{b}_i \perp \ve{b}_j \,\,\forall i \neq j \in \{1, \dots, n\} $.
\end{definition}

\begin{theorem}{Gram--Schmidt Theorem}{}
  Every non-trivial Euclidea vector space admits an orthonormal basis.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    To prove this theorem we use the following lemma.

    \begin{lemma}{}{gram-schmidt-auxiliary}
      Given orthonormal versors $ \ve{a}_1 , \dots , \ve{a}_k \in V $ and $ \ve{w} \in V - \braket{\ve{a}_1 , \dots , \ve{a}_k} $, then:
      \begin{equation*}
        \ve{a} \equiv \ve{w} - \sum_{i = 1}^k \braket{\ve{w} , \ve{a}_i} \ve{a}_i
        \quad : \quad
        \ve{a} \perp \ve{a}_i \,\,\forall i \in \{1, \dots, k\}
      \end{equation*}
    \end{lemma}

    \begin{proofbox}
      \begin{proof}
        Suppose $ \ve{a} = \ve{0} $: then $ \ve{w} = \sum_{i = 1}^k \braket{\ve{w} , \ve{a}_i} \ve{a}_i \in \braket{\ve{a}_1 , \dots , \ve{a}_k} \absurd $

        Then:
        \begin{equation*}
          \braket{\ve{a} , \ve{a}_i} = \braket{\ve{w} , \ve{a}_i} - \sum_{j = 1}^k \braket{\ve{w} , \ve{a}_j} \braket{\ve{a}_j , \ve{a}_i} = \braket{\ve{w} , \ve{a}_i} - \sum_{j = 1}^k \braket{\ve{w} , \ve{a}_j} \delta_{ij} = \braket{\ve{w} , \ve{a}_i} - \braket{\ve{w} , \ve{a}_i} = 0
        \end{equation*}
        which is the thesis.
      \end{proof}
    \end{proofbox}

    Now, consider $ \ve{v} \in V - \{\ve{0}\} $ and set $ \ve{a}_1 \equiv \frac{\ve{v}}{\norm{\ve{v}}} $: if $ V = \braket{\ve{a}_1} $ the proof is complete, otherwise $ \exists \ve{w} \in V - \braket{\ve{a}_1} $ and, by \lref{lemma:gram-schmidt-auxiliary}, $ \exists \ve{a} \in V : \ve{a} \perp \ve{a}_1 $. Set $ \ve{a}_2 \equiv \frac{\ve{a}}{\norm{\ve{a}}} $: if $ V = \braket{\ve{a}_1 , \ve{a}_2} $ the proof is complete, otherwise we iterate this process, which finds an orthonormal basis of $ V $ in $ n = \dim_\R V $ steps.
  \end{proof}
\end{proofbox}

This result allows us to define additional structure on $ V $.

\begin{proposition}{Orthogonal decomposition}{orthogonal-decomposition}
  Given a subspace $ W \ssq V $, then $ V = W \oplus W^\perp $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    If $ W = \{\ve{0}\} $ then trivially $ W^\perp = V $, so consider $ W \neq \{\ve{0}\} $: WTS $ W \cap W^\perp = \{\ve{0}\} $ and $ W + W^\perp = V $. First, $ \ve{w} \neq \ve{0} \implies \braket{\ve{w} , \ve{w}} \neq 0 \implies W \cap W^\perp = \{\ve{0}\} $. Then, take an orthonormal basis $ \bas = \{\ve{b}_1 , \dots , \ve{b}_k\} \ssq W $: thus $ \forall \ve{v} \in V \,\exists \ve{u}_1 \in W , \ve{u}_2 \in W^\perp : \ve{v} = \ve{u}_1 + \ve{u}_2 $ by \lref{lemma:gram-schmidt-auxiliary}:
    \begin{equation*}
      \ve{u}_1 = \sum_{i = 1}^k \braket{\ve{v} , \ve{b}_i} \ve{b}_i
      \qquad \qquad
      \ve{u}_2 = \ve{v} - \sum_{i = 1}^k \braket{\ve{v} , \ve{b}_i} \ve{b}_i
    \end{equation*}
    Hence, $ W + W^\perp $, which completes the proof.
  \end{proof}
\end{proofbox}

Given a subspace $ W \ssq V $ with orthonormal basis $ \bas = \{\ve{b}_1 , \dots , \ve{b}_k\} $, we denote the \bctxt{orthogonal projection} on $ W $ as the linear application $ \pi_W : V \ra W $ defined as:
\begin{equation}
  \pi_W(\ve{v}) \defeq \sum_{i = 1}^k \braket{\ve{v} , \ve{b}_i} \ve{b}_i
  \label{eq:orthogonal-projection-definition}
\end{equation}

\subsubsection{Symmetric endomorphisms}

\begin{definition}{Symmetric endomorphisms}{}
  Given $ f \in \End{V} $, it is \bcdef{symmetric} (or self-adjoint) if $ \braket{f(\ve{v}) , \ve{w}} = \braket{\ve{v} , f(\ve{w})} \,\,\forall \ve{v} , \ve{w} \in V $.
\end{definition}

From the linearity of the inner product, it is clear that the symmetry condition can be checked on the vectors of a particular basis, instead that on the whole vector space.

\begin{lemma}{Symmetric endomorphisms and representative matrices}{symmetric-representative-matrix}
  Given an orthonormal basis $ \bas = \{\ve{v}_1 , \dots , \ve{v}_n\} \ssq V $ of $ V $, then $ f \in \End{V} $ is symmetric if and only if $ \mt{M}_\bas^\bas(f) $ is symmetric.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Set $ \mt{M}_\bas^\bas(f) = [m_{ij}]_{i,j = 1, \dots, n} $, so that $ f(\ve{v}_i) = \sum_{k = 1}^n m_{ki} \ve{v}_k $. Then:
    \begin{equation*}
      \braket{f(\ve{v}_i) , \ve{v}_j} = \sum_{k = 1}^n m_{ki} \delta_{kj} = m_{ji}
      \qquad \qquad
      \braket{\ve{v}_i , f(\ve{v}_j)} = \sum_{k = 1}^n m_{kj} \delta_{ik} = m_{ij}
    \end{equation*}
    Hence, $ f $ is symmetric if and only if $ m_{ij} = m_{ji} $, i.e. if $ \mt{M}_\bas^\bas(f) $ is symmetric.
  \end{proof}
\end{proofbox}

It is possible to state two necessary conditions for an endomorphism to be symmetric.

\begin{proposition}{}{symmetry-necessary-conditions}
  Given a symmetric $ f \in \End{V} $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item given two distinct eigenvalues $ \lambda , \mu $ of $ f $ and $ \ve{v} \in V_\lambda(f) , \ve{w} \in V_\mu(f) $, then $ \ve{v} \perp \ve{w} $;
    \item the roots of $ p_f(t) \in \R_n[t] $ are all real.
  \end{enumerate}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item As $ f(\ve{v}) = \lambda \ve{v} $ and $ f(\ve{w}) = \mu \ve{w} $, then $ \braket{f(\ve{v}) , \ve{w}} = \lambda \braket{\ve{v} , \ve{w}} $ and $ \braket{\ve{v} , f(\ve{w})} = \mu \braket{\ve{v} , \ve{w}} $, but $ f $ is symmetric, hence $ (\lambda - \mu) \braket{\ve{v} , \ve{w}} $: since $ \lambda $ and $ \mu $ are distinct, $ \braket{\ve{v} , \ve{w}} = 0 $.
      \item Consider an orthonormal basis $ \bas \ssq V $ and $ \mt{A} \equiv \mt{M}_\bas^\bas(f) $, so that $ \mt{A}\tsp = \mt{A} \in \R^{n \times n} \ssq \C^{n \times n} $ by \lref{lemma:symmetric-representative-matrix}. Then, let $ \lambda \in \C : p_f(\lambda) = 0 $, which means that $ \exists \ve{v} \in \C^{n \times 1} - \{\ve{0}\} : \mt{A} \ve{v} = \lambda \ve{v} $: thus, $ \ve{v}\tsp \mt{A}\tsp = \lambda \ve{v}\tsp $ and $ \cc{\mt{A} \ve{v}} = \cc{\lambda \ve{v}} $, but $ \mt{A} $ is real and symmetric, so $ \ve{v}\tsp \mt{A} = \lambda \ve{v}\tsp $ and $ \mt{A} \cc{\ve{v}} = \cc{\lambda} \cc{\ve{v}} $. With further manipulation:
        \begin{equation*}
          \lambda \ve{v}\tsp \cc{\ve{v}} = \ve{v}\tsp \mt{A} \cc{\ve{v}} = \cc{\lambda} \ve{v}\tsp \cc{\ve{v}}
          \quad \implies \quad
          (\lambda - \cc{\lambda}) \ve{v}\tsp \cc{\ve{v}} = 0
        \end{equation*}
        However, $ \ve{v}\tsp \cc{\ve{v}} = \sum_{i = 1}^n \abs{v_i}^2 > 0 $, hence $ \lambda = \cc{\lambda} $, i.e. $ \lambda \in \R $.
    \end{enumerate}
    \ensp
  \end{proof}
\end{proofbox}

We can now prove the most important result for symmetric endomorphism: the real spectral theorem.

\begin{theorem}{Real spectral theorem}{real-spectral-theorem}
  Given a non-trivial Euclidean vector space $ V $, then $ f \in \End{V} $ is symmetric if and only if $ \exists \bas \ssq V $ orthonormal basis of $ V $ composed of eigenvectors of $ f $, i.e. if $ f $ is diagonalizable.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    $ (\Leftarrow) $ Trivially, if $ f $ is diagonalizable, then in the diagonalizing basis its representative matrix is diagonal, i.e. symmetric, and so is $ f $ too.

    $ (\Rightarrow) $ We use induction on $ n \equiv \dim_\R V $:
    \begin{itemize}
      \item $ n = 1 $ is true as $ V = \braket{\ve{v}} $, with $ \ve{v} \neq \ve{0} $, so $ \bas = \{\ve{a}\} $, with $ \ve{a} \equiv \frac{\ve{v}}{\norm{\ve{v}}} $, is an orthonormal basis and, since $ f(\ve{a}) \in V = \braket{\ve{a}} $, it is composed of eigenvectors of $ f $ as $ \exists \lambda \in \R : f(\ve{a}) = \lambda \ve{a} $.
      \item Assume that the implication is true for $ \dim_\R V = n - 1 $.
      \item If $ f \in \End{V} $ is symmetric, then all its eigenvalues are real by \pref{prop:symmetry-necessary-conditions}. Let $ \lambda \in \R $ be an eigenvalue of $ f $ and $ \ve{v} \in V_\lambda(f) - \{\ve{0}\} $, and define $ S \equiv \braket{\ve{v}} $: then, $ f $ is $ S^\perp $-invariant, i.e. $ f(S^\perp) \ssq S^\perp $, since:
        \begin{equation*}
          \ve{w} \in S^\perp
          \quad \implies \quad
          \braket{\ve{v} , f(\ve{w})} = \braket{f(\ve{v}) , \ve{w}} = \lambda \braket{\ve{v} , \ve{w}} = 0
          \quad \implies \quad
          f(\ve{w}) \in S^\perp
        \end{equation*}
        This means that $ f\vert_{S^\perp} \in \End{S^\perp} $, but $ \dim_\R S^\perp = n - 1 $ since $ V = S \oplus S^\perp $ (by \pref{prop:orthogonal-decomposition}), hence, by the inductive step, $ \exists \bas = \{\ve{b}_1 , \dots , \ve{b}_{n-1}\} \ssq S^\perp $ orthonormal basis of $ S^\perp $ such that $ \ve{b}_1 , \dots , \ve{b}_{n-1} $ are eigenvectors of $ f $. \\
        Now, set $ \ve{b}_n \equiv \frac{\ve{v}}{\norm{\ve{v}}} $: then, $ \bas = \{\ve{b}_1 , \dots , \ve{b}_n\} $ is an orthonormal basis of $ V $ composed of eigenvectors of $ f $, since $ \ve{b}_n \perp \ve{b}_i \,\,\forall i \in \{1, \dots, n-1\} \implies \bas $ is LI by \lref{lemma:orthogonality-li}, and $ f(\ve{b}_n) = \lambda \ve{b}_n $ by definition of $ \ve{v} $.
    \end{itemize}
    \ensp
  \end{proof}
\end{proofbox}

A computational approach to check if $ f \in \End{V} $ is symmetric is to consider a basis $ \bas = \{\ve{b}_1 , \dots , \ve{b}_n\} \ssq V $ and $ \mt{P} \equiv [\braket{\ve{b}_i , \ve{b}_j}]_{i,j = 1, \dots, n} $: then, given $ \ve{u} , \ve{v} \in V \cong \R^n $, we write $ \braket{\ve{u} , f(\ve{v})} = \ve{u}\tsp \mt{P} \mt{A} \ve{v} $ and $ \braket{f(\ve{u}) , \ve{v}} = \ve{u}\tsp \mt{A}\tsp \mt{P} \ve{v} $, with $ \mt{A} \equiv \mt{M}_\bas^\bas(f) $, so the symmetry condition becomes $ \mt{A}\tsp \mt{P} = \mt{P} \mt{A} $.

\subsection{Hermitian vector spaces}

We can generalize the results for Euclidean vector spaces to the case $ \K = \C $. In this subsection, we always consider $ V $ to be a $ \C $-vector space of dimension $ \dim_\C V = n \in \N $.

\begin{definition}{Hermitian product}{hermitian-product}
  An application $ b : V \times V \ra \C $ is a \bcdef{Hermitian product} if it has the following properties:
  \begin{enumerate}
    \item $ b(\ve{u} , \ve{v}) = \cc{b(\ve{v} , \ve{u})} \,\,\forall \ve{u} , \ve{v} \in V $
    \item $ b(\ve{u} + \ve{v} , \ve{w}) = b(\ve{u} , \ve{w}) + b(\ve{v} , \ve{w}) \,\land\, b(\ve{u} , \ve{v} + \ve{w}) = b(\ve{u} , \ve{v}) + b(\ve{u} , \ve{w}) \,\,\forall \ve{u} , \ve{v} , \ve{w} \in V $
    \item $ b(\lambda \ve{u} , \ve{v}) = \cc{\lambda} b(\ve{u} , \ve{v}) \,\land\, b(\ve{u} , \lambda \ve{v}) = \lambda b(\ve{u} , \ve{v}) \,\,\forall \lambda \in \C , \ve{u} , \ve{v} \in V $
    \item $ b(\ve{u} , \ve{u}) \ge 0 \,\,\forall \ve{u} \in V \,\land\, b(\ve{u} , \ve{u}) = 0 \iff \ve{u} = \ve{0} $
  \end{enumerate}
\end{definition}

A vector space with a Hermitian product is a \bctxt{Hermitian space}, in which the definitions of norm and orthogonality are analogous to those in the Euclidean case.

\begin{definition}{Hermitian endomorphism}{}
  Given $ f \in \End{V} $, it is \bcdef{Hermitian} if $ \braket{f(\ve{v}) , \ve{w}} = \braket{\ve{v} , f(\ve{w})} \,\,\forall \ve{v} , \ve{w} \in V $.
\end{definition}

To extend this notation to matrices, we define the \bctxt{Hermitian conjugate} of a matrix $ \mt{A} $ as $ \mt{A}\dg \defeq \cc{\mt{A}}\tsp $: if $ \mt{A} = \mt{A}\dg $, we say that $ \mt{A} $ is a Hermitian matrix.

The main results for Euclidean vector spaces hold for Hermitian spaces too: in particular, \lref{lemma:symmetric-representative-matrix} (with the Hermitianity condition, in place of the symmetry one) and \pref{prop:symmetry-necessary-conditions} remain as-is, while for \tref{th:real-spectral-theorem} only the forward implication holds, since diagonalizability does not necessarily imply Hermitianity.

\begin{example}{Counterexample for the complex spectral theorem}{}
  Consider $ V = \C^2 $ with canonical basis $ \bas = \{\binom{1}{0} , \binom{0}{1}\} $ and the endomorphism $ f \in \End \C^2 $ represented by:
  \begin{equation*}
  \mt{M}_\bas^\bas(f) =
  \begin{bmatrix}
    \img & 0 \\ 0 & -\img
  \end{bmatrix}
  \end{equation*}
  Clearly this is not a Hermitian endomorphism, even though it is diagonalizable.
\end{example}

\begin{proposition}{}{}
  Given $ f \in \End{V} $, then $ f $ is Hermitian if and only if $ \braket{f(\ve{v}) , \ve{v}} \in \R \,\,\forall \ve{v} \in V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ $ \braket{f(\ve{v}) , \ve{v}} = \braket{\ve{v} , f(\ve{v})} = \cc{\braket{f(\ve{v}) , \ve{v}}} \implies \braket{f(\ve{v}) , \ve{v}} \in \R $

    $ (\Leftarrow) $ $ \braket{f(\ve{v}) , \ve{v}} \in \R \implies \braket{f(\ve{v}) , \ve{v}} = \cc{\braket{f(\ve{v}) , \ve{v}}} = \braket{\ve{v} , f(\ve{v})} $
  \end{proof}
\end{proofbox}

\subsection{Unitary endomorphisms}

An interesting class of endomorphisms are unitary endomorphisms.

\begin{definition}{Unitary endomorphism}{}
  Given a Euclidean or Hermitian vector space $ V $, then $ f \in \End{V} $ is \bcdef{unitary} if $ \braket{f(\ve{v}) , f(\ve{w})} = \braket{\ve{v} , \ve{w}} \,\,\forall \ve{v} , \ve{w} \in V $.
\end{definition}

\begin{lemma}{Unitary endomorphisms as automorphisms}{}
  Every unitary endomorphisms is an automorphism.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Let $ \ve{v} \in \ker{f} $. Then $ \braket{\ve{v} , \ve{v}} = \braket{f(\ve{v}) , f(\ve{v})} = \braket{\ve{0} , \ve{0}} = 0 \iff \ve{v} = \ve{0} $, hence $ f $ is injective by \pref{prop:kernel-injections}, and so a bijection by \cref{cor:equidimensionality-bijections}.
  \end{proof}
\end{proofbox}

 It is easy to see from the definition that an endomorphism is an isometry if and only if it maps orthonormal bases to orthonormal bases.

In the case of a Euclidean vector space, unitary endomorphisms are called isometries.

\begin{proposition}{Isometries and representative matrices}{isometries-representative-matrices}
  Given an $ n $-dimensional Euclidean vector space $ V $ and an orthonormal basis $ \bas \ssq V $, then $ f \in \End{V} $ is an isometry if and only if $ \mt{M}_\bas^\bas(f) \in \On{n} $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Let $ \bas = \{\ve{b}_1 , \dots , \ve{b}_n\} $ and $ \mt{M}_\bas^\bas(f) = [m_{ij}]_{i,j = 1, \dots, n} $. WTS $ \mathcal{C} \equiv \{f(\ve{b}_1 , \dots , \ve{b}_n\} $ is an orthonormal basis of $ V $:
    \begin{equation*}
      \braket{f(\ve{b}_i) , f(\ve{b}_j)} = \sum_{k,\ell = 1}^n m_{ki} m_{\ell j} \braket{\ve{b}_k , \ve{b}_\ell} = \sum_{k = 1}^n m_{ki} m_{kj} = [(\mt{M}_\bas^\bas(f))\tsp \mt{M}_\bas^\bas(f)]_{ij}
    \end{equation*}
    It is then clear that $ f $ is an isometry if and only if $ (\mt{M}_\bas^\bas(f))\tsp \mt{M}_\bas^\bas(f) = \mt{I}_n $.
  \end{proof}
\end{proofbox}

In a generic basis $ \bas = \{\ve{b}_1 , \dots , \ve{b}_n\} \ssq V $, to check if $ f \in \End{V} $ is an isometry, consider $ \mt{P} \equiv [\braket{\ve{b}_i , \ve{b}_j}]_{i,j = 1, \dots , n} $ and set $ \mt{M}_\bas^\bas(f) \equiv \mt{A} $: then, $ \braket{\ve{u} , \ve{v}} = \ve{u}\tsp \mt{P}\ve{v} $ and $ \braket{f(\ve{u}) , f(\ve{v})} = \ve{u}\tsp \mt{A}\tsp \mt{P} \mt{A} \ve{v} $, so that the unitarity condition becomes $ \mt{A}\tsp \mt{P} \mt{A} = \mt{P} $.

\begin{proposition}{Unitary endomorphisms and representative matrices}{}
  Given an $ n $-dimensional Hermitian vector space $ V $ and an orthonormal basis $ \bas \ssq V $, then $ f \in \End{V} $ is unitary if and only if $ \mt{M}_\bas^\bas(f) \in \Un{n} $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    The proof is analogous to that of \pref{prop:isometries-representative-matrices}, recalling \dref{def:hermitian-product}.
  \end{proof}
\end{proofbox}










