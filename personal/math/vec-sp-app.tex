\selectlanguage{english}

\section{Matrices}

\begin{definition}{Matrix}{}
  Given a field $ \K $ and $ n , m \in \N $, an $ n \times m $ \bcdef{matrix}\index{matrix} on $ \K $ is the object:
  \begin{equation*}
    \mt{A} =
    \begin{bmatrix}
      a_{11} & a_{12} & \dots & a_{1m} \\
      a_{21} & a_{22} & \dots & a_{2m} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{n1} & a_{n2} & \dots & a_{nm}
    \end{bmatrix}
    \equiv [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m}
    \quad : \quad
    a_{ij} \in \K \,\,\forall i = 1, \dots, n ,\, j = 1, \dots, m
  \end{equation*}
  The set of all $ n \times m $ matrices on $ \K $ is denoted by $ \K^{n \times m} $.
\end{definition}

When the dimensions of the matrix $ \mt{A} $ are unambiguous, we simply write $ \mt{A} = [a_{ij}] $. We say that an $ n \times n $ matrix is a \bctxt{square matrix}, an $ n \times 1 $ matrix is a \bctxt{column vector} and a $ 1 \times n $ matrix is a \bctxt{row vector}.

It is possible to define three operations between matrices:
\begin{itemize}
  \item sum $ + : \K^{n \times m} \times \K^{n \times m} \rightarrow \K^{n \times m} : [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} + [b_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} \mapsto [a_{ij} + b_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $
  \item product by a scalar $ \cdot : \K \times \K^{n \times m} \rightarrow \K^{n \times m} : \alpha \cdot [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} = [\alpha a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $
  \item product $ \cdot : \K^{n \times p} \times \K^{p \times m} \rightarrow \K^{n \times m} : [a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, p} \cdot [b_{ij}]^{i = 1, \dots, p}_{j = 1, \dots, m} = [c_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $, $ c_{ij} = \sum_{k = 1}^p a_{ik} b_{kj} $
\end{itemize}
Note that $ \alpha a_{ij} $ is the $ \K $-product.

\begin{proposition}{}{mat-abel-gr}
  $ (\K^{n \times m} , +) $ is an abelian group.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    The matrix sum is equivalent to the $ \K $-sum of corresponding elements, which is associative and commutative. The neutral element is the zero matrix $ 0_{n \times m} = [0]^{i = 1, \dots, n}_{j = 1, \dots, m} $, while the inverse element is $ -\mt{A} = [-a_{ij}]^{i = 1, \dots, n}_{j = 1, \dots, m} $.
  \end{proof}
\end{proofbox}

\begin{theorem}{}{}
  $ (\K^{n \times n} , + , \cdot) $ is a non-commutative ring.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    By \pref{prop:mat-abel-gr}, $ (\K^{n \times n} , +) $ is an abelian group. It is trivial to show the associativity and distributivity of the matrix product, i.e.:
    \begin{enumerate}
      \item $ \mt{A} \cdot (\mt{B} \cdot \mt{C}) = (\mt{A} \cdot \mt{B}) \cdot \mt{C} ,\, \lambda (\mt{A} \cdot \mt{B}) = (\lambda \mt{A}) \cdot \mt{B} = \mt{A} \cdot (\lambda \mt{B}) \,\,\forall \mt{A} , \mt{B} , \mt{C} \in \K^{n \times n} , \lambda \in \K $
      \item $ \mt{A} \cdot (\mt{B} + \mt{C}) = \mt{A} \cdot \mt{B} + \mt{A} \cdot \mt{C} ,\, (\mt{A} + \mt{B}) \cdot \mt{C} = \mt{A} \cdot \mt{C} + \mt{B} \cdot \mt{C} \,\,\forall \mt{A} , \mt{B} , \mt{C} \in \K^{n \times n} $
    \end{enumerate}
    Finally, the neutral element of the matrix product is the identity matrix $ \mt{I}_n = [\delta_{ij}]_{i,j = 1, \dots, n} $.
  \end{proof}
\end{proofbox}

\begin{definition}{Transposed matrix}{}
  Given a matrix $ \mt{A} \in \K^{n \times m} $, its \bcdef{transpose} is defined as $ \mt{A}\tsp \in \K^{m \times n} : [a\tsp_{ij}]^{i = 1, \dots, m}_{j = 1, \dots, n} = [a_{ji}]^{j = 1, \dots, n}_{i = 1, \dots, m} $.
\end{definition}

A square matrix $ \mt{A} \in \K^{n \times n} $ is said \bctxt{symmetric} if $ \mt{A}\tsp = \mt{A} $ or \bctxt{antisymmetric} if $ \mt{A}\tsp = - \mt{A} $, and it is \bctxt{diagonal} if $ a_{ij} = 0 \,\,\forall i \neq j \in \{1, \dots, n\} $.

\begin{definition}{Inverse matrix}{}
  A square matrix $ \mt{A} \in \K^{n \times n} $ is \bcdef{invertible} if $ \exists \mt{A}^{-1} \in \K^{n \times n} : \mt{A}^{-1} \cdot \mt{A} = \mt{A} \cdot \mt{A}^{-1} = \mt{I}_n $.
\end{definition}

\begin{example}{Non-invertible matrix}{}
  The matrix $ \begin{bmatrix} 2 & 0 \\ 0 & 0 \end{bmatrix} $ is non-invertible, as $ \begin{bmatrix} 2 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} = \begin{bmatrix} 2 \alpha & 2 \beta \\ 0 & 0 \end{bmatrix} \neq \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \,\forall \alpha , \beta , \gamma , \delta \in \R $.
\end{example}

\begin{definition}{General linear group}{}
  The \bcdef{general linear group}\index{$ \GL{n,\K} $} $ \GL{n,\K} $ is defined as the subset of $ \K^{n \times n} $ of all invertible matrices.
\end{definition}

Note that $ \GL{1,\K} = \K - \{0\} $.

\begin{theorem}{}{}
  $ (\GL{n,\K} , \cdot) $ is a non-abelian group.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    The neutral element is $ \mt{I}_n $, as $ \mt{I}_n^{-1} = \mt{I}_n \implies \mt{I}_n \in \GL{n,\K} $, while the existence of the inverse is granted by definition. We only have to show closure under matrix multiplication:
    \begin{equation*}
      (\mt{A} \mt{B})^{-1} = \mt{B}^{-1} \mt{A}^{-1} \impliedby \mt{I}_n = \mt{A} \cdot \mt{A}^{-1} = \mt{A} \mt{I}_n \mt{A}^{-1} = \mt{A} \mt{B} \mt{B}^{-1} \mt{A}^{-1} = (\mt{A} \mt{B}) (\mt{A} \mt{B})^{-1}
    \end{equation*}
    Hence, $ \mt{A} , \mt{B} \in \GL{n,\K} \implies \mt{A} \mt{B} \in \GL{n,\K} $.
  \end{proof}
\end{proofbox}

\subsection{Linear systems of equations}

A \bctxt{linear equation} with $ n \in \N $ variables and $ \K $-coefficients is an expression of the form:
\begin{equation*}
  a_1 x_1 + \dots + a_n x_n = b
  \qquad
  a_i , b \in \K \,\,\forall i = 1, \dots, n
\end{equation*}
A \bctxt{solution} of the equation is an $ n $-tuple $ (\bar{x}_1, \dots, \bar{x}_n) \in \K^n $ which satisfies this expression.

\begin{definition}{Linear system of equations}{}
  A linear system of equations (or simply \bcdef{linear system}\index{linear system}) is a collection of $ m $ linear equations with $ n $ variables:
  \begin{equation*}
    \begin{cases}
      a_{11} x_1 + \dots + a_{1n} x_n = b_1 \\
      a_{21} x_1 + \dots + a_{2n} x_n = b_2 \\
      \qquad \qquad \quad \vdots \\
      a_{m1} x_1 + \dots + a_{mn} x_n = b_m \\
    \end{cases}
    \qquad \iff \qquad
    \mt{A} \ve{x} = \ve{b}
  \end{equation*}
  where we defined:
  \begin{equation*}
    \mt{A} =
    \begin{bmatrix}
      a_{11} & \dots & a_{1n} \\
      a_{21} & \dots & a_{2n} \\
      \vdots & \ddots & \vdots \\
      a_{m1} & \dots & a_{mn}
    \end{bmatrix}
    \in \K^{m \times n}
    \qquad \qquad
    \ve{b} =
    \begin{pmatrix}
      b_1 \\ b_2 \\ \vdots \\ b_m
    \end{pmatrix}
    \in \K^{m \times 1}
    \qquad \qquad
    \ve{x} =
    \begin{pmatrix}
      x_1 \\ x_2 \\ \vdots \\ x_n
    \end{pmatrix}
    \in \K^{n \times 1}
  \end{equation*}
\end{definition}

Two linear systems with the same set of solutions are called \bctxt{equivalent systems}: note that two equivalent systems must have the same number of variables, but not necessarily the same number of equations.

Based on the cardinality of its solution set, a linear system is said to be \bctxt{impossible} if it has no solutions, \bctxt{determined} if it has one solution and \bctxt{undetermined} if it has infinitely-many solutions. Moreover, if the solution set can be parametrized by $ k \in \N_0 $ variables, the system is of kind $ \infty^k $: a determined system is of kind $ \infty^0 $.

Linear systems can be systematically solved applying a reduction algorithm to their corresponding matrices: \bctxt{Gauss algorithm}\index{Gauss algorithm}. Starting with a general composed matrix $ [\mt{A} | \ve{b}] \in \K^{m \times (n+1)} $, first we multiply the first row by $ a_{11}^{-1} $, so that:
\begin{equation*}
  \left[
  \begin{array}{cccc|c}
    a_{11} & a_{12} & \dots & a_{1n} & b_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
  \quad \longrightarrow \quad
 \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
\end{equation*}
Then, at each row $ \mt{R}_2 , \dots , \mt{R}_m $ we apply the transformation $ \mt{R}_k \mapsto \mt{R}_k - a_{k1} \mt{R}_1 $, so that:
\begin{equation*}
  \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
  \quad \longrightarrow \quad
 \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    0 & a'_{22} & \dots & a'_{2n} & b'_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & a'_{m2} & \dots & a'_{mn} & b'_m \\
  \end{array}
  \right]
\end{equation*}
Reiterating this process to progressively smalles submatrices, the algorithm yields the general transformation:
\begin{equation*}
  \left[
  \begin{array}{cccc|c}
    a_{11} & a_{12} & \dots & a_{1n} & b_1 \\
    a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn} & b_m \\
  \end{array}
  \right]
  \quad \longrightarrow \quad
  \left[
  \begin{array}{cccc|c}
    1 & a'_{12} & \dots & a'_{1n} & b'_1 \\
    0 & 1 & \dots & a'_{2n} & b'_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \dots & 1 & b'_m \\
  \end{array}
  \right]
\end{equation*}
As these are linear transformations, the two matrices represent equivalent linear systems: the transformed linear system is substantially easier to solve, and its solution set is a solution set of the starting linear system too.

\begin{definition}{Character}{}
  Given a matrix $ \mt{M} \in \K^{n \times m} $, its \bcdef{character} $ \mathrm{car}(\mt{M}) $ is the number of non-zero rows remaining after Gauss reduction.
\end{definition}

It can be proven that the character is independent of the operations performed during the reduction algorithm.

\begin{theorem}{Rouché--Capelli theorem}{}
  A linear system $ \mt{A} \ve{x} = \ve{b} $ has solutions only if $ \mathrm{car}(\mt{A}) = \mathrm{car}([\mt{A} | \ve{b}]) $. Moreover, if the system has solutions, then it is of kind $ \infty^{n - r} $, with $ n $ number of variables and $ r = \mathrm{car}(\mt{A}) $.\index{theorem!Rouché--Capelli}
\end{theorem}

\section{Vector spaces}

\begin{definition}{Vector space}{}
  Given a set $ V \neq \emptyset $ and a field $ \K $, then $ V $ is a \bcdef{$ \K $-vector space} if there exist two operations:
  \begin{equation*}
    + : V \times V \ra V \,:\, (\ve{v} , \ve{w}) \mapsto \ve{v} + \ve{w}
    \qquad \qquad
    \cdot : \K \times V \ra V \,:\, (\lambda , \ve{v}) \mapsto \lambda \cdot \ve{v}
  \end{equation*}
  such that $ (V,+) $ is an abelian group and the following properties hold $ \forall \lambda , \mu \in \K , \ve{v} , \ve{w} \in V $:
  \begin{enumerate}
    \item $ (\lambda + \mu) \cdot (\ve{v} + \ve{w}) = \lambda \cdot \ve{v} + \mu \cdot \ve{v} + \lambda \cdot \ve{w} + \mu \cdot \ve{w} $
    \item $ (\lambda \cdot \mu) \cdot \ve{v} = \lambda \cdot (\mu \cdot \ve{v}) = \mu \cdot (\lambda \cdot \ve{v}) $
    \item $ 1_\K \cdot \ve{v} = \ve{v} $
  \end{enumerate}
\end{definition}

Note that there are three unique neutral elements: $ 0_\K \equiv 0 $, $ 1_\K \equiv 1 $ and $ 0_V \equiv \ve{0} $.
In the following, the multiplication symbol $ \cdot $ is suppressed, as the factors clarify which multiplication is occurring ($ \cdot : \K \times \K \ra \K $ or $ \cdot : \K \times V \ra V $, which have the same neutral element $ 1_\K $).

\begin{example}{Complex numbers}{}
  $ V = \C $ is a vector space both for $ \K = \R $ and $ \K = \C $, although they are different objects.
\end{example}

\begin{example}{Field as vector space}{}
  $ V = \K $ is a $ \K $-vector space. Note that, in this case, $ 0_\K \equiv 0_V $.
\end{example}

Note that, by the uniqueness of $ 0_V $, then $ \forall \ve{v} \in V \,\,\exists! -\ve{v} \in V : \ve{v} + (-\ve{v}) = 0_V $, so the following cancellation rule holds $ \forall \ve{u} , \ve{v} , \ve{w} \in V $:
\begin{equation}
  \ve{u} + \ve{v} = \ve{w} + \ve{v}
  \quad \implies \quad
  \ve{u} = \ve{w}
  \label{eq:cancellation-rule}
\end{equation}

We can now state some basic properties of vector spaces.

\begin{lemma}{Basic properties of vector spaces}{}
  Given a $ \K $-vector space $ V $, then $ \forall \lambda \in \K , \ve{v} \in V $:
  \begin{multicols}{2}
    \begin{enumerate}[label = {\alph*.}]
      \item $ 0_\K \cdot \ve{v} = 0_V $
      \item $ (-\lambda) \cdot \ve{v} = - (\lambda \cdot \ve{v}) $
      \item $ \lambda \cdot 0_V = 0_V $
      \item $ \lambda \cdot \ve{v} = 0_V \iff \lambda = 0_\K \lor \ve{v} = 0_V $
    \end{enumerate}
  \end{multicols}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item Consider $ c \in \K - \{0_\K\} $; then $ c \ve{v} + 0_V = c \ve{v} = (c + 0_\K) \ve{v} = c \ve{v} + 0_\K \cdot \ve{v} $, which by \eref{eq:cancellation-rule} proves $ 0_\K \cdot \ve{v} = 0_V $.
      \item $ \lambda \ve{v} + (-\lambda) \ve{v} = (\lambda - \lambda) \ve{v} = 0_\K \cdot \ve{v} = 0_V $, which by the uniqueness of the negative element proves $ (-\lambda) \ve{v} = - (\lambda \ve{v}) $.
      \item $ \lambda \cdot 0_V = \lambda (\ve{v} - \ve{v}) = \lambda \ve{v} + \lambda \cdot (-1_\K) \cdot \ve{v} = \lambda \ve{v} + (-\lambda) \ve{v} = \lambda \ve{v} - (\lambda \ve{v}) = 0_V $
      \item $ \lambda = 0_\K $ is trivial, so consider $ \lambda \neq 0_\K $; then $ \exists ! \lambda^{-1} \in \K : \lambda^{-1} \cdot \lambda = 1_\K $, so $ 0_V = \lambda^{-1} \cdot 0_V = \lambda^{-1} \cdot (\lambda \ve{v}) = (\lambda^{-1} \cdot \lambda) \ve{v} = 1_\K \cdot \ve{v} = \ve{v} $, i.e. $ \ve{v} = 0_V $.
    \end{enumerate}
  \end{proof}
\end{proofbox}

\subsection{Subspaces}

\begin{definition}{Subspace}{}
  Given a $ \K $-vector space $ V $ and a subset $ U \subseteq V : U \neq \emptyset $, then $ U $ is a \bcdef{subspace}\index{subspace} of $ V $ if it is closed under $ + : U \times U \ra U $ and $ \cdot : \K \times U \ra U $.
\end{definition}

\begin{lemma}{}{}
  If $ U $ is a subspace of $ V(\K) $, then $ 0_V \in U $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    By definition $ U \neq \emptyset \implies \exists \ve{v} \in U $. By the closure condition $ \lambda \ve{v} \in U \,\,\forall \lambda \in \K $, hence taking $ \lambda = 0_\K $ proves the thesis.
  \end{proof}
\end{proofbox}

A typical strategy to prove that $ U $ is a subspace of $ V(\K) $ is showing the closure properties, while to prove that it is \emph{not} a subspace we usually show that $ 0_V \notin U $.

\begin{example}{Polynomial subspaces}{}
  Given $ V = \K[x] $, then $ U = \K_n[x] $ is a subspace $ \forall n \in \N_0 $.
\end{example}

An important concept to analyze vector spaces is that of linear combination. Given two sets $ \{\lambda_k\}_{k = 1, \dots, n} \subset \K $ and $ \{\ve{v}_k\}_{k = 1, \dots, n} \subset V $, their \bctxt{linear combination}\index{linear combination} is:
\begin{equation}
  \sum_{k = 1}^n \lambda_k \ve{v}_k = \lambda_1 \ve{v}_1 + \dots \lambda_n \ve{v}_n \in V
\end{equation}

\begin{proposition}{Subspaces and linear combinations}{subspaces-linear-combinations}
  Given a $ \K $-vector space $ V $ and $ U \subset V : U \neq \emptyset $, then $ U $ is a subspace of $ V $ if and only if it is closed under linear combinations, that is:
  \begin{equation*}
    \{\lambda_k\}_{k = 1, \dots, n} \subset \K , \{\ve{v}_k\}_{k = 1, \dots, n} \subset U
    \quad \implies \quad
    \sum_{k = 1}^n \lambda_k \ve{v}_k \in U
  \end{equation*}
\end{proposition}

\begin{proofbox}
  \begin{proof}
    First, note that the general case of linear combinations of $ n $ vectors can be reduced to the case of $ 2 $ vectors.

    $ (\Rightarrow) $ Being $ U $ a subspace, it is closed under $ + : U \times U \ra U $ and $ \cdot : \K \times U \ra U $; then, by definition $ \lambda , \mu \in \K , \ve{v} , \ve{w} \in U \implies \lambda \ve{v} + \mu \ve{w} \in U $.

    $ (\Leftarrow) $ Given $ \lambda \in \K $ and $ \ve{v} , \ve{w} \in V $, then $ \ve{v} + \ve{w} = 1_\K \ve{v} + 1_\K \ve{w} $ and $ \lambda \ve{v} = \lambda \ve{v} + 0_\K \ve{w} $, hence closure under linear combinations implies closure under $ + : U \times U \ra U $ and $ \cdot : \K \times U \ra U $.
  \end{proof}
\end{proofbox}

Generally, it is easier to show closure under linear combinations rather than under addition and scalar multiplication.

\begin{lemma}{Intersection of subspaces}{}
  Given two subspaces of $ V_1 , V_2 $ of $ V(\K) $, then $ V_1 \cap V_2 $ is still a subset of $ V(\K) $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Being $ V_1 , V_2 $ subspaces, both $ V_1 $ and $ V_2 $ are closed under linear combinations, so $ V_1 \cap V_2 $ is too, as $ \ve{v} \in V_1 \cap V_2 \implies \ve{v} \in V_1 \land \ve{v} \in V_2 $.
  \end{proof}
\end{proofbox}

On the other hand, in general $ V_1 \cup V_2 $ is not a subspace. As a counterexample, consider e.g. $ V = \Vect_0(\mathbb{E}^3) $, the plane $ \pi : z = 0 $ and the line $ r : (x,y,z) = (0,0,t) , t \in \R $; then, consider the subspaces $ V_1 = \Vect_0(\pi) , V_2 = \Vect_0(r) $: their union is clearly not closed under addition, as:
\begin{equation*}
  \begin{pmatrix}
    1 \\ 1 \\ 0
  \end{pmatrix} \in V_1
  ,
  \begin{pmatrix}
    0 \\ 0 \\ 1
  \end{pmatrix} \in V_2
  \qquad \qquad
  \begin{pmatrix}
    1 \\ 1 \\ 0
  \end{pmatrix}
  +
  \begin{pmatrix}
    0 \\ 0 \\ 1
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 \\ 1 \\ 1
  \end{pmatrix} \notin V_1 \cup V_2
\end{equation*}

\begin{definition}{Sum of subspaces}{}
  Given a $ \K $-vector space $ V $ and two subspaces $ V_1 , V_2 $, their \bcdef{sum}\index{subspace!sum of} is defined as:
  \begin{equation*}
    V_1 + V_2 \defeq \{\ve{w} \in V : \ve{w} = \ve{u} + \ve{v}, \ve{u} \in V_1, \ve{v} \in V_2\}
  \end{equation*}
  This is a \bcdef{direct sum}\index{direct sum!of subspaces}, denoted by $ V_1 \oplus V_2 $, if every $ \ve{w} \in V_1 + V_2 $ has a unique representation as $ \ve{w} = \ve{u} + \ve{v} , \ve{u} \in V_1 , \ve{v} \in V_2 $.
\end{definition}

Trivially $ V_1 , V_2 \subseteq V_1 + V_2 $.

\begin{lemma}{Direct sum as disjoint sum}{}
  Given two subspaces $ V_1 , V_2 $ of $ V(\K) $, then $ V_1 + V_2 = V_1 \oplus V_2 \iff V_1 \cap V_2 = \{\ve{0}\} $.
\end{lemma}

\begin{proofbox}
  \begin{proof}
    $ (\Rightarrow) $ Suppose $ \exists \ve{v} \in V_1 \cap V_2 : \ve{v} \neq \ve{0} $; then $ \ve{v} = \ve{v} + \ve{0} = \ve{0} + \ve{v} $, i.e. the expression of $ \ve{v} \in V_1 + V_2 $, but the expression of $ \ve{v} \in V_1 \oplus V_2 $ must be unique, hence $ \ve{v} = \ve{0} \absurd $.

    $ (\Leftarrow) $ Suppose $ \exists \ve{w} \in V_1 + V_2 : \ve{w} = \ve{u}_1 + \ve{v}_1 = \ve{u}_2 + \ve{v}_2 , \ve{u}_1 \neq \ve{u}_2 \in V_1 , \ve{v}_1 \neq \ve{v}_2 \in V_2 $; then $ V_1 \ni \ve{u}_1 - \ve{u}_2 = \ve{v}_2 - \ve{v}_1 \in V_2 \implies \ve{v}_2 - \ve{v}_1 \in V_1 $, so $ \ve{v}_2 - \ve{v}_1 \in V_1 \cap V_2 $, but $ V_1 \cap V_2 = \{\ve{0}\} $, hence $ \ve{v}_2 = \ve{v}_1 $ and idem for $ \ve{u}_1 = \ve{u}_2 \absurd $.
  \end{proof}
\end{proofbox}

The sum of subspaces preserves the subspace structure, contrary to the simple union.

\begin{proposition}{Sum as subspace}{}
  Given a $ \K $-vector space and two subspaces $ V_1 , V_2 $, their sum $ V_1 + V_2 $ is still a subspace of $ V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Consider $ \ve{a} , \ve{b} \in V_1 + V_2 $ and define $ \ve{u}_{a,b} \in V_1 , \ve{v}_{a,b} \in V_2 : \ve{a} = \ve{u}_a + \ve{v}_a \land \ve{b} = \ve{u}_b + \ve{v}_b $: as $ V_1 , V_2 $ are subspaces, they are closed under linear combinations, so, given $ \lambda ,\mu \in \K $, then $ \lambda \ve{a} + \mu \ve{b} = (\lambda \ve{u}_a + \mu \ve{u}_b) + (\lambda \ve{v}_a + \mu \ve{v}_b) \equiv \ve{u} + \ve{v} \in V_1 + V_2 $, where $ \ve{u} \in V_1 $ and $ \ve{v} \in V_2 $, which shows that $ V_1 + V_2 $ too is closed under linear combinations and a subspace by \pref{prop:subspaces-linear-combinations}.
  \end{proof}
\end{proofbox}

\subsection{Bases}

To give a more explicit description of vector spaces, we have to define the concept of basis and its properties.

\subsubsection{Generators}

\begin{definition}{Linear dependence}{}
  Given a $ \K $-vector space $ V $ and a set $ \{\ve{v}_j\}_{j = 1, \dots, k} \equiv S \subseteq V $, then the vectors of $ S $ are:
  \begin{itemize}
    \item \bcdef{linearly dependent} (LD) if $ \exists \{\lambda_j\}_{j = 1, \dots, k} \subset \K - \{0\} : \lambda_1 \ve{v}_1 + \dots \lambda_k \ve{v}_k = \ve{0} $
    \item \bcdef{linearly independent}\index{linear independence!of vectors} (LI) if $ \lambda_1 \ve{v}_1 + \dots \lambda_k \ve{v}_k = \ve{0} \iff \lambda_j = 0 \,\,\forall j = 1, \dots, k $
  \end{itemize}
\end{definition}

The generalization to infinite sets is trivial: $ \{\ve{v}_\alpha\}_{\alpha \in \mathcal{I}} \equiv S \subset V(\K) $ is LI if every finite subset of $ S $ is LI, while it is LD if there exists at least one non-empty subset which is LD.

\begin{example}{Complex numbers}{}
  $ \{1, \img\} $ are LD in $ \C(\C) $, as $ 1 \cdot 1 + \img \cdot \img = 0 $, while they are LI in $ \C(\R) $.
\end{example}

\begin{example}{Polynomials}{}
  $ \{1, x, \dots, x^n, \dots\} $ are LI in $ \K[x] $.
\end{example}

We can prove some basic properties of linear dependence.

\begin{lemma}{Basic properties of linear dependence}{}
  Given a $ \K $-vector space $ V $ and $ S \subseteq V : S \neq \emptyset $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item given $ S \subseteq T \subseteq V $, then $ S \text{ LD} \implies T \text{ LD} $
    \item $ S = \{\ve{v}\} \text{ LD} \implies \ve{v} = \ve{0} $
    \item $ S = \{\ve{v}_1 , \ve{v}_2\} \text{ LD} \implies \exists \lambda \in \K : \ve{v}_1 = \lambda \ve{v}_2 $
    \item if $ S = \{\ve{v}_1 , \dots , \ve{v}_n\} $ LD, then at least one $ \ve{v}_i $ is a linear combination of the other vectors
    \item if $ S $ LI and $ S \cup \{\ve{w}\} $ LD, then $ \ve{w} $ is a linear combination of the vectors of $ S $
    \item if $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n = \ve{0} $ and $ \lambda_n \neq 0 $, then $ \ve{v}_n $ is a linear combination of $ \{\ve{v}_1 , \dots , \ve{v}_{n-1}\} $
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item $ S \subseteq T \implies \ve{v} \in T \,\,\forall \ve{v} \in S $, hence $ \{\ve{v}_i\}_{i = 1, \dots, n} \subset S \text{ LD} \implies \{\ve{v}_i\}_{i = 1, \dots, n} \subset T \text{ LD} $
      \item $ \lambda \ve{v} = \ve{0} \iff \lambda = 0 \lor \ve{v} = \ve{0} $, so $ \ve{v} = \ve{0} \implies S \text{ LD} $, while $ S \text{ LD} \implies \lambda \neq 0 \implies \ve{v} = 0 $
      \item $ \{\ve{v}_1 , \ve{v}_2\} \text{ LD} \implies \exists \lambda , \mu \in \K - \{0\} : \lambda \ve{v}_1 + \mu \ve{v}_2 = \ve{0} \iff \ve{v}_1 = \lambda^{-1} \mu \ve{v}_2 $
      \item If $ \{\ve{v}_j\}_{j = 1, \dots, n} $ LD, then by definition $ \exists \{\lambda_j\}_{j = 1, \dots, n} \subset \K - \{0\} : \sum_{j = 1}^n \lambda_j \ve{v}_j = \ve{0} $, hence WLOG $ \ve{v}_1 $ can be isolated as $ \ve{v}_1 = - \lambda_1^{-1} \sum_{j = 2}^n \lambda_j \ve{v}_j $
      \item $ \{\ve{v}_1 , \dots , \ve{v}_n , \ve{w}\} \text{ LD} \implies \exists \lambda_1 , \dots , \lambda_n , \alpha \in \K - \{0\} : \sum_{j = 1}^n \lambda_j \ve{v}_j + \alpha \ve{w} = \ve{0} $, so $ \ve{w} $ can be isolated as $ \ve{w} = - \alpha^{-1} \sum_{j = 1}^n \lambda_j \ve{v}_j $
      \item $ \sum_{j = 1}^n \lambda_j \ve{v}_j = \ve{0} \land \lambda_n \neq 0 \implies \ve{v}_n = - \lambda_n^{-1} \sum_{j = 1}^{n-1} \lambda_j \ve{v}_j $
    \end{enumerate}
  \end{proof}
\end{proofbox}

We can now introduce the notion of generators.

\begin{definition}{Generated subset}{}
  Given a $ \K $-vector space $ V $ and $ \{\ve{v}_\alpha\}_{\alpha \in \mathcal{I}} \equiv S \subseteq V $, the \bcdef{subset generated by $ S $} is the set:
  \begin{equation*}
    \lspan{S} \defeq \{\ve{v} \in V : \exists \lambda_1 , \dots , \lambda_n \in \K, \ve{v}_{\alpha_1} , \dots , \ve{v}_{\alpha_n} \in S : \ve{v} = \lambda_1 \ve{v}_{\alpha_1} + \dots + \lambda_n \ve{\alpha_n}\}
  \end{equation*}
  The elements of $ S $ are called \bcdef{generators} of $ \lspan{S} $.
\end{definition}

We often denote $ \lspan{S} \equiv \braket{S} $: this subset contains all vectors of $ V $ which can be expressed as linear combinations of vectors of $ S $.

\begin{proposition}{Generated subspace}{}
  Given a $ \K $-vector space and $ S \subseteq V : S \neq \emptyset $, then $ \braket{S} $ is a subspace of $ V $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Let $ S = \{\ve{s}_\alpha\}_{\alpha \in \mathcal{I}} $ and $ \ve{v} , \ve{w} \in S : \ve{v} = \sum_{j = 1}^k \lambda_j \ve{s}_{\alpha_j} , \ve{w} = \sum_{j = 1}^n \mu_j \ve{s}_{\beta_j} $, with coefficients $ \{\lambda_j\}_{j = 1, \dots, k} , \{\mu_j\}_{j = 1, \dots, n} \subset \K - \{0\} $. Adding vectors with vanishing coefficients, we can rewrite $ \ve{v} $ and $ \ve{w} $ in terms of the same vectors:
    \begin{equation*}
      \ve{v} = \sum_{j = 1}^m a_j \ve{s}_{\gamma_j}
      \qquad \qquad
      \ve{w} = \sum_{j = 1}^m b_j \ve{s}_{\gamma_j}
      \quad \implies \quad
      \zeta \ve{v} + \xi \ve{w} = \sum_{j = 1}^m \left( \zeta a_j + \xi b_j \right) \ve{s}_{\gamma_j} \in \braket{S}
    \end{equation*}
    This shows that $ \braket{S} $ is closed under linear combination, hence the thesis.
  \end{proof}
\end{proofbox}

Note that, give a subspace $ U \subseteq V(\K) $, then at most $ U = \braket{U} $, hence every subspace admits a family of generators. If $ U $ has a finite number of generators, then it is a \bctxt{finitely-generated subspace}: for example, $ \K_n[x] = \braket{1, \dots, x^n} $, $ \C(\C) = \braket{1} $ and $ \C(\R) = \braket{1, \img} $ are finitely-generated.
We can state two trivial properties of generated subsets.

\begin{lemma}{}{}
  Given $ S \subseteq V(\K) $ and $ U = \braket{S} $, then:
  \begin{enumerate}[label = {\alph*.}]
    \item given $ S \subseteq T \subseteq V $, then $ U = \braket{T} $
    \item if $ U = \braket{\ve{s}_1 , \dots , \ve{s}_n} $ and $ \ve{s}_n \in \braket{\ve{s}_1 , \ve{s}_{n-1}} $, then $ U = \braket{\ve{s}_1 , \dots , \ve{s}_{n-1}} $
  \end{enumerate}
\end{lemma}

\begin{proofbox}
  \begin{proof}
    Respectively:
    \begin{enumerate}[label = {\alph*.}]
      \item If $ S \subseteq T $, then each linear combination in $ S $ is a linear combination in $ T $ too, hence $ \braket{S} = \braket{T} $
      \item Given $ \ve{v} = \lambda_1 \ve{s}_1 + \dots + \lambda_n \ve{s}_n \in U $ and $ \ve{s}_n = \mu_1 \ve{s}_1 + \dots + \mu_{n-1} \ve{s}_{n-1} $, then $ \ve{v} = \left( \lambda_1 + \mu_1 \right) \ve{s}_1 + \dots + \left( \lambda_{n-1} + \mu_{n-1} \right) \ve{s}_{n-1} $, hence the thesis
    \end{enumerate}
  \end{proof}
\end{proofbox}

\subsubsection{Bases of generic vector spaces}

\begin{definition}{Basis of a vector space}{}
  Given a $ \K $-vector space $ V $, a \bcdef{basis} of $ V $ is a LI subset $ \bas \subseteq V : V = \braket{\bas} $.
\end{definition}

Every non-trivial vector space (i.e. $ V \neq \{\ve{0}\} $) admits the existence of a basis, but the proof is non-trivial as it relies on Zorn's Lemma (or equivalently to the Axiom of Choice).

\begin{theorem}{Basis theorem}{}
  Every non-trivial vector space admits a basis.
\end{theorem}

\begin{proofbox}
  \begin{proof}
    First, we prove that every LI subset of $ V $ can be extended to a basis of $ V $. Let $ A \subseteq V $ be a non-empty LI subset of $ V $, and define $ \mathscr{S} $ the collection of all LI supersets of $ A $.

    \begin{lemma}{}{}
      Given a chain $ \{A_\alpha\}_{\alpha \in \mathcal{I}} \subseteq \mathscr{S} : A_1 \subseteq A_2 \subseteq \dots $, then $ \bigcup_{\alpha \in \mathcal{I}} A_\alpha \in \mathscr{S} $.
    \end{lemma}

    \begin{proofbox}
      \begin{proof}
        Set $ \mathcal{A} \equiv \bigcup_{\alpha \in \mathcal{I}} A_\alpha $. If $ A \subseteq A_\alpha \,\,\forall \alpha \in \mathcal{I} $, then trivially $ A \subseteq \mathcal{A} $. To prove the linear independence, consider a linear combination $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n $ in $ \mathcal{A} $, with $ n \in \N $, and choose an $ A_{\alpha_n} $ large enough so that $ v_1 , \dots , v_n \in A_{\alpha_n} $. Then, $ \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{v}_n = \ve{0} \implies \lambda_1 , \dots , \lambda_n = 0 $, as $ A_{\alpha_n} $ is LI by definition. Since $ n \in \N $ is generic, $ \mathcal{A} $ is LI.
      \end{proof}
    \end{proofbox}

    It is then clear that $ \mathscr{S} $ satisfies the hypotheses of Zorn's Lemma (\lref{lemma:zorn}), therefore it has a maximal element $ \bas $. Now, suppose $ \braket{\bas} \neq V $, i.e. $ \exists \ve{b} \in V - \braket{\bas} $, and consider the linear combination $ \mu \ve{b} + \lambda_1 \ve{v}_1 + \dots + \lambda_n \ve{b}_n = \ve{0} $, with $ \ve{v}_1 , \dots , \ve{v}_n \in \bas $ and $ n \in \N $: then $ - \mu \ve{b} \in \braket{\bas} $, but $ \ve{b} \notin \braket{\bas} $, so $ \mu = 0 $ (as $ \ve{b} \neq \ve{0} \in \braket{\bas} $). Consequently, $ \lambda_1 = \dots = \lambda_n = 0 $ as $ \bas $ is LI, thus $ \bas \cup \{\ve{b}\} $ is LI and a superset of $ \bas \in \mathscr{S} $, which contradicts $ \bas $ being a maximal element of $ \mathscr{S} \absurd $.

    Having showed that every LI subset $ A \subseteq V $ can be extended to a basis $ \bas $ of $ V $, the thesis is trivially found taking $ A = \emptyset $, which is a subset of every non-trivial vector space.
  \end{proof}
\end{proofbox}

This, though trivial for finite-dimensional spaces, is quite impressive for infinite-dimensional ones (for dimensionality, see SECTION).

\begin{proposition}{}{}
  Given a $ \K $-vector space $ V $, then $ S \subseteq V $ is a basis of $ V $ if and only if every element of $ V $ has a unique representation as a linear combination of elements of $ S $.
\end{proposition}

\begin{proofbox}
  \begin{proof}
    Note that two representations are equal if they differ only by vanishing coefficients.

    $ (\Rightarrow) $ As $ V = \braket{S} $, then every $ \ve{v} \in V $ can be written as a linear combination of elements of $ S $. Suppose that $ \ve{v} $ has two representations:
    \begin{equation*}
      \ve{v} = \lambda_1 \ve{s}_1 + \dots \lambda_n \ve{s}_n
      \qquad \qquad
      \ve{v} = \mu_1 \ve{t}_1 + \dots + \mu_m \ve{t}_m
    \end{equation*}
    with $ \{\ve{s}_j\}_{j = 1, \dots, n} , \{\ve{t}_k\}_{k = 1, \dots, m} \subseteq S $ and $ \{\lambda_j\}_{j = 1, \dots, n} , \{\mu_k\}_{k = 1, \dots, m} \subseteq \K $. Now, we can extend both representations by adding vanishing coefficients, so that both include the same vectors of $ S $:
    \begin{equation*}
      \ve{v} = \zeta_1 \ve{v}_1 + \dots + \zeta_r \ve{v}_r
      \qquad \qquad
      \ve{v} = \xi_1 \ve{v}_1 + \dots + \xi_r \ve{v}_r
    \end{equation*}
    with $ \{\ve{v}_j\}_{j = 1, \dots, r} \subseteq S $ and $ \{\zeta_j\}_{j = 1, \dots, r} , \{\xi_j\}_{j = 1, \dots, r} \subseteq \K $. Subtracting these two expressions:
    \begin{equation*}
      \ve{0} = \left( \zeta_1 - \xi_1 \right) \ve{v}_1 + \dots + \left( \zeta_r - \xi_r \right) \ve{v}_r
    \end{equation*}
    But $ S $ is LI, hence $ \zeta_j = \xi_j \,\,\forall j = 1, \dots, r $, i.e. the two representations are equal.

    $ (\Leftarrow) $ As every $ \ve{v} \in V $ can be written as a linear combination of elements of $ S $, then $ V = \braket{S} $. We only have to prove that $ S $ is LI. Consider $ \ve{0} \in V $: by hypothesis, it has a unique representation as a linear combination of vectors in $ S $, and a possible representation is $ \ve{0} = 0 \cdot \ve{s} $ for some $ \ve{s} \in S $, i.e. the trivial representation with all vanishing coefficients. Now, consider a linear combination in $ S $:
    \begin{equation*}
      \lambda_1 \ve{s}_1 + \dots + \lambda_n \ve{s}_n = \ve{0}
    \end{equation*}
    with $ n \in \N $. This too is a representation of $ \ve{0} $, hence $ \lambda_j = 0 \,\,\forall j = 1, \dots, n $ by the uniqueness of the representation. As $ n \in \N $ is generic, this is the definition of $ S $ being LI.
  \end{proof}
\end{proofbox}

\subsubsection{Bases of finitely-generated vector spaces}

We now turn our attention to finitely-generated vector spaces, i.e. $ V = \braket{\ve{v}_1 , \dots , \ve{v}_n} $ with $ n \in \N $.









\section{Linear applications}

\section{Inner products}
